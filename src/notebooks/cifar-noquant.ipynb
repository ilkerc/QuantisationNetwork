{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '9437' (I am process '9451')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/ilker/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.12-64/lock_dir\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97210a2072fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mLasagne\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstallation\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadthedocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;31m#theano\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not import Theano.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minstall_instr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_gpu_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;31m# We can't test the driver during import of theano.sandbox.cuda as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# this cause circular import dependency. So we also test it manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;31m# import dependency. So we also test it in the file theano/__init__.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_driver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_gpu_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     assert config.device == \"cpu\", (\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36muse\u001b[0;34m(device, force, default_to_move_computation_to_gpu, move_shared_float32_to_gpu, enable_cuda, test_driver)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0mwarn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mdnn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mhdr_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0mcudnn_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36mdnn_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;31m# If we can compile, check that we can import and run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mdnn_available\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36mdnn_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m         f = theano.function([], DnnVersion()(),\n\u001b[1;32m    417\u001b[0m                             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                             profile=False)\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mdnn_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdnn_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1651\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1652\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1055\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                                                  \u001b[0mno_recycling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1058\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 924\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 828\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1587\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1589\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mcompilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m             \u001b[0;31m# 1) Maybe somebody else compiled it for us while we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0;31m#    where waiting for the lock. Try to load it again.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/compilelock.pyc\u001b[0m in \u001b[0;36mlock_ctx\u001b[0;34m(lock_dir, keep_lock, **kw)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlock_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mget_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/compilelock.pyc\u001b[0m in \u001b[0;36m_get_lock\u001b[0;34m(lock_dir, **kw)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Only really try to acquire the lock if we do not have it already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_lock\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnlocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlocker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Store time at which the lock was set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilker/anaconda2/envs/theano/lib/python2.7/site-packages/theano/gof/compilelock.pyc\u001b[0m in \u001b[0;36mlock\u001b[0;34m(tmp_dir, timeout, min_wait, max_wait, verbosity)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0mno_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mnb_wait\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "# os.environ['THEANO_FLAGS']='contexts=dev0->cuda0;dev1->cuda1'\n",
    "os.environ['THEANO_FLAGS']='device=gpu1'\n",
    "import time\n",
    "import matplotlib\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pylab as P\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "from helpers.DiscreteLayer import DiscreteLayer\n",
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer as conv\n",
    "    from lasagne.layers.dnn import MaxPool2DDNNLayer as pool\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as conv\n",
    "    from lasagne.layers import MaxPool2DLayer as pool\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EPOCHS = 1500\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "DIM = 32\n",
    "CHANNEL = 3\n",
    "NUM_CLASSES = 10\n",
    "mnist_cluttered = \"data/mnist_cluttered_60x60_6distortions.npz\"\n",
    "\n",
    "# Disc. Layer Settings\n",
    "DISC = False\n",
    "QUANT = np.array([0.125 for i in range(512)], dtype='float32')\n",
    "VARIANCE_DEVIDER = 16.0\n",
    "DISC_UNIT = 65 # Last dense (param regressor)\n",
    "\n",
    "# Test Specs\n",
    "TEST_NAME = 'cifar-nonquant'\n",
    "\n",
    "# Additional Settings\n",
    "lasagne.random.set_rng(np.random.RandomState(12345))  # Set random state so we can investigate results\n",
    "np.random.seed(1234)\n",
    "#theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50000, 3, 32, 32))\n",
      "(50000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    X_train = np.transpose(X_train, (0, 3, 2, 1))\n",
    "    X_test = np.transpose(X_test, (0, 3, 2, 1))\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = np.squeeze(y_train)\n",
    "    y_test = np.squeeze(y_test)\n",
    "    \n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        input_channel=X_train.shape[1],\n",
    "        output_dim=10,)\n",
    "data = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_width, input_height, input_channel, output_dim,\n",
    "                batch_size=BATCH_SIZE, withdisc=True):\n",
    "    # Initers, Layers\n",
    "    ini = lasagne.init.HeUniform()\n",
    "    rect = lasagne.nonlinearities.rectify\n",
    "    drop = lasagne.layers.DropoutLayer\n",
    "    flatlay = lasagne.layers.flatten\n",
    "    dense = lasagne.layers.DenseLayer\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    \n",
    "    # Input Layer\n",
    "    \n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, input_channel, input_width, input_height))\n",
    "    \n",
    "    # 1st - Deep\n",
    "    \n",
    "    conv1 = conv(l_in, num_filters=32, filter_size=(3, 3), nonlinearity=rect, W=ini, pad='same')\n",
    "    conv2 = conv(conv1, num_filters=32, filter_size=(3, 3), nonlinearity=rect, W=ini)\n",
    "    pool1 = pool(conv2, pool_size=(2, 2))\n",
    "    drop1 = drop(pool1, p=0.25)\n",
    "    \n",
    "    # 2nd - Deeper\n",
    "    \n",
    "    conv3 = conv(drop1, num_filters=64, filter_size=(3, 3), nonlinearity=rect, W=ini, pad='same')\n",
    "    conv4 = conv(conv3, num_filters=64, filter_size=(3, 3), nonlinearity=rect, W=ini)\n",
    "    pool1 = pool(conv4, pool_size=(2, 2))\n",
    "    drop2 = drop(pool1, p=0.25)\n",
    "    \n",
    "    # Last Steps\n",
    "    \n",
    "    flatte = flatlay(drop2)\n",
    "    dense1 = dense(flatte, num_units=512, nonlinearity=rect)\n",
    "    dense2 = dense(dense1, num_units=DISC_UNIT, nonlinearity=rect, name='param_regressor')\n",
    "    \n",
    "    # Choose the last regularizer\n",
    "    if withdisc:\n",
    "        sharedBins = theano.shared(None, name='sharedBins')\n",
    "        l_dis = DiscreteLayer(dense2, sharedBins=sharedBins, name='disclayer')\n",
    "    else:\n",
    "        l_dis = drop(dense2, p=0.10)\n",
    "        \n",
    "    l_out = dense(l_dis, num_units=output_dim, nonlinearity=softmax)\n",
    "\n",
    "    if withdisc:\n",
    "        return l_out, sharedBins\n",
    "    else:\n",
    "        return l_out\n",
    "\n",
    "if DISC:\n",
    "    model, sharedBins = build_model(DIM, DIM, CHANNEL, NUM_CLASSES, withdisc=DISC)\n",
    "else:\n",
    "    model = build_model(DIM, DIM, CHANNEL, NUM_CLASSES, withdisc=DISC)\n",
    "\n",
    "model_params = lasagne.layers.get_all_params(model, trainable=True)\n",
    "# params = lasagne.layers.get_all_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: dist, dist.shape = (-1, num_units)\n",
    "Find quantization bins of a given dist history\n",
    "Returns a list of (x, num_units), where x's length is a random variable\n",
    "\"\"\"\n",
    "def find_quantization_bins(dist, sharedBins):\n",
    "    # Quantizer function\n",
    "    def Q(x, y):\n",
    "        return y * np.floor((x/y) + .5)\n",
    "    \n",
    "    shape = dist.shape\n",
    "    init_Q = QUANT\n",
    "    final_Q = []\n",
    "    \n",
    "    # Theta iterator\n",
    "    for i in range(shape[1]):\n",
    "        theta_i = dist[:, i]\n",
    "        \n",
    "        # Whats is the error threshold for this distribution\n",
    "        Q_eps = np.var(theta_i) / VARIANCE_DEVIDER\n",
    "        \n",
    "        # Batch Iterator\n",
    "        final_Q_i = []\n",
    "        for j in range(shape[0]):\n",
    "            theta = theta_i[j]\n",
    "            \n",
    "            # Quantized theta = Quantization bins\n",
    "            q = init_Q[i]\n",
    "            x_i = theta\n",
    "            x_o = Q(x_i, q)\n",
    "            \n",
    "            # Optimize x_o\n",
    "\n",
    "            while(np.abs(x_o - x_i) > Q_eps):\n",
    "                q = q / 2\n",
    "                x_o = Q(x_i, q)\n",
    "            \n",
    "            # End of optimisation\n",
    "            final_Q_i.append(x_o)\n",
    "        \n",
    "        # Append to outer list\n",
    "        uniques = np.unique(np.array(final_Q_i))\n",
    "        final_Q.append(uniques.astype(theano.config.floatX))\n",
    "        \n",
    "    # Report\n",
    "    print \"New Bin Sizes: [\" + \", \".join([str(final_Q[x].shape[0]) for x in range(shape[1])] ) + \"]\"\n",
    "    sharedBins.set_value(final_Q)\n",
    "    return final_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_functions():\n",
    "    X = T.tensor4(dtype=theano.config.floatX)\n",
    "    y = T.ivector()\n",
    "\n",
    "    ## Layer History\n",
    "    if DISC:\n",
    "        l_disc = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'disclayer')\n",
    "        l_paramreg = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'param_regressor')\n",
    "        l_disc_output, l_paramreg_output = lasagne.layers.get_output([l_disc, l_paramreg], X, deterministic=False)\n",
    "    ## Layer History\n",
    "\n",
    "    # training output\n",
    "    output_train = lasagne.layers.get_output(model, X, deterministic=False)\n",
    "\n",
    "    # evaluation output. Also includes output of transform for plotting\n",
    "    output_eval = lasagne.layers.get_output(model, X, deterministic=True)\n",
    "\n",
    "    sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(output_train, y))\n",
    "    updates = lasagne.updates.adam(cost, model_params, learning_rate=sh_lr)\n",
    "\n",
    "    eval = theano.function([X], [output_eval])\n",
    "    \n",
    "    if DISC:\n",
    "        train = theano.function([X, y], [cost, output_train, l_disc_output, l_paramreg_output], updates=updates)\n",
    "    else:\n",
    "        train = theano.function([X, y], [cost, output_train], updates=updates)\n",
    "    \n",
    "    \n",
    "    return train, eval, sh_lr\n",
    "\n",
    "train, eval, sh_lr = build_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y):\n",
    "    # History Keeping\n",
    "    param_output = []\n",
    "    disc_output = []\n",
    "    # History\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        if DISC:\n",
    "            cost, output_train, l_disc_output, l_paramreg_output = train(X_batch, y_batch)\n",
    "            param_output = np.append(param_output, l_paramreg_output)\n",
    "            disc_output = np.append(disc_output, l_disc_output)\n",
    "        else:\n",
    "            cost, output_train = train(X_batch, y_batch)\n",
    "        costs += [cost]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "    \n",
    "    return np.mean(costs), correct / float(num_samples), param_output, disc_output\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    output_eval = eval(X)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: T.cost 1.706351, Train acc 0.372680, test acc 0.534600, took 24.8 sec.\n",
      "Epoch 1: T.cost 1.265792, Train acc 0.549660, test acc 0.623500, took 24.7 sec.\n",
      "Epoch 2: T.cost 1.075184, Train acc 0.619400, test acc 0.663600, took 24.7 sec.\n",
      "Epoch 3: T.cost 0.951524, Train acc 0.667620, test acc 0.692400, took 24.7 sec.\n",
      "Epoch 4: T.cost 0.846261, Train acc 0.705580, test acc 0.716200, took 24.8 sec.\n",
      "Epoch 5: T.cost 0.759863, Train acc 0.733820, test acc 0.739300, took 24.7 sec.\n",
      "Epoch 6: T.cost 0.694184, Train acc 0.756640, test acc 0.747400, took 24.8 sec.\n",
      "Epoch 7: T.cost 0.633298, Train acc 0.777560, test acc 0.750700, took 24.7 sec.\n",
      "Epoch 8: T.cost 0.577788, Train acc 0.798400, test acc 0.761700, took 24.8 sec.\n",
      "Epoch 9: T.cost 0.532452, Train acc 0.811660, test acc 0.755800, took 24.7 sec.\n",
      "Epoch 10: T.cost 0.485457, Train acc 0.828440, test acc 0.750400, took 24.6 sec.\n",
      "Epoch 11: T.cost 0.448802, Train acc 0.840760, test acc 0.760100, took 19.4 sec.\n",
      "Epoch 12: T.cost 0.414611, Train acc 0.852700, test acc 0.752700, took 15.2 sec.\n",
      "Epoch 13: T.cost 0.391291, Train acc 0.861620, test acc 0.755600, took 19.9 sec.\n",
      "Epoch 14: T.cost 0.364120, Train acc 0.871840, test acc 0.768800, took 19.8 sec.\n",
      "Epoch 15: T.cost 0.338147, Train acc 0.880500, test acc 0.774700, took 20.1 sec.\n",
      "Epoch 16: T.cost 0.306699, Train acc 0.890860, test acc 0.766600, took 19.8 sec.\n",
      "Epoch 17: T.cost 0.292238, Train acc 0.896080, test acc 0.765000, took 20.0 sec.\n",
      "Epoch 18: T.cost 0.256024, Train acc 0.908400, test acc 0.759100, took 19.5 sec.\n",
      "New LR: 0.000990000047022\n",
      "Epoch 19: T.cost 0.236426, Train acc 0.915760, test acc 0.766000, took 20.1 sec.\n",
      "Epoch 20: T.cost 0.229326, Train acc 0.919560, test acc 0.770300, took 19.6 sec.\n",
      "Epoch 21: T.cost 0.217225, Train acc 0.923580, test acc 0.775200, took 20.0 sec.\n",
      "Epoch 22: T.cost 0.205502, Train acc 0.927680, test acc 0.774300, took 19.7 sec.\n",
      "Epoch 23: T.cost 0.190398, Train acc 0.933020, test acc 0.771500, took 20.0 sec.\n",
      "Epoch 24: T.cost 0.187883, Train acc 0.934340, test acc 0.769600, took 19.6 sec.\n",
      "Epoch 25: T.cost 0.175028, Train acc 0.938920, test acc 0.759200, took 20.0 sec.\n",
      "Epoch 26: T.cost 0.166365, Train acc 0.942020, test acc 0.767800, took 19.7 sec.\n",
      "Epoch 27: T.cost 0.157821, Train acc 0.943840, test acc 0.765200, took 20.1 sec.\n",
      "Epoch 28: T.cost 0.158194, Train acc 0.944840, test acc 0.769400, took 19.4 sec.\n",
      "Epoch 29: T.cost 0.140771, Train acc 0.950900, test acc 0.768700, took 20.1 sec.\n",
      "Epoch 30: T.cost 0.136464, Train acc 0.952840, test acc 0.767600, took 19.6 sec.\n",
      "Epoch 31: T.cost 0.143590, Train acc 0.951280, test acc 0.771100, took 20.1 sec.\n",
      "Epoch 32: T.cost 0.137840, Train acc 0.951960, test acc 0.770500, took 19.5 sec.\n",
      "Epoch 33: T.cost 0.125951, Train acc 0.956580, test acc 0.775900, took 12.5 sec.\n",
      "Epoch 34: T.cost 0.121847, Train acc 0.958500, test acc 0.775000, took 20.6 sec.\n",
      "Epoch 35: T.cost 0.117576, Train acc 0.959280, test acc 0.766100, took 20.4 sec.\n",
      "Epoch 36: T.cost 0.113632, Train acc 0.961100, test acc 0.770900, took 20.7 sec.\n",
      "Epoch 37: T.cost 0.113381, Train acc 0.960720, test acc 0.776300, took 20.5 sec.\n",
      "Epoch 38: T.cost 0.109401, Train acc 0.962640, test acc 0.763800, took 20.8 sec.\n",
      "New LR: 0.00098010008689\n",
      "Epoch 39: T.cost 0.111469, Train acc 0.962740, test acc 0.772900, took 20.4 sec.\n",
      "Epoch 40: T.cost 0.103642, Train acc 0.964400, test acc 0.773700, took 20.8 sec.\n",
      "Epoch 41: T.cost 0.101144, Train acc 0.965700, test acc 0.769500, took 20.4 sec.\n",
      "Epoch 42: T.cost 0.102266, Train acc 0.965500, test acc 0.772500, took 20.9 sec.\n",
      "Epoch 43: T.cost 0.097848, Train acc 0.966940, test acc 0.769800, took 20.3 sec.\n",
      "Epoch 44: T.cost 0.093361, Train acc 0.968400, test acc 0.774400, took 20.7 sec.\n",
      "Epoch 45: T.cost 0.089834, Train acc 0.969700, test acc 0.763900, took 20.4 sec.\n",
      "Epoch 46: T.cost 0.092799, Train acc 0.968640, test acc 0.773600, took 20.5 sec.\n",
      "Epoch 47: T.cost 0.088618, Train acc 0.970520, test acc 0.775800, took 20.9 sec.\n",
      "Epoch 48: T.cost 0.082285, Train acc 0.972420, test acc 0.771600, took 20.5 sec.\n",
      "Epoch 49: T.cost 0.084575, Train acc 0.970460, test acc 0.762700, took 20.7 sec.\n",
      "Epoch 50: T.cost 0.083690, Train acc 0.971400, test acc 0.781300, took 20.4 sec.\n",
      "Epoch 51: T.cost 0.084131, Train acc 0.971240, test acc 0.776000, took 20.8 sec.\n",
      "Epoch 52: T.cost 0.085263, Train acc 0.971320, test acc 0.774700, took 15.2 sec.\n",
      "Epoch 53: T.cost 0.077147, Train acc 0.974260, test acc 0.780200, took 16.4 sec.\n",
      "Epoch 54: T.cost 0.077065, Train acc 0.974340, test acc 0.777300, took 21.1 sec.\n",
      "Epoch 55: T.cost 0.082104, Train acc 0.972960, test acc 0.769600, took 20.6 sec.\n",
      "Epoch 56: T.cost 0.076883, Train acc 0.974260, test acc 0.771300, took 21.0 sec.\n",
      "Epoch 57: T.cost 0.073782, Train acc 0.975400, test acc 0.768900, took 20.5 sec.\n",
      "Epoch 58: T.cost 0.070245, Train acc 0.976580, test acc 0.773200, took 20.4 sec.\n",
      "New LR: 0.000970299127512\n",
      "Epoch 59: T.cost 0.079540, Train acc 0.974480, test acc 0.774300, took 20.9 sec.\n",
      "Epoch 60: T.cost 0.069942, Train acc 0.977780, test acc 0.768500, took 20.3 sec.\n",
      "Epoch 61: T.cost 0.070960, Train acc 0.976760, test acc 0.776900, took 21.0 sec.\n",
      "Epoch 62: T.cost 0.069947, Train acc 0.976820, test acc 0.775100, took 20.6 sec.\n",
      "Epoch 63: T.cost 0.066188, Train acc 0.978100, test acc 0.777000, took 20.8 sec.\n",
      "Epoch 64: T.cost 0.066268, Train acc 0.978340, test acc 0.779100, took 20.4 sec.\n",
      "Epoch 65: T.cost 0.063936, Train acc 0.978180, test acc 0.775900, took 21.0 sec.\n",
      "Epoch 66: T.cost 0.065318, Train acc 0.978140, test acc 0.781000, took 20.5 sec.\n",
      "Epoch 67: T.cost 0.061840, Train acc 0.978660, test acc 0.779800, took 20.8 sec.\n",
      "Epoch 68: T.cost 0.062037, Train acc 0.979980, test acc 0.775500, took 20.9 sec.\n",
      "Epoch 69: T.cost 0.063489, Train acc 0.979000, test acc 0.783500, took 20.4 sec.\n",
      "Epoch 70: T.cost 0.060655, Train acc 0.979560, test acc 0.781400, took 21.0 sec.\n",
      "Epoch 71: T.cost 0.066988, Train acc 0.978500, test acc 0.771900, took 17.4 sec.\n",
      "Epoch 72: T.cost 0.064538, Train acc 0.978500, test acc 0.778800, took 14.3 sec.\n",
      "Epoch 73: T.cost 0.060719, Train acc 0.979760, test acc 0.779700, took 21.1 sec.\n",
      "Epoch 74: T.cost 0.058413, Train acc 0.979540, test acc 0.776300, took 20.4 sec.\n",
      "Epoch 75: T.cost 0.060933, Train acc 0.980100, test acc 0.776800, took 21.1 sec.\n",
      "Epoch 76: T.cost 0.061035, Train acc 0.978680, test acc 0.774000, took 20.6 sec.\n",
      "Epoch 77: T.cost 0.056260, Train acc 0.981200, test acc 0.777800, took 21.1 sec.\n",
      "Epoch 78: T.cost 0.058368, Train acc 0.981420, test acc 0.781600, took 20.4 sec.\n",
      "New LR: 0.000960596131627\n",
      "Epoch 79: T.cost 0.060260, Train acc 0.980420, test acc 0.777100, took 20.4 sec.\n",
      "Epoch 80: T.cost 0.056923, Train acc 0.981400, test acc 0.779900, took 20.8 sec.\n",
      "Epoch 81: T.cost 0.056145, Train acc 0.981620, test acc 0.775900, took 20.3 sec.\n",
      "Epoch 82: T.cost 0.058883, Train acc 0.981000, test acc 0.779100, took 20.9 sec.\n",
      "Epoch 83: T.cost 0.053379, Train acc 0.983120, test acc 0.780100, took 20.3 sec.\n",
      "Epoch 84: T.cost 0.053516, Train acc 0.982160, test acc 0.771200, took 20.8 sec.\n",
      "Epoch 85: T.cost 0.053219, Train acc 0.982560, test acc 0.780100, took 20.1 sec.\n",
      "Epoch 86: T.cost 0.054261, Train acc 0.981700, test acc 0.781300, took 21.0 sec.\n",
      "Epoch 87: T.cost 0.055699, Train acc 0.980860, test acc 0.781800, took 20.3 sec.\n",
      "Epoch 88: T.cost 0.055048, Train acc 0.981480, test acc 0.785500, took 20.9 sec.\n",
      "Epoch 89: T.cost 0.049206, Train acc 0.984000, test acc 0.784700, took 20.3 sec.\n",
      "Epoch 90: T.cost 0.049108, Train acc 0.983460, test acc 0.777700, took 20.9 sec.\n",
      "Epoch 91: T.cost 0.049395, Train acc 0.983620, test acc 0.780600, took 12.5 sec.\n",
      "Epoch 92: T.cost 0.057603, Train acc 0.980720, test acc 0.781100, took 18.8 sec.\n",
      "Epoch 93: T.cost 0.048293, Train acc 0.983820, test acc 0.773300, took 21.0 sec.\n",
      "Epoch 94: T.cost 0.047193, Train acc 0.984780, test acc 0.780600, took 20.1 sec.\n",
      "Epoch 95: T.cost 0.048654, Train acc 0.984080, test acc 0.784000, took 20.3 sec.\n",
      "Epoch 96: T.cost 0.047187, Train acc 0.984920, test acc 0.783700, took 21.0 sec.\n",
      "Epoch 97: T.cost 0.050246, Train acc 0.983560, test acc 0.776300, took 20.3 sec.\n",
      "Epoch 98: T.cost 0.046670, Train acc 0.984900, test acc 0.782000, took 21.0 sec.\n",
      "New LR: 0.000950990177225\n",
      "Epoch 99: T.cost 0.050056, Train acc 0.982980, test acc 0.774700, took 20.2 sec.\n",
      "Epoch 100: T.cost 0.049499, Train acc 0.983100, test acc 0.779900, took 21.1 sec.\n",
      "Epoch 101: T.cost 0.048061, Train acc 0.984700, test acc 0.782800, took 20.3 sec.\n",
      "Epoch 102: T.cost 0.043662, Train acc 0.985660, test acc 0.784000, took 21.0 sec.\n",
      "Epoch 103: T.cost 0.044697, Train acc 0.986220, test acc 0.780900, took 20.1 sec.\n",
      "Epoch 104: T.cost 0.044766, Train acc 0.985900, test acc 0.776700, took 21.2 sec.\n",
      "Epoch 105: T.cost 0.048889, Train acc 0.983300, test acc 0.782400, took 20.1 sec.\n",
      "Epoch 106: T.cost 0.045272, Train acc 0.985540, test acc 0.775500, took 20.2 sec.\n",
      "Epoch 107: T.cost 0.046340, Train acc 0.984920, test acc 0.779500, took 21.0 sec.\n",
      "Epoch 108: T.cost 0.044514, Train acc 0.985720, test acc 0.780600, took 20.1 sec.\n",
      "Epoch 109: T.cost 0.040491, Train acc 0.986800, test acc 0.780200, took 21.0 sec.\n",
      "Epoch 110: T.cost 0.042914, Train acc 0.986160, test acc 0.782400, took 17.2 sec.\n",
      "Epoch 111: T.cost 0.042840, Train acc 0.986040, test acc 0.783200, took 13.6 sec.\n",
      "Epoch 112: T.cost 0.045134, Train acc 0.985260, test acc 0.782000, took 21.0 sec.\n",
      "Epoch 113: T.cost 0.043543, Train acc 0.986340, test acc 0.780100, took 20.1 sec.\n",
      "Epoch 114: T.cost 0.043423, Train acc 0.985640, test acc 0.783800, took 20.9 sec.\n",
      "Epoch 115: T.cost 0.043399, Train acc 0.986240, test acc 0.782300, took 20.0 sec.\n",
      "Epoch 116: T.cost 0.040153, Train acc 0.986640, test acc 0.785200, took 21.0 sec.\n",
      "Epoch 117: T.cost 0.037465, Train acc 0.987520, test acc 0.784400, took 20.2 sec.\n",
      "Epoch 118: T.cost 0.043466, Train acc 0.985400, test acc 0.777800, took 21.0 sec.\n",
      "New LR: 0.000941480284673\n",
      "Epoch 119: T.cost 0.038765, Train acc 0.987660, test acc 0.778200, took 20.1 sec.\n",
      "Epoch 120: T.cost 0.043610, Train acc 0.985780, test acc 0.783500, took 21.2 sec.\n",
      "Epoch 121: T.cost 0.037083, Train acc 0.987600, test acc 0.785000, took 19.9 sec.\n",
      "Epoch 122: T.cost 0.040676, Train acc 0.987260, test acc 0.782000, took 20.3 sec.\n",
      "Epoch 123: T.cost 0.042104, Train acc 0.986000, test acc 0.784200, took 21.1 sec.\n",
      "Epoch 124: T.cost 0.039633, Train acc 0.986880, test acc 0.774100, took 20.1 sec.\n",
      "Epoch 125: T.cost 0.036723, Train acc 0.987500, test acc 0.777500, took 21.1 sec.\n",
      "Epoch 126: T.cost 0.037141, Train acc 0.988040, test acc 0.784200, took 20.2 sec.\n",
      "Epoch 127: T.cost 0.036253, Train acc 0.988420, test acc 0.785400, took 20.7 sec.\n",
      "Epoch 128: T.cost 0.037192, Train acc 0.988040, test acc 0.789500, took 20.1 sec.\n",
      "Epoch 129: T.cost 0.034585, Train acc 0.989040, test acc 0.784500, took 21.1 sec.\n",
      "Epoch 130: T.cost 0.035737, Train acc 0.988520, test acc 0.784100, took 14.8 sec.\n",
      "Epoch 131: T.cost 0.040460, Train acc 0.986640, test acc 0.784000, took 16.9 sec.\n",
      "Epoch 132: T.cost 0.035924, Train acc 0.988020, test acc 0.785800, took 21.1 sec.\n",
      "Epoch 133: T.cost 0.035026, Train acc 0.988480, test acc 0.783700, took 20.2 sec.\n",
      "Epoch 134: T.cost 0.041123, Train acc 0.986740, test acc 0.776800, took 21.5 sec.\n",
      "Epoch 135: T.cost 0.037113, Train acc 0.987680, test acc 0.781700, took 19.9 sec.\n",
      "Epoch 136: T.cost 0.034435, Train acc 0.988780, test acc 0.784100, took 20.2 sec.\n",
      "Epoch 137: T.cost 0.038053, Train acc 0.987560, test acc 0.788100, took 21.2 sec.\n",
      "Epoch 138: T.cost 0.039658, Train acc 0.987100, test acc 0.782200, took 20.1 sec.\n",
      "New LR: 0.000932065474335\n",
      "Epoch 139: T.cost 0.039473, Train acc 0.986900, test acc 0.782600, took 21.2 sec.\n",
      "Epoch 140: T.cost 0.038192, Train acc 0.987780, test acc 0.782700, took 20.0 sec.\n",
      "Epoch 141: T.cost 0.035712, Train acc 0.988520, test acc 0.784300, took 21.0 sec.\n",
      "Epoch 142: T.cost 0.034410, Train acc 0.988720, test acc 0.779900, took 20.0 sec.\n",
      "Epoch 143: T.cost 0.034758, Train acc 0.989020, test acc 0.782400, took 21.1 sec.\n",
      "Epoch 144: T.cost 0.036357, Train acc 0.988400, test acc 0.788400, took 19.9 sec.\n",
      "Epoch 145: T.cost 0.035446, Train acc 0.988460, test acc 0.783600, took 21.2 sec.\n",
      "Epoch 146: T.cost 0.030482, Train acc 0.990140, test acc 0.783700, took 19.8 sec.\n",
      "Epoch 147: T.cost 0.032231, Train acc 0.989820, test acc 0.780200, took 20.9 sec.\n",
      "Epoch 148: T.cost 0.028933, Train acc 0.990640, test acc 0.785900, took 20.1 sec.\n",
      "Epoch 149: T.cost 0.035410, Train acc 0.988600, test acc 0.785100, took 19.8 sec.\n",
      "Epoch 150: T.cost 0.030111, Train acc 0.990460, test acc 0.790200, took 12.5 sec.\n",
      "Epoch 151: T.cost 0.034657, Train acc 0.988820, test acc 0.784500, took 21.3 sec.\n",
      "Epoch 152: T.cost 0.040641, Train acc 0.986540, test acc 0.778800, took 21.4 sec.\n",
      "Epoch 153: T.cost 0.030996, Train acc 0.990200, test acc 0.784400, took 22.7 sec.\n",
      "Epoch 154: T.cost 0.031058, Train acc 0.990020, test acc 0.788100, took 21.4 sec.\n",
      "Epoch 155: T.cost 0.029124, Train acc 0.990780, test acc 0.789300, took 21.3 sec.\n",
      "Epoch 156: T.cost 0.033717, Train acc 0.989360, test acc 0.786500, took 22.8 sec.\n",
      "Epoch 157: T.cost 0.032604, Train acc 0.989360, test acc 0.781400, took 21.4 sec.\n",
      "Epoch 158: T.cost 0.030856, Train acc 0.989860, test acc 0.787400, took 21.8 sec.\n",
      "New LR: 0.000922744824202\n",
      "Epoch 159: T.cost 0.030934, Train acc 0.989460, test acc 0.782100, took 22.5 sec.\n",
      "Epoch 160: T.cost 0.033773, Train acc 0.988600, test acc 0.781000, took 21.4 sec.\n",
      "Epoch 161: T.cost 0.032770, Train acc 0.989440, test acc 0.782200, took 22.8 sec.\n",
      "Epoch 162: T.cost 0.031661, Train acc 0.990060, test acc 0.781600, took 21.3 sec.\n",
      "Epoch 163: T.cost 0.030598, Train acc 0.990460, test acc 0.778000, took 21.3 sec.\n",
      "Epoch 164: T.cost 0.031531, Train acc 0.989740, test acc 0.777200, took 22.8 sec.\n",
      "Epoch 165: T.cost 0.031453, Train acc 0.989480, test acc 0.780800, took 21.3 sec.\n",
      "Epoch 166: T.cost 0.034415, Train acc 0.989460, test acc 0.783100, took 20.7 sec.\n",
      "Epoch 167: T.cost 0.027381, Train acc 0.990480, test acc 0.780200, took 12.5 sec.\n",
      "Epoch 168: T.cost 0.032438, Train acc 0.990080, test acc 0.780900, took 22.3 sec.\n",
      "Epoch 169: T.cost 0.033598, Train acc 0.989200, test acc 0.787100, took 21.3 sec.\n",
      "Epoch 170: T.cost 0.027969, Train acc 0.991180, test acc 0.782800, took 22.7 sec.\n",
      "Epoch 171: T.cost 0.028962, Train acc 0.991140, test acc 0.775100, took 21.2 sec.\n",
      "Epoch 172: T.cost 0.033207, Train acc 0.989260, test acc 0.781400, took 21.1 sec.\n",
      "Epoch 173: T.cost 0.034631, Train acc 0.989020, test acc 0.784500, took 22.7 sec.\n",
      "Epoch 174: T.cost 0.027629, Train acc 0.991320, test acc 0.789000, took 21.2 sec.\n",
      "Epoch 175: T.cost 0.027137, Train acc 0.991260, test acc 0.782900, took 21.7 sec.\n",
      "Epoch 176: T.cost 0.033482, Train acc 0.989820, test acc 0.781200, took 22.3 sec.\n",
      "Epoch 177: T.cost 0.027057, Train acc 0.991400, test acc 0.779700, took 21.2 sec.\n",
      "Epoch 178: T.cost 0.030117, Train acc 0.990600, test acc 0.780600, took 22.7 sec.\n",
      "New LR: 0.000913517354638\n",
      "Epoch 179: T.cost 0.028757, Train acc 0.990380, test acc 0.776600, took 21.2 sec.\n",
      "Epoch 180: T.cost 0.032479, Train acc 0.989820, test acc 0.782500, took 21.2 sec.\n",
      "Epoch 181: T.cost 0.025788, Train acc 0.991620, test acc 0.785200, took 22.8 sec.\n",
      "Epoch 182: T.cost 0.026657, Train acc 0.991060, test acc 0.788500, took 21.1 sec.\n",
      "Epoch 183: T.cost 0.027048, Train acc 0.991020, test acc 0.785300, took 21.7 sec.\n",
      "Epoch 184: T.cost 0.029057, Train acc 0.990700, test acc 0.786900, took 12.5 sec.\n",
      "Epoch 185: T.cost 0.026098, Train acc 0.991740, test acc 0.786900, took 21.8 sec.\n",
      "Epoch 186: T.cost 0.033547, Train acc 0.989620, test acc 0.784100, took 21.1 sec.\n",
      "Epoch 187: T.cost 0.025982, Train acc 0.991760, test acc 0.785100, took 22.8 sec.\n",
      "Epoch 188: T.cost 0.028288, Train acc 0.991380, test acc 0.778700, took 21.0 sec.\n",
      "Epoch 189: T.cost 0.026626, Train acc 0.991540, test acc 0.780500, took 20.9 sec.\n",
      "Epoch 190: T.cost 0.025373, Train acc 0.991480, test acc 0.783800, took 22.6 sec.\n",
      "Epoch 191: T.cost 0.026656, Train acc 0.991760, test acc 0.780400, took 21.0 sec.\n",
      "Epoch 192: T.cost 0.028567, Train acc 0.991000, test acc 0.786300, took 22.8 sec.\n",
      "Epoch 193: T.cost 0.026163, Train acc 0.992040, test acc 0.778300, took 20.9 sec.\n",
      "Epoch 194: T.cost 0.025454, Train acc 0.991840, test acc 0.784800, took 20.9 sec.\n",
      "Epoch 195: T.cost 0.025000, Train acc 0.992320, test acc 0.779600, took 22.8 sec.\n",
      "Epoch 196: T.cost 0.029058, Train acc 0.991300, test acc 0.776600, took 21.0 sec.\n",
      "Epoch 197: T.cost 0.028123, Train acc 0.991260, test acc 0.785800, took 21.6 sec.\n",
      "Epoch 198: T.cost 0.024572, Train acc 0.992220, test acc 0.782900, took 22.3 sec.\n",
      "New LR: 0.000904382201261\n",
      "Epoch 199: T.cost 0.028113, Train acc 0.991580, test acc 0.785100, took 20.9 sec.\n",
      "Epoch 200: T.cost 0.025164, Train acc 0.991820, test acc 0.783700, took 22.8 sec.\n",
      "Epoch 201: T.cost 0.026478, Train acc 0.991760, test acc 0.779500, took 13.7 sec.\n",
      "Epoch 202: T.cost 0.028639, Train acc 0.991020, test acc 0.784100, took 18.8 sec.\n",
      "Epoch 203: T.cost 0.028353, Train acc 0.991400, test acc 0.784700, took 21.1 sec.\n",
      "Epoch 204: T.cost 0.025274, Train acc 0.991960, test acc 0.786400, took 22.7 sec.\n",
      "Epoch 205: T.cost 0.029018, Train acc 0.990700, test acc 0.781400, took 20.9 sec.\n",
      "Epoch 206: T.cost 0.027706, Train acc 0.991220, test acc 0.783500, took 22.8 sec.\n",
      "Epoch 207: T.cost 0.026534, Train acc 0.991320, test acc 0.791300, took 20.9 sec.\n",
      "Epoch 208: T.cost 0.029922, Train acc 0.990340, test acc 0.786700, took 20.8 sec.\n",
      "Epoch 209: T.cost 0.026639, Train acc 0.992060, test acc 0.783300, took 22.8 sec.\n",
      "Epoch 210: T.cost 0.023135, Train acc 0.993160, test acc 0.784300, took 20.8 sec.\n",
      "Epoch 211: T.cost 0.026551, Train acc 0.991820, test acc 0.784800, took 22.5 sec.\n",
      "Epoch 212: T.cost 0.026868, Train acc 0.991860, test acc 0.786900, took 21.4 sec.\n",
      "Epoch 213: T.cost 0.022884, Train acc 0.992460, test acc 0.778700, took 20.8 sec.\n",
      "Epoch 214: T.cost 0.021900, Train acc 0.992920, test acc 0.785900, took 22.8 sec.\n",
      "Epoch 215: T.cost 0.024670, Train acc 0.992120, test acc 0.783900, took 20.8 sec.\n",
      "Epoch 216: T.cost 0.025845, Train acc 0.992200, test acc 0.789600, took 21.4 sec.\n",
      "Epoch 217: T.cost 0.024683, Train acc 0.992080, test acc 0.782600, took 22.4 sec.\n",
      "Epoch 218: T.cost 0.024406, Train acc 0.992280, test acc 0.778100, took 17.5 sec.\n",
      "New LR: 0.000895338384435\n",
      "Epoch 219: T.cost 0.027131, Train acc 0.991520, test acc 0.779600, took 14.8 sec.\n",
      "Epoch 220: T.cost 0.027438, Train acc 0.991520, test acc 0.785900, took 22.7 sec.\n",
      "Epoch 221: T.cost 0.026510, Train acc 0.991640, test acc 0.784800, took 20.8 sec.\n",
      "Epoch 222: T.cost 0.025739, Train acc 0.991660, test acc 0.789600, took 21.4 sec.\n",
      "Epoch 223: T.cost 0.025557, Train acc 0.992120, test acc 0.783500, took 22.4 sec.\n",
      "Epoch 224: T.cost 0.025960, Train acc 0.991800, test acc 0.784300, took 20.6 sec.\n",
      "Epoch 225: T.cost 0.024806, Train acc 0.992720, test acc 0.776300, took 22.8 sec.\n",
      "Epoch 226: T.cost 0.025898, Train acc 0.991960, test acc 0.786100, took 20.7 sec.\n",
      "Epoch 227: T.cost 0.023820, Train acc 0.992620, test acc 0.783200, took 20.9 sec.\n",
      "Epoch 228: T.cost 0.020149, Train acc 0.993480, test acc 0.785400, took 22.8 sec.\n",
      "Epoch 229: T.cost 0.026528, Train acc 0.991500, test acc 0.786900, took 20.6 sec.\n",
      "Epoch 230: T.cost 0.023885, Train acc 0.992380, test acc 0.782800, took 22.8 sec.\n",
      "Epoch 231: T.cost 0.023062, Train acc 0.992660, test acc 0.780300, took 20.6 sec.\n",
      "Epoch 232: T.cost 0.022129, Train acc 0.993080, test acc 0.787200, took 20.6 sec.\n",
      "Epoch 233: T.cost 0.021678, Train acc 0.993260, test acc 0.788400, took 22.7 sec.\n",
      "Epoch 234: T.cost 0.024963, Train acc 0.991760, test acc 0.787300, took 20.5 sec.\n",
      "Epoch 235: T.cost 0.025246, Train acc 0.992300, test acc 0.784400, took 22.7 sec.\n",
      "Epoch 236: T.cost 0.021402, Train acc 0.993020, test acc 0.784600, took 13.1 sec.\n",
      "Epoch 237: T.cost 0.024855, Train acc 0.991920, test acc 0.785700, took 18.6 sec.\n",
      "Epoch 238: T.cost 0.022684, Train acc 0.992880, test acc 0.783100, took 21.2 sec.\n",
      "New LR: 0.00088638498215\n",
      "Epoch 239: T.cost 0.028091, Train acc 0.991580, test acc 0.785400, took 22.3 sec.\n",
      "Epoch 240: T.cost 0.022082, Train acc 0.992920, test acc 0.785800, took 20.7 sec.\n",
      "Epoch 241: T.cost 0.022558, Train acc 0.992580, test acc 0.787300, took 22.8 sec.\n",
      "Epoch 242: T.cost 0.022126, Train acc 0.992940, test acc 0.783600, took 20.4 sec.\n",
      "Epoch 243: T.cost 0.022679, Train acc 0.993180, test acc 0.784800, took 20.9 sec.\n",
      "Epoch 244: T.cost 0.024554, Train acc 0.992000, test acc 0.786800, took 22.4 sec.\n",
      "Epoch 245: T.cost 0.020533, Train acc 0.993580, test acc 0.787600, took 20.5 sec.\n",
      "Epoch 246: T.cost 0.020538, Train acc 0.993120, test acc 0.787600, took 22.8 sec.\n",
      "Epoch 247: T.cost 0.023819, Train acc 0.992540, test acc 0.785100, took 20.4 sec.\n",
      "Epoch 248: T.cost 0.022908, Train acc 0.992960, test acc 0.782200, took 20.9 sec.\n",
      "Epoch 249: T.cost 0.022365, Train acc 0.993340, test acc 0.787800, took 22.4 sec.\n",
      "Epoch 250: T.cost 0.024025, Train acc 0.992500, test acc 0.785300, took 20.4 sec.\n",
      "Epoch 251: T.cost 0.024270, Train acc 0.992460, test acc 0.785100, took 22.7 sec.\n",
      "Epoch 252: T.cost 0.019470, Train acc 0.993940, test acc 0.781900, took 20.3 sec.\n",
      "Epoch 253: T.cost 0.022725, Train acc 0.992660, test acc 0.786800, took 21.2 sec.\n",
      "Epoch 254: T.cost 0.024350, Train acc 0.992340, test acc 0.785500, took 12.5 sec.\n",
      "Epoch 255: T.cost 0.024098, Train acc 0.992180, test acc 0.785200, took 21.2 sec.\n",
      "Epoch 256: T.cost 0.021846, Train acc 0.993460, test acc 0.790700, took 20.2 sec.\n",
      "Epoch 257: T.cost 0.023467, Train acc 0.992520, test acc 0.787900, took 22.7 sec.\n",
      "Epoch 258: T.cost 0.023340, Train acc 0.993200, test acc 0.781200, took 20.4 sec.\n",
      "New LR: 0.000877521130024\n",
      "Epoch 259: T.cost 0.021181, Train acc 0.993380, test acc 0.785400, took 22.5 sec.\n",
      "Epoch 260: T.cost 0.022252, Train acc 0.993360, test acc 0.791100, took 21.0 sec.\n",
      "Epoch 261: T.cost 0.021556, Train acc 0.993540, test acc 0.794300, took 20.3 sec.\n",
      "Epoch 262: T.cost 0.024459, Train acc 0.992840, test acc 0.791100, took 22.7 sec.\n",
      "Epoch 263: T.cost 0.020241, Train acc 0.993960, test acc 0.789900, took 20.2 sec.\n",
      "Epoch 264: T.cost 0.021867, Train acc 0.993380, test acc 0.783800, took 22.7 sec.\n",
      "Epoch 265: T.cost 0.022808, Train acc 0.993200, test acc 0.791900, took 20.0 sec.\n",
      "Epoch 266: T.cost 0.023094, Train acc 0.993060, test acc 0.787800, took 20.3 sec.\n",
      "Epoch 267: T.cost 0.017478, Train acc 0.994420, test acc 0.787500, took 22.9 sec.\n",
      "Epoch 268: T.cost 0.021063, Train acc 0.993520, test acc 0.789200, took 20.3 sec.\n",
      "Epoch 269: T.cost 0.021901, Train acc 0.993300, test acc 0.790400, took 22.9 sec.\n",
      "Epoch 270: T.cost 0.023907, Train acc 0.992400, test acc 0.786500, took 20.3 sec.\n",
      "Epoch 271: T.cost 0.021035, Train acc 0.993180, test acc 0.791600, took 20.3 sec.\n",
      "Epoch 272: T.cost 0.019931, Train acc 0.993800, test acc 0.784700, took 12.5 sec.\n",
      "Epoch 273: T.cost 0.020385, Train acc 0.993800, test acc 0.791400, took 22.5 sec.\n",
      "Epoch 274: T.cost 0.022132, Train acc 0.992960, test acc 0.788900, took 20.2 sec.\n",
      "Epoch 275: T.cost 0.022781, Train acc 0.992840, test acc 0.792200, took 22.8 sec.\n",
      "Epoch 276: T.cost 0.017812, Train acc 0.994360, test acc 0.788900, took 20.3 sec.\n",
      "Epoch 277: T.cost 0.018539, Train acc 0.994280, test acc 0.792600, took 21.2 sec.\n",
      "Epoch 278: T.cost 0.022213, Train acc 0.993160, test acc 0.795400, took 21.8 sec.\n",
      "New LR: 0.000868745906046\n",
      "Epoch 279: T.cost 0.020356, Train acc 0.993680, test acc 0.784500, took 20.2 sec.\n",
      "Epoch 280: T.cost 0.019807, Train acc 0.993620, test acc 0.786000, took 22.7 sec.\n",
      "Epoch 281: T.cost 0.024167, Train acc 0.992340, test acc 0.784500, took 20.2 sec.\n",
      "Epoch 282: T.cost 0.019304, Train acc 0.993900, test acc 0.785500, took 22.0 sec.\n",
      "Epoch 283: T.cost 0.021627, Train acc 0.993420, test acc 0.781400, took 21.2 sec.\n",
      "Epoch 284: T.cost 0.022631, Train acc 0.992840, test acc 0.795300, took 20.0 sec.\n",
      "Epoch 285: T.cost 0.017064, Train acc 0.994540, test acc 0.786400, took 22.8 sec.\n",
      "Epoch 286: T.cost 0.022848, Train acc 0.992960, test acc 0.778600, took 19.9 sec.\n",
      "Epoch 287: T.cost 0.019987, Train acc 0.993480, test acc 0.792300, took 22.9 sec.\n",
      "Epoch 288: T.cost 0.020993, Train acc 0.993440, test acc 0.788800, took 19.8 sec.\n",
      "Epoch 289: T.cost 0.020412, Train acc 0.993620, test acc 0.789200, took 20.1 sec.\n",
      "Epoch 290: T.cost 0.020166, Train acc 0.994180, test acc 0.789500, took 12.5 sec.\n",
      "Epoch 291: T.cost 0.019853, Train acc 0.993680, test acc 0.783400, took 22.4 sec.\n",
      "Epoch 292: T.cost 0.021905, Train acc 0.993240, test acc 0.788400, took 19.6 sec.\n",
      "Epoch 293: T.cost 0.019604, Train acc 0.994060, test acc 0.788100, took 22.8 sec.\n",
      "Epoch 294: T.cost 0.019246, Train acc 0.994040, test acc 0.788600, took 19.6 sec.\n",
      "Epoch 295: T.cost 0.021230, Train acc 0.993500, test acc 0.785200, took 22.5 sec.\n",
      "Epoch 296: T.cost 0.021538, Train acc 0.993180, test acc 0.787500, took 19.7 sec.\n",
      "Epoch 297: T.cost 0.020058, Train acc 0.993960, test acc 0.783400, took 20.2 sec.\n",
      "Epoch 298: T.cost 0.022239, Train acc 0.993300, test acc 0.784800, took 22.1 sec.\n",
      "New LR: 0.000860058445833\n",
      "Epoch 299: T.cost 0.021406, Train acc 0.993440, test acc 0.782100, took 19.1 sec.\n",
      "Epoch 300: T.cost 0.018677, Train acc 0.993900, test acc 0.785500, took 22.8 sec.\n",
      "Epoch 301: T.cost 0.021959, Train acc 0.993320, test acc 0.783200, took 18.9 sec.\n",
      "Epoch 302: T.cost 0.018687, Train acc 0.993840, test acc 0.785600, took 22.8 sec.\n",
      "Epoch 303: T.cost 0.020011, Train acc 0.994100, test acc 0.785100, took 19.1 sec.\n",
      "Epoch 304: T.cost 0.017537, Train acc 0.994500, test acc 0.788900, took 22.0 sec.\n",
      "Epoch 305: T.cost 0.019782, Train acc 0.993400, test acc 0.790100, took 20.1 sec.\n",
      "Epoch 306: T.cost 0.018766, Train acc 0.994160, test acc 0.790000, took 19.7 sec.\n",
      "Epoch 307: T.cost 0.019924, Train acc 0.994240, test acc 0.785900, took 21.9 sec.\n",
      "Epoch 308: T.cost 0.019810, Train acc 0.993820, test acc 0.789600, took 17.9 sec.\n",
      "Epoch 309: T.cost 0.017862, Train acc 0.994400, test acc 0.786600, took 13.0 sec.\n",
      "Epoch 310: T.cost 0.019124, Train acc 0.993740, test acc 0.787600, took 22.8 sec.\n",
      "Epoch 311: T.cost 0.021808, Train acc 0.993520, test acc 0.785300, took 18.7 sec.\n",
      "Epoch 312: T.cost 0.019782, Train acc 0.994240, test acc 0.787200, took 22.7 sec.\n",
      "Epoch 313: T.cost 0.018358, Train acc 0.994720, test acc 0.790400, took 18.4 sec.\n",
      "Epoch 314: T.cost 0.017844, Train acc 0.994520, test acc 0.786200, took 22.7 sec.\n",
      "Epoch 315: T.cost 0.020049, Train acc 0.994040, test acc 0.783800, took 18.4 sec.\n",
      "Epoch 316: T.cost 0.018835, Train acc 0.994460, test acc 0.782500, took 22.4 sec.\n",
      "Epoch 317: T.cost 0.022311, Train acc 0.992960, test acc 0.787300, took 18.9 sec.\n",
      "Epoch 318: T.cost 0.017225, Train acc 0.994660, test acc 0.786700, took 20.9 sec.\n",
      "New LR: 0.000851457885001\n",
      "Epoch 319: T.cost 0.019865, Train acc 0.993960, test acc 0.785000, took 20.2 sec.\n",
      "Epoch 320: T.cost 0.018456, Train acc 0.994080, test acc 0.788800, took 19.7 sec.\n",
      "Epoch 321: T.cost 0.021082, Train acc 0.993400, test acc 0.785900, took 21.4 sec.\n",
      "Epoch 322: T.cost 0.020157, Train acc 0.993640, test acc 0.792200, took 18.4 sec.\n",
      "Epoch 323: T.cost 0.017187, Train acc 0.994500, test acc 0.789600, took 22.7 sec.\n",
      "Epoch 324: T.cost 0.019033, Train acc 0.994040, test acc 0.788000, took 17.8 sec.\n",
      "Epoch 325: T.cost 0.019881, Train acc 0.994040, test acc 0.789000, took 22.8 sec.\n",
      "Epoch 326: T.cost 0.019928, Train acc 0.994000, test acc 0.789100, took 17.8 sec.\n",
      "Epoch 327: T.cost 0.018632, Train acc 0.994120, test acc 0.788300, took 22.8 sec.\n",
      "Epoch 328: T.cost 0.016359, Train acc 0.995160, test acc 0.787600, took 15.4 sec.\n",
      "Epoch 329: T.cost 0.017829, Train acc 0.994820, test acc 0.787000, took 14.5 sec.\n",
      "Epoch 330: T.cost 0.020661, Train acc 0.993560, test acc 0.789000, took 22.8 sec.\n",
      "Epoch 331: T.cost 0.020788, Train acc 0.993880, test acc 0.788000, took 17.5 sec.\n",
      "Epoch 332: T.cost 0.018268, Train acc 0.994600, test acc 0.783000, took 22.7 sec.\n",
      "Epoch 333: T.cost 0.022238, Train acc 0.993360, test acc 0.790700, took 17.6 sec.\n",
      "Epoch 334: T.cost 0.019633, Train acc 0.994460, test acc 0.785000, took 22.8 sec.\n",
      "Epoch 335: T.cost 0.016325, Train acc 0.995280, test acc 0.789300, took 17.5 sec.\n",
      "Epoch 336: T.cost 0.017499, Train acc 0.994500, test acc 0.780300, took 22.8 sec.\n",
      "Epoch 337: T.cost 0.022199, Train acc 0.993040, test acc 0.784300, took 17.2 sec.\n",
      "Epoch 338: T.cost 0.021779, Train acc 0.993180, test acc 0.780100, took 22.8 sec.\n",
      "New LR: 0.000842943301541\n",
      "Epoch 339: T.cost 0.017824, Train acc 0.994440, test acc 0.787400, took 17.3 sec.\n",
      "Epoch 340: T.cost 0.017630, Train acc 0.994780, test acc 0.789200, took 22.8 sec.\n",
      "Epoch 341: T.cost 0.019620, Train acc 0.994020, test acc 0.790700, took 17.2 sec.\n",
      "Epoch 342: T.cost 0.016498, Train acc 0.994620, test acc 0.788100, took 23.0 sec.\n",
      "Epoch 343: T.cost 0.019201, Train acc 0.994040, test acc 0.788000, took 16.8 sec.\n",
      "Epoch 344: T.cost 0.020095, Train acc 0.994020, test acc 0.787100, took 22.9 sec.\n",
      "Epoch 345: T.cost 0.015012, Train acc 0.995000, test acc 0.788400, took 16.7 sec.\n",
      "Epoch 346: T.cost 0.017572, Train acc 0.994660, test acc 0.786600, took 22.9 sec.\n",
      "Epoch 347: T.cost 0.017010, Train acc 0.994440, test acc 0.788000, took 16.9 sec.\n",
      "Epoch 348: T.cost 0.019951, Train acc 0.993900, test acc 0.789800, took 22.7 sec.\n",
      "Epoch 349: T.cost 0.016935, Train acc 0.994680, test acc 0.787200, took 12.9 sec.\n",
      "Epoch 350: T.cost 0.017682, Train acc 0.994240, test acc 0.788500, took 16.1 sec.\n",
      "Epoch 351: T.cost 0.017681, Train acc 0.994760, test acc 0.786300, took 22.8 sec.\n",
      "Epoch 352: T.cost 0.020270, Train acc 0.993600, test acc 0.787700, took 16.8 sec.\n",
      "Epoch 353: T.cost 0.018146, Train acc 0.994960, test acc 0.784200, took 22.7 sec.\n",
      "Epoch 354: T.cost 0.018160, Train acc 0.994580, test acc 0.786500, took 16.5 sec.\n",
      "Epoch 355: T.cost 0.017786, Train acc 0.994500, test acc 0.786200, took 22.8 sec.\n",
      "Epoch 356: T.cost 0.017763, Train acc 0.994400, test acc 0.787800, took 16.3 sec.\n",
      "Epoch 357: T.cost 0.017259, Train acc 0.994640, test acc 0.784000, took 22.8 sec.\n",
      "Epoch 358: T.cost 0.015243, Train acc 0.995120, test acc 0.784100, took 16.5 sec.\n",
      "New LR: 0.000834513888694\n",
      "Epoch 359: T.cost 0.017958, Train acc 0.994780, test acc 0.787600, took 22.8 sec.\n",
      "Epoch 360: T.cost 0.015520, Train acc 0.995340, test acc 0.789100, took 16.2 sec.\n",
      "Epoch 361: T.cost 0.017420, Train acc 0.994940, test acc 0.790800, took 22.7 sec.\n",
      "Epoch 362: T.cost 0.014965, Train acc 0.995420, test acc 0.790200, took 16.2 sec.\n",
      "Epoch 363: T.cost 0.020358, Train acc 0.994200, test acc 0.788300, took 22.6 sec.\n",
      "Epoch 364: T.cost 0.017262, Train acc 0.994820, test acc 0.786500, took 17.3 sec.\n",
      "Epoch 365: T.cost 0.018837, Train acc 0.994620, test acc 0.791400, took 21.9 sec.\n",
      "Epoch 366: T.cost 0.016381, Train acc 0.994820, test acc 0.789800, took 18.0 sec.\n",
      "Epoch 367: T.cost 0.017044, Train acc 0.995060, test acc 0.789300, took 20.8 sec.\n",
      "Epoch 368: T.cost 0.017839, Train acc 0.994400, test acc 0.791300, took 19.0 sec.\n",
      "Epoch 369: T.cost 0.016901, Train acc 0.994800, test acc 0.791000, took 19.7 sec.\n",
      "Epoch 370: T.cost 0.014379, Train acc 0.995580, test acc 0.791500, took 20.1 sec.\n",
      "Epoch 371: T.cost 0.018231, Train acc 0.994600, test acc 0.789100, took 12.5 sec.\n",
      "Epoch 372: T.cost 0.017657, Train acc 0.994800, test acc 0.789400, took 18.4 sec.\n",
      "Epoch 373: T.cost 0.017765, Train acc 0.994760, test acc 0.783400, took 21.5 sec.\n",
      "Epoch 374: T.cost 0.018005, Train acc 0.994600, test acc 0.790100, took 17.0 sec.\n",
      "Epoch 375: T.cost 0.021632, Train acc 0.993580, test acc 0.788500, took 22.8 sec.\n",
      "Epoch 376: T.cost 0.019257, Train acc 0.993980, test acc 0.793600, took 15.4 sec.\n",
      "Epoch 377: T.cost 0.017562, Train acc 0.994380, test acc 0.792300, took 22.9 sec.\n",
      "Epoch 378: T.cost 0.016141, Train acc 0.994880, test acc 0.794900, took 15.5 sec.\n",
      "New LR: 0.000826168724452\n",
      "Epoch 379: T.cost 0.017314, Train acc 0.994860, test acc 0.786300, took 22.8 sec.\n",
      "Epoch 380: T.cost 0.016700, Train acc 0.994760, test acc 0.790500, took 15.6 sec.\n",
      "Epoch 381: T.cost 0.017661, Train acc 0.994900, test acc 0.784100, took 22.7 sec.\n",
      "Epoch 382: T.cost 0.014828, Train acc 0.995600, test acc 0.791600, took 17.0 sec.\n",
      "Epoch 383: T.cost 0.015379, Train acc 0.995380, test acc 0.788100, took 21.2 sec.\n",
      "Epoch 384: T.cost 0.013724, Train acc 0.995860, test acc 0.790000, took 18.6 sec.\n",
      "Epoch 385: T.cost 0.016604, Train acc 0.995040, test acc 0.788800, took 19.7 sec.\n",
      "Epoch 386: T.cost 0.016677, Train acc 0.995060, test acc 0.786900, took 20.2 sec.\n",
      "Epoch 387: T.cost 0.017118, Train acc 0.994900, test acc 0.787900, took 17.9 sec.\n",
      "Epoch 388: T.cost 0.015489, Train acc 0.994620, test acc 0.788200, took 21.9 sec.\n",
      "Epoch 389: T.cost 0.018646, Train acc 0.994360, test acc 0.786700, took 16.0 sec.\n",
      "Epoch 390: T.cost 0.018980, Train acc 0.994180, test acc 0.789500, took 22.7 sec.\n",
      "Epoch 391: T.cost 0.017886, Train acc 0.994340, test acc 0.791900, took 15.0 sec.\n",
      "Epoch 392: T.cost 0.017923, Train acc 0.994540, test acc 0.787800, took 22.9 sec.\n",
      "Epoch 393: T.cost 0.014163, Train acc 0.995520, test acc 0.790500, took 15.4 sec.\n",
      "Epoch 394: T.cost 0.016217, Train acc 0.995020, test acc 0.791200, took 12.5 sec.\n",
      "Epoch 395: T.cost 0.016885, Train acc 0.994960, test acc 0.787400, took 22.5 sec.\n",
      "Epoch 396: T.cost 0.019996, Train acc 0.994200, test acc 0.781600, took 17.4 sec.\n",
      "Epoch 397: T.cost 0.015269, Train acc 0.995360, test acc 0.786200, took 20.3 sec.\n",
      "Epoch 398: T.cost 0.017817, Train acc 0.994640, test acc 0.784900, took 19.5 sec.\n",
      "New LR: 0.000817907059682\n",
      "Epoch 399: T.cost 0.014616, Train acc 0.995840, test acc 0.787200, took 18.2 sec.\n",
      "Epoch 400: T.cost 0.017809, Train acc 0.994780, test acc 0.787100, took 21.6 sec.\n",
      "Epoch 401: T.cost 0.014602, Train acc 0.995560, test acc 0.785200, took 15.9 sec.\n",
      "Epoch 402: T.cost 0.017552, Train acc 0.994980, test acc 0.784100, took 22.9 sec.\n",
      "Epoch 403: T.cost 0.017334, Train acc 0.994980, test acc 0.787600, took 14.3 sec.\n",
      "Epoch 404: T.cost 0.018183, Train acc 0.994920, test acc 0.789500, took 22.8 sec.\n",
      "Epoch 405: T.cost 0.014621, Train acc 0.995520, test acc 0.785300, took 16.1 sec.\n",
      "Epoch 406: T.cost 0.015115, Train acc 0.995540, test acc 0.786700, took 21.3 sec.\n",
      "Epoch 407: T.cost 0.016894, Train acc 0.994840, test acc 0.790400, took 18.5 sec.\n",
      "Epoch 408: T.cost 0.020624, Train acc 0.994180, test acc 0.782200, took 18.5 sec.\n",
      "Epoch 409: T.cost 0.019505, Train acc 0.994380, test acc 0.789000, took 21.4 sec.\n",
      "Epoch 410: T.cost 0.015437, Train acc 0.995420, test acc 0.786700, took 15.6 sec.\n",
      "Epoch 411: T.cost 0.014058, Train acc 0.995780, test acc 0.787100, took 22.9 sec.\n",
      "Epoch 412: T.cost 0.015751, Train acc 0.995200, test acc 0.789900, took 14.0 sec.\n",
      "Epoch 413: T.cost 0.019481, Train acc 0.994440, test acc 0.788700, took 22.9 sec.\n",
      "Epoch 414: T.cost 0.017393, Train acc 0.995360, test acc 0.792500, took 16.7 sec.\n",
      "Epoch 415: T.cost 0.014837, Train acc 0.995360, test acc 0.787400, took 20.2 sec.\n",
      "Epoch 416: T.cost 0.015684, Train acc 0.995400, test acc 0.784700, took 19.6 sec.\n",
      "Epoch 417: T.cost 0.013350, Train acc 0.996060, test acc 0.788800, took 12.5 sec.\n",
      "Epoch 418: T.cost 0.014782, Train acc 0.995300, test acc 0.784000, took 17.3 sec.\n",
      "New LR: 0.000809727972373\n",
      "Epoch 419: T.cost 0.018543, Train acc 0.994260, test acc 0.784300, took 22.5 sec.\n",
      "Epoch 420: T.cost 0.015717, Train acc 0.995320, test acc 0.789900, took 14.1 sec.\n",
      "Epoch 421: T.cost 0.016067, Train acc 0.995380, test acc 0.784800, took 22.8 sec.\n",
      "Epoch 422: T.cost 0.016614, Train acc 0.994940, test acc 0.783400, took 15.3 sec.\n",
      "Epoch 423: T.cost 0.018453, Train acc 0.994280, test acc 0.783000, took 21.4 sec.\n",
      "Epoch 424: T.cost 0.016446, Train acc 0.995340, test acc 0.789300, took 18.4 sec.\n",
      "Epoch 425: T.cost 0.014693, Train acc 0.995340, test acc 0.792800, took 18.1 sec.\n",
      "Epoch 426: T.cost 0.014997, Train acc 0.995560, test acc 0.783400, took 21.7 sec.\n",
      "Epoch 427: T.cost 0.015195, Train acc 0.995320, test acc 0.785200, took 14.6 sec.\n",
      "Epoch 428: T.cost 0.016343, Train acc 0.995320, test acc 0.785800, took 22.9 sec.\n",
      "Epoch 429: T.cost 0.013675, Train acc 0.996180, test acc 0.789900, took 14.8 sec.\n",
      "Epoch 430: T.cost 0.018080, Train acc 0.994320, test acc 0.787200, took 21.6 sec.\n",
      "Epoch 431: T.cost 0.019086, Train acc 0.994860, test acc 0.787500, took 18.2 sec.\n",
      "Epoch 432: T.cost 0.014381, Train acc 0.995260, test acc 0.788300, took 17.9 sec.\n",
      "Epoch 433: T.cost 0.016938, Train acc 0.995340, test acc 0.786500, took 22.0 sec.\n",
      "Epoch 434: T.cost 0.013529, Train acc 0.995560, test acc 0.785000, took 14.1 sec.\n",
      "Epoch 435: T.cost 0.014926, Train acc 0.995500, test acc 0.784100, took 22.9 sec.\n",
      "Epoch 436: T.cost 0.013582, Train acc 0.995720, test acc 0.786700, took 15.3 sec.\n",
      "Epoch 437: T.cost 0.017329, Train acc 0.994880, test acc 0.784700, took 20.4 sec.\n",
      "Epoch 438: T.cost 0.015445, Train acc 0.995140, test acc 0.784800, took 19.4 sec.\n",
      "New LR: 0.000801630713395\n",
      "Epoch 439: T.cost 0.016574, Train acc 0.995160, test acc 0.787900, took 16.3 sec.\n",
      "Epoch 440: T.cost 0.015990, Train acc 0.995560, test acc 0.786800, took 23.0 sec.\n",
      "Epoch 441: T.cost 0.015168, Train acc 0.995660, test acc 0.782700, took 12.9 sec.\n",
      "Epoch 442: T.cost 0.013725, Train acc 0.995780, test acc 0.785400, took 12.8 sec.\n",
      "Epoch 443: T.cost 0.015310, Train acc 0.995120, test acc 0.780300, took 22.8 sec.\n",
      "Epoch 444: T.cost 0.014558, Train acc 0.995560, test acc 0.786000, took 16.7 sec.\n",
      "Epoch 445: T.cost 0.015506, Train acc 0.995360, test acc 0.784200, took 19.2 sec.\n",
      "Epoch 446: T.cost 0.011822, Train acc 0.996260, test acc 0.783100, took 20.6 sec.\n",
      "Epoch 447: T.cost 0.016296, Train acc 0.995120, test acc 0.787100, took 14.9 sec.\n",
      "Epoch 448: T.cost 0.016200, Train acc 0.995480, test acc 0.789600, took 23.0 sec.\n",
      "Epoch 449: T.cost 0.018944, Train acc 0.994540, test acc 0.789800, took 14.3 sec.\n",
      "Epoch 450: T.cost 0.014611, Train acc 0.995440, test acc 0.787900, took 21.4 sec.\n",
      "Epoch 451: T.cost 0.018283, Train acc 0.994900, test acc 0.786900, took 18.4 sec.\n",
      "Epoch 452: T.cost 0.014608, Train acc 0.995560, test acc 0.783300, took 17.0 sec.\n",
      "Epoch 453: T.cost 0.017100, Train acc 0.995280, test acc 0.787400, took 22.8 sec.\n",
      "Epoch 454: T.cost 0.012587, Train acc 0.995660, test acc 0.785800, took 12.6 sec.\n",
      "Epoch 455: T.cost 0.015085, Train acc 0.995500, test acc 0.785300, took 22.9 sec.\n",
      "Epoch 456: T.cost 0.014303, Train acc 0.995680, test acc 0.784500, took 16.8 sec.\n",
      "Epoch 457: T.cost 0.014501, Train acc 0.996040, test acc 0.790000, took 18.6 sec.\n",
      "Epoch 458: T.cost 0.014731, Train acc 0.995740, test acc 0.787800, took 21.2 sec.\n",
      "New LR: 0.000793614418362\n",
      "Epoch 459: T.cost 0.017544, Train acc 0.994660, test acc 0.787100, took 14.0 sec.\n",
      "Epoch 460: T.cost 0.012946, Train acc 0.996000, test acc 0.790000, took 23.1 sec.\n",
      "Epoch 461: T.cost 0.015357, Train acc 0.995320, test acc 0.786300, took 15.1 sec.\n",
      "Epoch 462: T.cost 0.014330, Train acc 0.995820, test acc 0.786900, took 19.5 sec.\n",
      "Epoch 463: T.cost 0.015736, Train acc 0.995440, test acc 0.786600, took 20.4 sec.\n",
      "Epoch 464: T.cost 0.015655, Train acc 0.995480, test acc 0.788400, took 14.7 sec.\n",
      "Epoch 465: T.cost 0.017296, Train acc 0.995320, test acc 0.787800, took 23.1 sec.\n",
      "Epoch 466: T.cost 0.013514, Train acc 0.995660, test acc 0.790600, took 14.5 sec.\n",
      "Epoch 467: T.cost 0.016528, Train acc 0.994900, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 468: T.cost 0.014283, Train acc 0.995900, test acc 0.789500, took 21.4 sec.\n",
      "Epoch 469: T.cost 0.013579, Train acc 0.995820, test acc 0.794000, took 18.4 sec.\n",
      "Epoch 470: T.cost 0.014728, Train acc 0.995540, test acc 0.785400, took 15.9 sec.\n",
      "Epoch 471: T.cost 0.017967, Train acc 0.994700, test acc 0.781600, took 22.9 sec.\n",
      "Epoch 472: T.cost 0.016086, Train acc 0.995460, test acc 0.790400, took 13.4 sec.\n",
      "Epoch 473: T.cost 0.014513, Train acc 0.995680, test acc 0.789200, took 21.4 sec.\n",
      "Epoch 474: T.cost 0.011749, Train acc 0.996300, test acc 0.788900, took 18.5 sec.\n",
      "Epoch 475: T.cost 0.014778, Train acc 0.995460, test acc 0.788500, took 16.0 sec.\n",
      "Epoch 476: T.cost 0.015526, Train acc 0.995680, test acc 0.789700, took 23.0 sec.\n",
      "Epoch 477: T.cost 0.012542, Train acc 0.996240, test acc 0.791100, took 13.2 sec.\n",
      "Epoch 478: T.cost 0.014621, Train acc 0.995680, test acc 0.785700, took 20.7 sec.\n",
      "New LR: 0.000785678280517\n",
      "Epoch 479: T.cost 0.015385, Train acc 0.995520, test acc 0.788000, took 19.1 sec.\n",
      "Epoch 480: T.cost 0.016492, Train acc 0.995580, test acc 0.789700, took 15.2 sec.\n",
      "Epoch 481: T.cost 0.013724, Train acc 0.996080, test acc 0.787200, took 23.0 sec.\n",
      "Epoch 482: T.cost 0.015227, Train acc 0.995380, test acc 0.789500, took 14.0 sec.\n",
      "Epoch 483: T.cost 0.015787, Train acc 0.995380, test acc 0.780100, took 20.2 sec.\n",
      "Epoch 484: T.cost 0.014332, Train acc 0.995520, test acc 0.784100, took 19.6 sec.\n",
      "Epoch 485: T.cost 0.014538, Train acc 0.996000, test acc 0.787000, took 14.3 sec.\n",
      "Epoch 486: T.cost 0.011024, Train acc 0.996460, test acc 0.791700, took 22.9 sec.\n",
      "Epoch 487: T.cost 0.014786, Train acc 0.995800, test acc 0.786500, took 15.0 sec.\n",
      "Epoch 488: T.cost 0.015209, Train acc 0.995820, test acc 0.791900, took 19.0 sec.\n",
      "Epoch 489: T.cost 0.013991, Train acc 0.995760, test acc 0.787000, took 20.9 sec.\n",
      "Epoch 490: T.cost 0.011827, Train acc 0.996120, test acc 0.789000, took 13.4 sec.\n",
      "Epoch 491: T.cost 0.011685, Train acc 0.996440, test acc 0.789000, took 23.1 sec.\n",
      "Epoch 492: T.cost 0.014971, Train acc 0.995700, test acc 0.791300, took 15.8 sec.\n",
      "Epoch 493: T.cost 0.015908, Train acc 0.995200, test acc 0.792000, took 12.5 sec.\n",
      "Epoch 494: T.cost 0.013388, Train acc 0.995760, test acc 0.788800, took 18.5 sec.\n",
      "Epoch 495: T.cost 0.015528, Train acc 0.995500, test acc 0.789200, took 21.3 sec.\n",
      "Epoch 496: T.cost 0.014824, Train acc 0.995720, test acc 0.788600, took 12.9 sec.\n",
      "Epoch 497: T.cost 0.014738, Train acc 0.995900, test acc 0.790000, took 22.9 sec.\n",
      "Epoch 498: T.cost 0.011379, Train acc 0.996400, test acc 0.785600, took 16.4 sec.\n",
      "New LR: 0.000777821493102\n",
      "Epoch 499: T.cost 0.015415, Train acc 0.995620, test acc 0.786900, took 17.3 sec.\n",
      "Epoch 500: T.cost 0.012446, Train acc 0.996100, test acc 0.787600, took 22.5 sec.\n",
      "Epoch 501: T.cost 0.017046, Train acc 0.994960, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 502: T.cost 0.015500, Train acc 0.995200, test acc 0.787200, took 22.2 sec.\n",
      "Epoch 503: T.cost 0.013827, Train acc 0.995880, test acc 0.793300, took 17.7 sec.\n",
      "Epoch 504: T.cost 0.012485, Train acc 0.996180, test acc 0.787300, took 16.5 sec.\n",
      "Epoch 505: T.cost 0.013669, Train acc 0.996080, test acc 0.788000, took 23.0 sec.\n",
      "Epoch 506: T.cost 0.011054, Train acc 0.996840, test acc 0.787300, took 12.8 sec.\n",
      "Epoch 507: T.cost 0.014041, Train acc 0.995880, test acc 0.785500, took 20.9 sec.\n",
      "Epoch 508: T.cost 0.013262, Train acc 0.996160, test acc 0.791400, took 18.9 sec.\n",
      "Epoch 509: T.cost 0.013533, Train acc 0.996040, test acc 0.789500, took 14.4 sec.\n",
      "Epoch 510: T.cost 0.013552, Train acc 0.996280, test acc 0.789200, took 23.0 sec.\n",
      "Epoch 511: T.cost 0.016384, Train acc 0.995160, test acc 0.788600, took 14.8 sec.\n",
      "Epoch 512: T.cost 0.014236, Train acc 0.995920, test acc 0.787100, took 19.1 sec.\n",
      "Epoch 513: T.cost 0.017015, Train acc 0.995420, test acc 0.790200, took 20.7 sec.\n",
      "Epoch 514: T.cost 0.013286, Train acc 0.996180, test acc 0.787900, took 12.8 sec.\n",
      "Epoch 515: T.cost 0.011526, Train acc 0.996400, test acc 0.791000, took 23.1 sec.\n",
      "Epoch 516: T.cost 0.013789, Train acc 0.996140, test acc 0.785700, took 16.5 sec.\n",
      "Epoch 517: T.cost 0.014344, Train acc 0.996140, test acc 0.788500, took 17.0 sec.\n",
      "Epoch 518: T.cost 0.014811, Train acc 0.995820, test acc 0.791300, took 22.8 sec.\n",
      "New LR: 0.000770043306984\n",
      "Epoch 519: T.cost 0.013794, Train acc 0.995920, test acc 0.783900, took 12.5 sec.\n",
      "Epoch 520: T.cost 0.013772, Train acc 0.995780, test acc 0.792100, took 12.8 sec.\n",
      "Epoch 521: T.cost 0.012890, Train acc 0.996440, test acc 0.788200, took 23.0 sec.\n",
      "Epoch 522: T.cost 0.011970, Train acc 0.996280, test acc 0.791300, took 16.5 sec.\n",
      "Epoch 523: T.cost 0.013547, Train acc 0.996060, test acc 0.784300, took 19.3 sec.\n",
      "Epoch 524: T.cost 0.013338, Train acc 0.995720, test acc 0.787900, took 20.5 sec.\n",
      "Epoch 525: T.cost 0.014567, Train acc 0.995640, test acc 0.791600, took 15.4 sec.\n",
      "Epoch 526: T.cost 0.014472, Train acc 0.996080, test acc 0.787500, took 22.8 sec.\n",
      "Epoch 527: T.cost 0.014592, Train acc 0.996340, test acc 0.792100, took 14.0 sec.\n",
      "Epoch 528: T.cost 0.011538, Train acc 0.996460, test acc 0.790500, took 21.5 sec.\n",
      "Epoch 529: T.cost 0.013623, Train acc 0.995800, test acc 0.785000, took 18.3 sec.\n",
      "Epoch 530: T.cost 0.017346, Train acc 0.995320, test acc 0.784100, took 16.3 sec.\n",
      "Epoch 531: T.cost 0.012303, Train acc 0.996400, test acc 0.787400, took 22.9 sec.\n",
      "Epoch 532: T.cost 0.014333, Train acc 0.995880, test acc 0.783700, took 13.0 sec.\n",
      "Epoch 533: T.cost 0.015260, Train acc 0.995420, test acc 0.791200, took 21.9 sec.\n",
      "Epoch 534: T.cost 0.015775, Train acc 0.995480, test acc 0.787300, took 18.0 sec.\n",
      "Epoch 535: T.cost 0.014605, Train acc 0.995700, test acc 0.788600, took 16.3 sec.\n",
      "Epoch 536: T.cost 0.015023, Train acc 0.995700, test acc 0.790200, took 22.9 sec.\n",
      "Epoch 537: T.cost 0.012271, Train acc 0.996280, test acc 0.790400, took 13.1 sec.\n",
      "Epoch 538: T.cost 0.013457, Train acc 0.996700, test acc 0.787000, took 21.8 sec.\n",
      "New LR: 0.000762342857779\n",
      "Epoch 539: T.cost 0.013712, Train acc 0.995880, test acc 0.790500, took 18.0 sec.\n",
      "Epoch 540: T.cost 0.013391, Train acc 0.996120, test acc 0.787300, took 16.5 sec.\n",
      "Epoch 541: T.cost 0.014678, Train acc 0.995900, test acc 0.791000, took 22.9 sec.\n",
      "Epoch 542: T.cost 0.012841, Train acc 0.996240, test acc 0.790000, took 12.9 sec.\n",
      "Epoch 543: T.cost 0.013560, Train acc 0.996620, test acc 0.784200, took 21.9 sec.\n",
      "Epoch 544: T.cost 0.012838, Train acc 0.996760, test acc 0.788600, took 18.0 sec.\n",
      "Epoch 545: T.cost 0.011602, Train acc 0.996580, test acc 0.787600, took 12.5 sec.\n",
      "Epoch 546: T.cost 0.013717, Train acc 0.995800, test acc 0.793900, took 17.0 sec.\n",
      "Epoch 547: T.cost 0.014104, Train acc 0.995660, test acc 0.786500, took 22.8 sec.\n",
      "Epoch 548: T.cost 0.013608, Train acc 0.995820, test acc 0.785100, took 12.5 sec.\n",
      "Epoch 549: T.cost 0.013111, Train acc 0.995760, test acc 0.785700, took 22.4 sec.\n",
      "Epoch 550: T.cost 0.014036, Train acc 0.996000, test acc 0.786800, took 17.4 sec.\n",
      "Epoch 551: T.cost 0.011638, Train acc 0.996520, test acc 0.791000, took 17.2 sec.\n",
      "Epoch 552: T.cost 0.011797, Train acc 0.996440, test acc 0.787400, took 22.6 sec.\n",
      "Epoch 553: T.cost 0.013979, Train acc 0.995980, test acc 0.788800, took 12.5 sec.\n",
      "Epoch 554: T.cost 0.014908, Train acc 0.996000, test acc 0.790000, took 22.6 sec.\n",
      "Epoch 555: T.cost 0.015102, Train acc 0.995460, test acc 0.781400, took 17.2 sec.\n",
      "Epoch 556: T.cost 0.012671, Train acc 0.996340, test acc 0.788600, took 16.6 sec.\n",
      "Epoch 557: T.cost 0.013630, Train acc 0.996040, test acc 0.784800, took 23.2 sec.\n",
      "Epoch 558: T.cost 0.014349, Train acc 0.995920, test acc 0.785100, took 12.5 sec.\n",
      "New LR: 0.00075471945398\n",
      "Epoch 559: T.cost 0.011984, Train acc 0.996220, test acc 0.780500, took 22.0 sec.\n",
      "Epoch 560: T.cost 0.012840, Train acc 0.996200, test acc 0.786500, took 17.8 sec.\n",
      "Epoch 561: T.cost 0.010866, Train acc 0.996600, test acc 0.781200, took 16.7 sec.\n",
      "Epoch 562: T.cost 0.014531, Train acc 0.995580, test acc 0.784500, took 23.0 sec.\n",
      "Epoch 563: T.cost 0.009606, Train acc 0.997000, test acc 0.787400, took 12.5 sec.\n",
      "Epoch 564: T.cost 0.011182, Train acc 0.996620, test acc 0.789400, took 21.6 sec.\n",
      "Epoch 565: T.cost 0.013213, Train acc 0.996180, test acc 0.789000, took 18.2 sec.\n",
      "Epoch 566: T.cost 0.014888, Train acc 0.996000, test acc 0.787600, took 15.7 sec.\n",
      "Epoch 567: T.cost 0.011985, Train acc 0.996640, test acc 0.790700, took 23.0 sec.\n",
      "Epoch 568: T.cost 0.014881, Train acc 0.996300, test acc 0.787400, took 13.5 sec.\n",
      "Epoch 569: T.cost 0.013209, Train acc 0.996020, test acc 0.786900, took 21.0 sec.\n",
      "Epoch 570: T.cost 0.013017, Train acc 0.995940, test acc 0.786600, took 18.9 sec.\n",
      "Epoch 571: T.cost 0.013491, Train acc 0.995940, test acc 0.785800, took 12.5 sec.\n",
      "Epoch 572: T.cost 0.012510, Train acc 0.996280, test acc 0.789800, took 15.7 sec.\n",
      "Epoch 573: T.cost 0.014441, Train acc 0.996060, test acc 0.786300, took 22.9 sec.\n",
      "Epoch 574: T.cost 0.012155, Train acc 0.996960, test acc 0.791800, took 13.6 sec.\n",
      "Epoch 575: T.cost 0.011145, Train acc 0.996840, test acc 0.784900, took 20.4 sec.\n",
      "Epoch 576: T.cost 0.014385, Train acc 0.996300, test acc 0.790800, took 19.5 sec.\n",
      "Epoch 577: T.cost 0.010893, Train acc 0.996700, test acc 0.792200, took 14.8 sec.\n",
      "Epoch 578: T.cost 0.015818, Train acc 0.995680, test acc 0.785900, took 22.9 sec.\n",
      "New LR: 0.000747172231204\n",
      "Epoch 579: T.cost 0.014761, Train acc 0.995780, test acc 0.785000, took 14.6 sec.\n",
      "Epoch 580: T.cost 0.013052, Train acc 0.996060, test acc 0.790200, took 19.3 sec.\n",
      "Epoch 581: T.cost 0.017219, Train acc 0.995420, test acc 0.779800, took 20.5 sec.\n",
      "Epoch 582: T.cost 0.012271, Train acc 0.995940, test acc 0.789100, took 13.3 sec.\n",
      "Epoch 583: T.cost 0.012973, Train acc 0.996460, test acc 0.794900, took 23.0 sec.\n",
      "Epoch 584: T.cost 0.012520, Train acc 0.996160, test acc 0.793400, took 16.0 sec.\n",
      "Epoch 585: T.cost 0.011529, Train acc 0.996660, test acc 0.784800, took 18.3 sec.\n",
      "Epoch 586: T.cost 0.009772, Train acc 0.996860, test acc 0.785900, took 21.6 sec.\n",
      "Epoch 587: T.cost 0.012962, Train acc 0.996600, test acc 0.787900, took 12.5 sec.\n",
      "Epoch 588: T.cost 0.014656, Train acc 0.995660, test acc 0.788800, took 22.9 sec.\n",
      "Epoch 589: T.cost 0.013134, Train acc 0.996000, test acc 0.790700, took 16.9 sec.\n",
      "Epoch 590: T.cost 0.013994, Train acc 0.996320, test acc 0.788000, took 18.5 sec.\n",
      "Epoch 591: T.cost 0.010544, Train acc 0.997120, test acc 0.784300, took 21.3 sec.\n",
      "Epoch 592: T.cost 0.014150, Train acc 0.996080, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 593: T.cost 0.010827, Train acc 0.997020, test acc 0.790100, took 21.7 sec.\n",
      "Epoch 594: T.cost 0.012827, Train acc 0.996500, test acc 0.793000, took 18.1 sec.\n",
      "Epoch 595: T.cost 0.012318, Train acc 0.996400, test acc 0.791700, took 15.9 sec.\n",
      "Epoch 596: T.cost 0.010542, Train acc 0.996700, test acc 0.791800, took 23.0 sec.\n",
      "Epoch 597: T.cost 0.010384, Train acc 0.996840, test acc 0.788700, took 13.3 sec.\n",
      "Epoch 598: T.cost 0.014769, Train acc 0.996060, test acc 0.783200, took 12.5 sec.\n",
      "New LR: 0.000739700497943\n",
      "Epoch 599: T.cost 0.014719, Train acc 0.995740, test acc 0.786500, took 21.6 sec.\n",
      "Epoch 600: T.cost 0.012583, Train acc 0.996520, test acc 0.788000, took 18.2 sec.\n",
      "Epoch 601: T.cost 0.012872, Train acc 0.996800, test acc 0.789200, took 14.4 sec.\n",
      "Epoch 602: T.cost 0.011937, Train acc 0.996720, test acc 0.787400, took 22.9 sec.\n",
      "Epoch 603: T.cost 0.014736, Train acc 0.995780, test acc 0.792000, took 14.9 sec.\n",
      "Epoch 604: T.cost 0.013553, Train acc 0.996600, test acc 0.787200, took 19.1 sec.\n",
      "Epoch 605: T.cost 0.012707, Train acc 0.996380, test acc 0.789000, took 20.8 sec.\n",
      "Epoch 606: T.cost 0.010179, Train acc 0.996840, test acc 0.792200, took 13.6 sec.\n",
      "Epoch 607: T.cost 0.013196, Train acc 0.996640, test acc 0.792100, took 23.1 sec.\n",
      "Epoch 608: T.cost 0.012806, Train acc 0.996400, test acc 0.787800, took 15.6 sec.\n",
      "Epoch 609: T.cost 0.013086, Train acc 0.996380, test acc 0.787900, took 16.5 sec.\n",
      "Epoch 610: T.cost 0.011057, Train acc 0.997060, test acc 0.794400, took 22.9 sec.\n",
      "Epoch 611: T.cost 0.012724, Train acc 0.996440, test acc 0.787700, took 12.9 sec.\n",
      "Epoch 612: T.cost 0.011350, Train acc 0.996680, test acc 0.790800, took 20.8 sec.\n",
      "Epoch 613: T.cost 0.015176, Train acc 0.995880, test acc 0.788800, took 19.0 sec.\n",
      "Epoch 614: T.cost 0.012885, Train acc 0.996320, test acc 0.790000, took 14.9 sec.\n",
      "Epoch 615: T.cost 0.012733, Train acc 0.995880, test acc 0.786100, took 22.9 sec.\n",
      "Epoch 616: T.cost 0.011726, Train acc 0.996540, test acc 0.789900, took 14.5 sec.\n",
      "Epoch 617: T.cost 0.011957, Train acc 0.996740, test acc 0.787100, took 17.9 sec.\n",
      "Epoch 618: T.cost 0.013875, Train acc 0.996140, test acc 0.788600, took 21.9 sec.\n",
      "New LR: 0.000732303505065\n",
      "Epoch 619: T.cost 0.013685, Train acc 0.996260, test acc 0.788200, took 12.5 sec.\n",
      "Epoch 620: T.cost 0.011373, Train acc 0.996920, test acc 0.779400, took 22.1 sec.\n",
      "Epoch 621: T.cost 0.012106, Train acc 0.996420, test acc 0.785500, took 17.8 sec.\n",
      "Epoch 622: T.cost 0.012270, Train acc 0.996400, test acc 0.787300, took 16.4 sec.\n",
      "Epoch 623: T.cost 0.011610, Train acc 0.996500, test acc 0.784700, took 23.0 sec.\n",
      "Epoch 624: T.cost 0.011336, Train acc 0.996600, test acc 0.788900, took 12.8 sec.\n",
      "Epoch 625: T.cost 0.011552, Train acc 0.996980, test acc 0.787800, took 12.5 sec.\n",
      "Epoch 626: T.cost 0.013090, Train acc 0.996240, test acc 0.784100, took 20.0 sec.\n",
      "Epoch 627: T.cost 0.014243, Train acc 0.996060, test acc 0.789000, took 19.8 sec.\n",
      "Epoch 628: T.cost 0.012695, Train acc 0.996320, test acc 0.787900, took 13.5 sec.\n",
      "Epoch 629: T.cost 0.013676, Train acc 0.996280, test acc 0.782500, took 22.9 sec.\n",
      "Epoch 630: T.cost 0.009126, Train acc 0.997240, test acc 0.788800, took 15.9 sec.\n",
      "Epoch 631: T.cost 0.010501, Train acc 0.996880, test acc 0.788200, took 18.4 sec.\n",
      "Epoch 632: T.cost 0.012475, Train acc 0.996740, test acc 0.790100, took 21.4 sec.\n",
      "Epoch 633: T.cost 0.011144, Train acc 0.997060, test acc 0.786000, took 12.5 sec.\n",
      "Epoch 634: T.cost 0.013046, Train acc 0.996440, test acc 0.787500, took 21.5 sec.\n",
      "Epoch 635: T.cost 0.011283, Train acc 0.996780, test acc 0.792000, took 18.4 sec.\n",
      "Epoch 636: T.cost 0.014228, Train acc 0.996280, test acc 0.787500, took 14.9 sec.\n",
      "Epoch 637: T.cost 0.014062, Train acc 0.995940, test acc 0.785600, took 23.0 sec.\n",
      "Epoch 638: T.cost 0.012584, Train acc 0.996220, test acc 0.788300, took 14.4 sec.\n",
      "New LR: 0.000724980445812\n",
      "Epoch 639: T.cost 0.013069, Train acc 0.996200, test acc 0.789000, took 19.7 sec.\n",
      "Epoch 640: T.cost 0.011214, Train acc 0.996900, test acc 0.789900, took 20.1 sec.\n",
      "Epoch 641: T.cost 0.011053, Train acc 0.997000, test acc 0.788700, took 12.5 sec.\n",
      "Epoch 642: T.cost 0.010111, Train acc 0.997060, test acc 0.787000, took 22.6 sec.\n",
      "Epoch 643: T.cost 0.015159, Train acc 0.995880, test acc 0.787200, took 17.2 sec.\n",
      "Epoch 644: T.cost 0.011603, Train acc 0.996820, test acc 0.788800, took 16.0 sec.\n",
      "Epoch 645: T.cost 0.011983, Train acc 0.996760, test acc 0.789000, took 23.0 sec.\n",
      "Epoch 646: T.cost 0.010726, Train acc 0.996580, test acc 0.789100, took 13.2 sec.\n",
      "Epoch 647: T.cost 0.011777, Train acc 0.996740, test acc 0.786900, took 21.1 sec.\n",
      "Epoch 648: T.cost 0.012676, Train acc 0.996660, test acc 0.792100, took 18.8 sec.\n",
      "Epoch 649: T.cost 0.013818, Train acc 0.995840, test acc 0.782300, took 13.0 sec.\n",
      "Epoch 650: T.cost 0.010933, Train acc 0.996820, test acc 0.786300, took 23.0 sec.\n",
      "Epoch 651: T.cost 0.013030, Train acc 0.996140, test acc 0.790400, took 16.2 sec.\n",
      "Epoch 652: T.cost 0.011508, Train acc 0.996680, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 653: T.cost 0.010405, Train acc 0.997060, test acc 0.792700, took 16.7 sec.\n",
      "Epoch 654: T.cost 0.009682, Train acc 0.997080, test acc 0.794400, took 23.0 sec.\n",
      "Epoch 655: T.cost 0.014169, Train acc 0.996140, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 656: T.cost 0.009987, Train acc 0.996800, test acc 0.788800, took 21.3 sec.\n",
      "Epoch 657: T.cost 0.013345, Train acc 0.996520, test acc 0.789000, took 18.6 sec.\n",
      "Epoch 658: T.cost 0.012047, Train acc 0.996440, test acc 0.788300, took 13.3 sec.\n",
      "New LR: 0.000717730628676\n",
      "Epoch 659: T.cost 0.010276, Train acc 0.997080, test acc 0.786700, took 23.1 sec.\n",
      "Epoch 660: T.cost 0.013303, Train acc 0.996180, test acc 0.787100, took 15.8 sec.\n",
      "Epoch 661: T.cost 0.011476, Train acc 0.996780, test acc 0.792500, took 17.3 sec.\n",
      "Epoch 662: T.cost 0.011415, Train acc 0.996860, test acc 0.787900, took 22.5 sec.\n",
      "Epoch 663: T.cost 0.009774, Train acc 0.996880, test acc 0.788900, took 12.5 sec.\n",
      "Epoch 664: T.cost 0.011800, Train acc 0.996840, test acc 0.783200, took 22.0 sec.\n",
      "Epoch 665: T.cost 0.013524, Train acc 0.996240, test acc 0.788100, took 17.8 sec.\n",
      "Epoch 666: T.cost 0.011519, Train acc 0.996720, test acc 0.788400, took 14.1 sec.\n",
      "Epoch 667: T.cost 0.010676, Train acc 0.996960, test acc 0.792500, took 23.0 sec.\n",
      "Epoch 668: T.cost 0.011701, Train acc 0.996860, test acc 0.787500, took 15.2 sec.\n",
      "Epoch 669: T.cost 0.012154, Train acc 0.996640, test acc 0.782700, took 17.9 sec.\n",
      "Epoch 670: T.cost 0.010250, Train acc 0.996960, test acc 0.783700, took 22.0 sec.\n",
      "Epoch 671: T.cost 0.009857, Train acc 0.997180, test acc 0.788300, took 12.5 sec.\n",
      "Epoch 672: T.cost 0.015177, Train acc 0.996240, test acc 0.786000, took 22.5 sec.\n",
      "Epoch 673: T.cost 0.014104, Train acc 0.996320, test acc 0.785900, took 17.3 sec.\n",
      "Epoch 674: T.cost 0.010793, Train acc 0.997100, test acc 0.793200, took 14.3 sec.\n",
      "Epoch 675: T.cost 0.011685, Train acc 0.996720, test acc 0.788300, took 23.0 sec.\n",
      "Epoch 676: T.cost 0.009861, Train acc 0.996980, test acc 0.789200, took 15.0 sec.\n",
      "Epoch 677: T.cost 0.011271, Train acc 0.996880, test acc 0.792900, took 18.2 sec.\n",
      "Epoch 678: T.cost 0.011976, Train acc 0.996620, test acc 0.788400, took 21.7 sec.\n",
      "New LR: 0.000710553304525\n",
      "Epoch 679: T.cost 0.010833, Train acc 0.997100, test acc 0.788600, took 12.5 sec.\n",
      "Epoch 680: T.cost 0.011762, Train acc 0.996840, test acc 0.787200, took 12.5 sec.\n",
      "Epoch 681: T.cost 0.012450, Train acc 0.996780, test acc 0.783800, took 23.1 sec.\n",
      "Epoch 682: T.cost 0.011530, Train acc 0.997120, test acc 0.794500, took 16.8 sec.\n",
      "Epoch 683: T.cost 0.009916, Train acc 0.997300, test acc 0.788200, took 14.4 sec.\n",
      "Epoch 684: T.cost 0.012482, Train acc 0.996580, test acc 0.788400, took 23.0 sec.\n",
      "Epoch 685: T.cost 0.011504, Train acc 0.996980, test acc 0.790300, took 14.8 sec.\n",
      "Epoch 686: T.cost 0.010227, Train acc 0.997040, test acc 0.788800, took 18.3 sec.\n",
      "Epoch 687: T.cost 0.009589, Train acc 0.997040, test acc 0.790600, took 21.6 sec.\n",
      "Epoch 688: T.cost 0.011728, Train acc 0.996760, test acc 0.789300, took 12.5 sec.\n",
      "Epoch 689: T.cost 0.008526, Train acc 0.997600, test acc 0.788900, took 22.6 sec.\n",
      "Epoch 690: T.cost 0.010819, Train acc 0.996900, test acc 0.784500, took 17.3 sec.\n",
      "Epoch 691: T.cost 0.012135, Train acc 0.996860, test acc 0.788000, took 14.1 sec.\n",
      "Epoch 692: T.cost 0.012841, Train acc 0.996540, test acc 0.791200, took 22.9 sec.\n",
      "Epoch 693: T.cost 0.011226, Train acc 0.996880, test acc 0.791800, took 15.3 sec.\n",
      "Epoch 694: T.cost 0.010657, Train acc 0.997000, test acc 0.791500, took 17.6 sec.\n",
      "Epoch 695: T.cost 0.010509, Train acc 0.997100, test acc 0.793100, took 22.2 sec.\n",
      "Epoch 696: T.cost 0.011569, Train acc 0.996800, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 697: T.cost 0.011135, Train acc 0.997060, test acc 0.791300, took 21.7 sec.\n",
      "Epoch 698: T.cost 0.012665, Train acc 0.996640, test acc 0.791600, took 18.2 sec.\n",
      "New LR: 0.000703447781852\n",
      "Epoch 699: T.cost 0.010634, Train acc 0.996780, test acc 0.793900, took 13.4 sec.\n",
      "Epoch 700: T.cost 0.010405, Train acc 0.996820, test acc 0.788200, took 22.9 sec.\n",
      "Epoch 701: T.cost 0.012293, Train acc 0.997200, test acc 0.789200, took 16.0 sec.\n",
      "Epoch 702: T.cost 0.010356, Train acc 0.997020, test acc 0.791800, took 16.6 sec.\n",
      "Epoch 703: T.cost 0.011930, Train acc 0.996880, test acc 0.786100, took 23.0 sec.\n",
      "Epoch 704: T.cost 0.014858, Train acc 0.996500, test acc 0.789900, took 12.7 sec.\n",
      "Epoch 705: T.cost 0.009980, Train acc 0.996880, test acc 0.786900, took 20.6 sec.\n",
      "Epoch 706: T.cost 0.012463, Train acc 0.996560, test acc 0.788100, took 19.2 sec.\n",
      "Epoch 707: T.cost 0.011668, Train acc 0.996880, test acc 0.784900, took 12.5 sec.\n",
      "Epoch 708: T.cost 0.011589, Train acc 0.996880, test acc 0.792900, took 12.6 sec.\n",
      "Epoch 709: T.cost 0.010168, Train acc 0.997240, test acc 0.789100, took 23.0 sec.\n",
      "Epoch 710: T.cost 0.010753, Train acc 0.997020, test acc 0.789200, took 16.7 sec.\n",
      "Epoch 711: T.cost 0.009496, Train acc 0.997320, test acc 0.789500, took 15.7 sec.\n",
      "Epoch 712: T.cost 0.010877, Train acc 0.996960, test acc 0.790200, took 23.0 sec.\n",
      "Epoch 713: T.cost 0.013656, Train acc 0.996140, test acc 0.787100, took 13.5 sec.\n",
      "Epoch 714: T.cost 0.013812, Train acc 0.996400, test acc 0.792100, took 20.1 sec.\n",
      "Epoch 715: T.cost 0.011242, Train acc 0.996580, test acc 0.790300, took 19.8 sec.\n",
      "Epoch 716: T.cost 0.009813, Train acc 0.997080, test acc 0.786900, took 12.5 sec.\n",
      "Epoch 717: T.cost 0.010128, Train acc 0.997040, test acc 0.790000, took 22.0 sec.\n",
      "Epoch 718: T.cost 0.008360, Train acc 0.997540, test acc 0.788600, took 17.8 sec.\n",
      "New LR: 0.000696413311525\n",
      "Epoch 719: T.cost 0.010897, Train acc 0.996800, test acc 0.787200, took 14.6 sec.\n",
      "Epoch 720: T.cost 0.009986, Train acc 0.997300, test acc 0.793600, took 22.9 sec.\n",
      "Epoch 721: T.cost 0.011474, Train acc 0.996740, test acc 0.787900, took 14.8 sec.\n",
      "Epoch 722: T.cost 0.012812, Train acc 0.996620, test acc 0.787300, took 18.2 sec.\n",
      "Epoch 723: T.cost 0.012060, Train acc 0.996620, test acc 0.789200, took 21.6 sec.\n",
      "Epoch 724: T.cost 0.009454, Train acc 0.997200, test acc 0.784200, took 12.5 sec.\n",
      "Epoch 725: T.cost 0.011865, Train acc 0.996660, test acc 0.788600, took 20.1 sec.\n",
      "Epoch 726: T.cost 0.014073, Train acc 0.995780, test acc 0.781800, took 19.8 sec.\n",
      "Epoch 727: T.cost 0.010163, Train acc 0.997000, test acc 0.787100, took 12.6 sec.\n",
      "Epoch 728: T.cost 0.008574, Train acc 0.997540, test acc 0.791000, took 22.9 sec.\n",
      "Epoch 729: T.cost 0.009873, Train acc 0.997300, test acc 0.791000, took 16.8 sec.\n",
      "Epoch 730: T.cost 0.011270, Train acc 0.997060, test acc 0.789200, took 16.1 sec.\n",
      "Epoch 731: T.cost 0.012994, Train acc 0.996320, test acc 0.791700, took 23.0 sec.\n",
      "Epoch 732: T.cost 0.010448, Train acc 0.996940, test acc 0.790300, took 13.2 sec.\n",
      "Epoch 733: T.cost 0.009745, Train acc 0.997120, test acc 0.783000, took 18.0 sec.\n",
      "Epoch 734: T.cost 0.011570, Train acc 0.996760, test acc 0.790000, took 21.8 sec.\n",
      "Epoch 735: T.cost 0.013802, Train acc 0.996560, test acc 0.787800, took 12.5 sec.\n",
      "Epoch 736: T.cost 0.006643, Train acc 0.997920, test acc 0.791800, took 12.5 sec.\n",
      "Epoch 737: T.cost 0.010970, Train acc 0.997580, test acc 0.789900, took 21.5 sec.\n",
      "Epoch 738: T.cost 0.011085, Train acc 0.996880, test acc 0.784000, took 18.3 sec.\n",
      "New LR: 0.000689449202036\n",
      "Epoch 739: T.cost 0.013210, Train acc 0.996520, test acc 0.788600, took 14.9 sec.\n",
      "Epoch 740: T.cost 0.010525, Train acc 0.996660, test acc 0.791000, took 23.0 sec.\n",
      "Epoch 741: T.cost 0.010896, Train acc 0.996860, test acc 0.789600, took 14.3 sec.\n",
      "Epoch 742: T.cost 0.011170, Train acc 0.996960, test acc 0.787500, took 16.4 sec.\n",
      "Epoch 743: T.cost 0.015073, Train acc 0.996460, test acc 0.788200, took 22.9 sec.\n",
      "Epoch 744: T.cost 0.010297, Train acc 0.997180, test acc 0.786400, took 12.9 sec.\n",
      "Epoch 745: T.cost 0.009217, Train acc 0.996920, test acc 0.790400, took 19.8 sec.\n",
      "Epoch 746: T.cost 0.011741, Train acc 0.996880, test acc 0.789400, took 20.1 sec.\n",
      "Epoch 747: T.cost 0.010187, Train acc 0.996900, test acc 0.790000, took 13.2 sec.\n",
      "Epoch 748: T.cost 0.010060, Train acc 0.997140, test acc 0.790200, took 23.0 sec.\n",
      "Epoch 749: T.cost 0.011494, Train acc 0.996920, test acc 0.795300, took 16.1 sec.\n",
      "Epoch 750: T.cost 0.008491, Train acc 0.997620, test acc 0.790700, took 14.6 sec.\n",
      "Epoch 751: T.cost 0.008362, Train acc 0.997400, test acc 0.786300, took 23.0 sec.\n",
      "Epoch 752: T.cost 0.016799, Train acc 0.995620, test acc 0.786300, took 14.6 sec.\n",
      "Epoch 753: T.cost 0.009594, Train acc 0.997120, test acc 0.789500, took 17.8 sec.\n",
      "Epoch 754: T.cost 0.012084, Train acc 0.996980, test acc 0.792100, took 22.0 sec.\n",
      "Epoch 755: T.cost 0.010348, Train acc 0.997240, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 756: T.cost 0.010545, Train acc 0.997100, test acc 0.790900, took 21.5 sec.\n",
      "Epoch 757: T.cost 0.008555, Train acc 0.997460, test acc 0.793000, took 18.3 sec.\n",
      "Epoch 758: T.cost 0.013728, Train acc 0.996180, test acc 0.788200, took 12.6 sec.\n",
      "New LR: 0.000682554704254\n",
      "Epoch 759: T.cost 0.010203, Train acc 0.997340, test acc 0.784600, took 23.0 sec.\n",
      "Epoch 760: T.cost 0.011170, Train acc 0.996760, test acc 0.795000, took 16.8 sec.\n",
      "Epoch 761: T.cost 0.011034, Train acc 0.997040, test acc 0.797100, took 15.6 sec.\n",
      "Epoch 762: T.cost 0.011105, Train acc 0.996980, test acc 0.793100, took 22.9 sec.\n",
      "Epoch 763: T.cost 0.009741, Train acc 0.997080, test acc 0.795400, took 13.7 sec.\n",
      "Epoch 764: T.cost 0.009389, Train acc 0.997360, test acc 0.791600, took 12.5 sec.\n",
      "Epoch 765: T.cost 0.008102, Train acc 0.997520, test acc 0.791000, took 19.3 sec.\n",
      "Epoch 766: T.cost 0.011410, Train acc 0.996640, test acc 0.793700, took 20.6 sec.\n",
      "Epoch 767: T.cost 0.009094, Train acc 0.997640, test acc 0.795900, took 12.5 sec.\n",
      "Epoch 768: T.cost 0.008883, Train acc 0.997340, test acc 0.788200, took 20.6 sec.\n",
      "Epoch 769: T.cost 0.011627, Train acc 0.996920, test acc 0.786800, took 19.2 sec.\n",
      "Epoch 770: T.cost 0.011144, Train acc 0.996960, test acc 0.791600, took 13.2 sec.\n",
      "Epoch 771: T.cost 0.009807, Train acc 0.997200, test acc 0.793800, took 23.0 sec.\n",
      "Epoch 772: T.cost 0.009106, Train acc 0.997080, test acc 0.792500, took 16.1 sec.\n",
      "Epoch 773: T.cost 0.011432, Train acc 0.997080, test acc 0.790700, took 16.4 sec.\n",
      "Epoch 774: T.cost 0.008890, Train acc 0.997140, test acc 0.792600, took 22.9 sec.\n",
      "Epoch 775: T.cost 0.009543, Train acc 0.997200, test acc 0.796200, took 13.0 sec.\n",
      "Epoch 776: T.cost 0.008235, Train acc 0.997520, test acc 0.792700, took 17.8 sec.\n",
      "Epoch 777: T.cost 0.009313, Train acc 0.997240, test acc 0.791300, took 22.0 sec.\n",
      "Epoch 778: T.cost 0.011031, Train acc 0.997000, test acc 0.791700, took 12.5 sec.\n",
      "New LR: 0.000675729184295\n",
      "Epoch 779: T.cost 0.010145, Train acc 0.997120, test acc 0.787700, took 20.8 sec.\n",
      "Epoch 780: T.cost 0.011136, Train acc 0.996960, test acc 0.795600, took 19.1 sec.\n",
      "Epoch 781: T.cost 0.011935, Train acc 0.996560, test acc 0.789800, took 13.9 sec.\n",
      "Epoch 782: T.cost 0.008963, Train acc 0.997320, test acc 0.788900, took 23.0 sec.\n",
      "Epoch 783: T.cost 0.012746, Train acc 0.996740, test acc 0.789600, took 15.4 sec.\n",
      "Epoch 784: T.cost 0.011282, Train acc 0.996960, test acc 0.795800, took 15.7 sec.\n",
      "Epoch 785: T.cost 0.010366, Train acc 0.997420, test acc 0.792500, took 22.9 sec.\n",
      "Epoch 786: T.cost 0.011211, Train acc 0.996740, test acc 0.790000, took 13.7 sec.\n",
      "Epoch 787: T.cost 0.008407, Train acc 0.997480, test acc 0.797100, took 18.2 sec.\n",
      "Epoch 788: T.cost 0.008994, Train acc 0.997720, test acc 0.789000, took 21.7 sec.\n",
      "Epoch 789: T.cost 0.010992, Train acc 0.997060, test acc 0.798300, took 12.5 sec.\n",
      "Epoch 790: T.cost 0.010818, Train acc 0.996900, test acc 0.793800, took 21.7 sec.\n",
      "Epoch 791: T.cost 0.011030, Train acc 0.996480, test acc 0.788100, took 18.2 sec.\n",
      "Epoch 792: T.cost 0.010640, Train acc 0.997280, test acc 0.794500, took 12.5 sec.\n",
      "Epoch 793: T.cost 0.011608, Train acc 0.997100, test acc 0.791600, took 13.2 sec.\n",
      "Epoch 794: T.cost 0.011630, Train acc 0.996940, test acc 0.786300, took 23.0 sec.\n",
      "Epoch 795: T.cost 0.010693, Train acc 0.997260, test acc 0.793000, took 16.0 sec.\n",
      "Epoch 796: T.cost 0.010272, Train acc 0.997120, test acc 0.792100, took 15.7 sec.\n",
      "Epoch 797: T.cost 0.011160, Train acc 0.997240, test acc 0.787500, took 23.0 sec.\n",
      "Epoch 798: T.cost 0.012114, Train acc 0.996820, test acc 0.789900, took 13.6 sec.\n",
      "New LR: 0.000668971893028\n",
      "Epoch 799: T.cost 0.008802, Train acc 0.997540, test acc 0.794100, took 18.9 sec.\n",
      "Epoch 800: T.cost 0.007959, Train acc 0.997520, test acc 0.789000, took 20.9 sec.\n",
      "Epoch 801: T.cost 0.011358, Train acc 0.996960, test acc 0.791600, took 12.5 sec.\n",
      "Epoch 802: T.cost 0.011867, Train acc 0.996740, test acc 0.787200, took 20.6 sec.\n",
      "Epoch 803: T.cost 0.009822, Train acc 0.997440, test acc 0.791600, took 19.2 sec.\n",
      "Epoch 804: T.cost 0.009458, Train acc 0.997400, test acc 0.789900, took 12.6 sec.\n",
      "Epoch 805: T.cost 0.008527, Train acc 0.997420, test acc 0.791300, took 22.9 sec.\n",
      "Epoch 806: T.cost 0.007988, Train acc 0.997640, test acc 0.792400, took 16.7 sec.\n",
      "Epoch 807: T.cost 0.011200, Train acc 0.997160, test acc 0.791600, took 15.7 sec.\n",
      "Epoch 808: T.cost 0.011804, Train acc 0.996980, test acc 0.789200, took 23.0 sec.\n",
      "Epoch 809: T.cost 0.009894, Train acc 0.997240, test acc 0.789100, took 13.5 sec.\n",
      "Epoch 810: T.cost 0.010102, Train acc 0.997260, test acc 0.788300, took 17.0 sec.\n",
      "Epoch 811: T.cost 0.010746, Train acc 0.997060, test acc 0.796700, took 22.8 sec.\n",
      "Epoch 812: T.cost 0.007760, Train acc 0.997760, test acc 0.792700, took 12.5 sec.\n",
      "Epoch 813: T.cost 0.010549, Train acc 0.997100, test acc 0.793800, took 19.5 sec.\n",
      "Epoch 814: T.cost 0.010393, Train acc 0.997180, test acc 0.789300, took 20.4 sec.\n",
      "Epoch 815: T.cost 0.011861, Train acc 0.997080, test acc 0.792000, took 12.5 sec.\n",
      "Epoch 816: T.cost 0.011317, Train acc 0.996520, test acc 0.785600, took 22.9 sec.\n",
      "Epoch 817: T.cost 0.008838, Train acc 0.997380, test acc 0.790000, took 16.9 sec.\n",
      "Epoch 818: T.cost 0.011149, Train acc 0.997040, test acc 0.790400, took 13.7 sec.\n",
      "New LR: 0.000662282196572\n",
      "Epoch 819: T.cost 0.010763, Train acc 0.997180, test acc 0.794400, took 23.0 sec.\n",
      "Epoch 820: T.cost 0.010554, Train acc 0.997020, test acc 0.785200, took 15.6 sec.\n",
      "Epoch 821: T.cost 0.009591, Train acc 0.997380, test acc 0.796400, took 12.5 sec.\n",
      "Epoch 822: T.cost 0.009172, Train acc 0.997480, test acc 0.791300, took 16.3 sec.\n",
      "Epoch 823: T.cost 0.009462, Train acc 0.997340, test acc 0.795000, took 22.9 sec.\n",
      "Epoch 824: T.cost 0.008645, Train acc 0.997440, test acc 0.791900, took 13.0 sec.\n",
      "Epoch 825: T.cost 0.008698, Train acc 0.997680, test acc 0.792800, took 19.1 sec.\n",
      "Epoch 826: T.cost 0.009800, Train acc 0.997180, test acc 0.791800, took 20.7 sec.\n",
      "Epoch 827: T.cost 0.009781, Train acc 0.997320, test acc 0.790000, took 12.5 sec.\n",
      "Epoch 828: T.cost 0.010507, Train acc 0.996840, test acc 0.793100, took 20.2 sec.\n",
      "Epoch 829: T.cost 0.008841, Train acc 0.997620, test acc 0.793800, took 19.6 sec.\n",
      "Epoch 830: T.cost 0.011060, Train acc 0.997020, test acc 0.791000, took 12.5 sec.\n",
      "Epoch 831: T.cost 0.010633, Train acc 0.997240, test acc 0.795400, took 22.6 sec.\n",
      "Epoch 832: T.cost 0.009507, Train acc 0.997640, test acc 0.791700, took 17.2 sec.\n",
      "Epoch 833: T.cost 0.007587, Train acc 0.997760, test acc 0.789300, took 15.3 sec.\n",
      "Epoch 834: T.cost 0.006923, Train acc 0.997820, test acc 0.792300, took 23.0 sec.\n",
      "Epoch 835: T.cost 0.010059, Train acc 0.997280, test acc 0.789900, took 13.9 sec.\n",
      "Epoch 836: T.cost 0.009764, Train acc 0.997420, test acc 0.788100, took 16.0 sec.\n",
      "Epoch 837: T.cost 0.013006, Train acc 0.996280, test acc 0.794300, took 23.0 sec.\n",
      "Epoch 838: T.cost 0.008754, Train acc 0.997760, test acc 0.791100, took 13.3 sec.\n",
      "New LR: 0.000655659403419\n",
      "Epoch 839: T.cost 0.007894, Train acc 0.997700, test acc 0.790500, took 18.4 sec.\n",
      "Epoch 840: T.cost 0.009077, Train acc 0.997360, test acc 0.790800, took 21.4 sec.\n",
      "Epoch 841: T.cost 0.009862, Train acc 0.997360, test acc 0.789900, took 12.5 sec.\n",
      "Epoch 842: T.cost 0.010706, Train acc 0.997040, test acc 0.790800, took 21.2 sec.\n",
      "Epoch 843: T.cost 0.007283, Train acc 0.997980, test acc 0.787800, took 18.6 sec.\n",
      "Epoch 844: T.cost 0.010342, Train acc 0.997280, test acc 0.790800, took 12.5 sec.\n",
      "Epoch 845: T.cost 0.010438, Train acc 0.997240, test acc 0.788100, took 22.3 sec.\n",
      "Epoch 846: T.cost 0.010006, Train acc 0.997420, test acc 0.790700, took 17.5 sec.\n",
      "Epoch 847: T.cost 0.008048, Train acc 0.997660, test acc 0.791100, took 13.8 sec.\n",
      "Epoch 848: T.cost 0.008854, Train acc 0.997680, test acc 0.789200, took 22.9 sec.\n",
      "Epoch 849: T.cost 0.010491, Train acc 0.997480, test acc 0.788600, took 15.5 sec.\n",
      "Epoch 850: T.cost 0.011015, Train acc 0.997120, test acc 0.786000, took 12.5 sec.\n",
      "Epoch 851: T.cost 0.007459, Train acc 0.998160, test acc 0.790300, took 17.4 sec.\n",
      "Epoch 852: T.cost 0.008861, Train acc 0.997720, test acc 0.786300, took 22.5 sec.\n",
      "Epoch 853: T.cost 0.011056, Train acc 0.997020, test acc 0.788400, took 12.5 sec.\n",
      "Epoch 854: T.cost 0.008483, Train acc 0.997500, test acc 0.796200, took 18.1 sec.\n",
      "Epoch 855: T.cost 0.008221, Train acc 0.997780, test acc 0.790400, took 21.7 sec.\n",
      "Epoch 856: T.cost 0.010225, Train acc 0.997100, test acc 0.792700, took 12.5 sec.\n",
      "Epoch 857: T.cost 0.010207, Train acc 0.997180, test acc 0.788200, took 20.2 sec.\n",
      "Epoch 858: T.cost 0.011550, Train acc 0.996780, test acc 0.789900, took 19.6 sec.\n",
      "New LR: 0.000649102822063\n",
      "Epoch 859: T.cost 0.007769, Train acc 0.997780, test acc 0.793900, took 12.5 sec.\n",
      "Epoch 860: T.cost 0.010236, Train acc 0.997360, test acc 0.790200, took 22.9 sec.\n",
      "Epoch 861: T.cost 0.008005, Train acc 0.997740, test acc 0.786600, took 17.0 sec.\n",
      "Epoch 862: T.cost 0.010161, Train acc 0.997160, test acc 0.789000, took 13.1 sec.\n",
      "Epoch 863: T.cost 0.009020, Train acc 0.997580, test acc 0.788700, took 23.0 sec.\n",
      "Epoch 864: T.cost 0.009539, Train acc 0.997240, test acc 0.789200, took 16.1 sec.\n",
      "Epoch 865: T.cost 0.007669, Train acc 0.997940, test acc 0.790100, took 15.2 sec.\n",
      "Epoch 866: T.cost 0.008919, Train acc 0.997800, test acc 0.794200, took 23.0 sec.\n",
      "Epoch 867: T.cost 0.009233, Train acc 0.997360, test acc 0.790000, took 14.1 sec.\n",
      "Epoch 868: T.cost 0.011381, Train acc 0.996680, test acc 0.797500, took 18.0 sec.\n",
      "Epoch 869: T.cost 0.008065, Train acc 0.997540, test acc 0.788300, took 21.8 sec.\n",
      "Epoch 870: T.cost 0.006143, Train acc 0.998040, test acc 0.792900, took 12.5 sec.\n",
      "Epoch 871: T.cost 0.009757, Train acc 0.997160, test acc 0.792000, took 19.1 sec.\n",
      "Epoch 872: T.cost 0.011182, Train acc 0.997180, test acc 0.790600, took 20.8 sec.\n",
      "Epoch 873: T.cost 0.007768, Train acc 0.998000, test acc 0.793400, took 12.5 sec.\n",
      "Epoch 874: T.cost 0.008754, Train acc 0.997720, test acc 0.785500, took 21.0 sec.\n",
      "Epoch 875: T.cost 0.009815, Train acc 0.996980, test acc 0.787400, took 18.8 sec.\n",
      "Epoch 876: T.cost 0.011571, Train acc 0.997400, test acc 0.788700, took 13.1 sec.\n",
      "Epoch 877: T.cost 0.008496, Train acc 0.997440, test acc 0.790500, took 22.9 sec.\n",
      "Epoch 878: T.cost 0.011410, Train acc 0.997040, test acc 0.791200, took 16.2 sec.\n",
      "New LR: 0.000642611818621\n",
      "Epoch 879: T.cost 0.009108, Train acc 0.997500, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 880: T.cost 0.009342, Train acc 0.997400, test acc 0.794000, took 13.9 sec.\n",
      "Epoch 881: T.cost 0.011702, Train acc 0.996920, test acc 0.785200, took 22.9 sec.\n",
      "Epoch 882: T.cost 0.009856, Train acc 0.997360, test acc 0.785700, took 15.4 sec.\n",
      "Epoch 883: T.cost 0.010488, Train acc 0.997200, test acc 0.791500, took 15.6 sec.\n",
      "Epoch 884: T.cost 0.007885, Train acc 0.997620, test acc 0.791400, took 22.9 sec.\n",
      "Epoch 885: T.cost 0.007570, Train acc 0.997780, test acc 0.791200, took 13.7 sec.\n",
      "Epoch 886: T.cost 0.010280, Train acc 0.997080, test acc 0.790800, took 18.4 sec.\n",
      "Epoch 887: T.cost 0.008219, Train acc 0.997740, test acc 0.788100, took 21.4 sec.\n",
      "Epoch 888: T.cost 0.012096, Train acc 0.996820, test acc 0.788900, took 12.5 sec.\n",
      "Epoch 889: T.cost 0.008781, Train acc 0.997400, test acc 0.787500, took 18.8 sec.\n",
      "Epoch 890: T.cost 0.010637, Train acc 0.997240, test acc 0.790100, took 21.0 sec.\n",
      "Epoch 891: T.cost 0.009580, Train acc 0.997460, test acc 0.791000, took 12.5 sec.\n",
      "Epoch 892: T.cost 0.008702, Train acc 0.997700, test acc 0.787900, took 20.6 sec.\n",
      "Epoch 893: T.cost 0.007086, Train acc 0.998000, test acc 0.791900, took 19.2 sec.\n",
      "Epoch 894: T.cost 0.008277, Train acc 0.997960, test acc 0.790800, took 12.7 sec.\n",
      "Epoch 895: T.cost 0.005866, Train acc 0.998280, test acc 0.796500, took 23.0 sec.\n",
      "Epoch 896: T.cost 0.009621, Train acc 0.997420, test acc 0.792600, took 16.5 sec.\n",
      "Epoch 897: T.cost 0.009177, Train acc 0.997440, test acc 0.789700, took 12.9 sec.\n",
      "Epoch 898: T.cost 0.011601, Train acc 0.997140, test acc 0.786500, took 22.9 sec.\n",
      "New LR: 0.000636185701587\n",
      "Epoch 899: T.cost 0.010245, Train acc 0.997340, test acc 0.788300, took 16.4 sec.\n",
      "Epoch 900: T.cost 0.006943, Train acc 0.997980, test acc 0.793200, took 15.0 sec.\n",
      "Epoch 901: T.cost 0.008804, Train acc 0.997460, test acc 0.794000, took 23.0 sec.\n",
      "Epoch 902: T.cost 0.009437, Train acc 0.997640, test acc 0.790300, took 14.3 sec.\n",
      "Epoch 903: T.cost 0.009507, Train acc 0.997320, test acc 0.785000, took 17.1 sec.\n",
      "Epoch 904: T.cost 0.010900, Train acc 0.997620, test acc 0.786900, took 22.7 sec.\n",
      "Epoch 905: T.cost 0.009326, Train acc 0.997100, test acc 0.787900, took 12.5 sec.\n",
      "Epoch 906: T.cost 0.008483, Train acc 0.997660, test acc 0.793400, took 18.0 sec.\n",
      "Epoch 907: T.cost 0.007423, Train acc 0.997860, test acc 0.793800, took 21.9 sec.\n",
      "Epoch 908: T.cost 0.005685, Train acc 0.998320, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 909: T.cost 0.012723, Train acc 0.996520, test acc 0.791300, took 12.5 sec.\n",
      "Epoch 910: T.cost 0.009919, Train acc 0.997540, test acc 0.787700, took 20.4 sec.\n",
      "Epoch 911: T.cost 0.009402, Train acc 0.997040, test acc 0.793600, took 19.4 sec.\n",
      "Epoch 912: T.cost 0.010310, Train acc 0.997320, test acc 0.788800, took 12.5 sec.\n",
      "Epoch 913: T.cost 0.010708, Train acc 0.997500, test acc 0.793600, took 22.6 sec.\n",
      "Epoch 914: T.cost 0.008980, Train acc 0.997500, test acc 0.788400, took 17.2 sec.\n",
      "Epoch 915: T.cost 0.008142, Train acc 0.998060, test acc 0.791900, took 12.6 sec.\n",
      "Epoch 916: T.cost 0.008770, Train acc 0.997420, test acc 0.790600, took 23.0 sec.\n",
      "Epoch 917: T.cost 0.010147, Train acc 0.997300, test acc 0.791200, took 16.7 sec.\n",
      "Epoch 918: T.cost 0.009510, Train acc 0.997780, test acc 0.789700, took 14.2 sec.\n",
      "New LR: 0.00062982383708\n",
      "Epoch 919: T.cost 0.007628, Train acc 0.997880, test acc 0.793100, took 23.0 sec.\n",
      "Epoch 920: T.cost 0.007668, Train acc 0.997800, test acc 0.791100, took 15.1 sec.\n",
      "Epoch 921: T.cost 0.009102, Train acc 0.997480, test acc 0.786700, took 16.4 sec.\n",
      "Epoch 922: T.cost 0.008961, Train acc 0.997640, test acc 0.791400, took 23.0 sec.\n",
      "Epoch 923: T.cost 0.010640, Train acc 0.997480, test acc 0.791400, took 12.8 sec.\n",
      "Epoch 924: T.cost 0.010722, Train acc 0.997080, test acc 0.792300, took 16.9 sec.\n",
      "Epoch 925: T.cost 0.008967, Train acc 0.997360, test acc 0.791500, took 22.9 sec.\n",
      "Epoch 926: T.cost 0.008543, Train acc 0.997700, test acc 0.791000, took 12.5 sec.\n",
      "Epoch 927: T.cost 0.008097, Train acc 0.997600, test acc 0.789400, took 18.3 sec.\n",
      "Epoch 928: T.cost 0.008840, Train acc 0.997840, test acc 0.789300, took 21.5 sec.\n",
      "Epoch 929: T.cost 0.010658, Train acc 0.997440, test acc 0.788500, took 12.5 sec.\n",
      "Epoch 930: T.cost 0.009338, Train acc 0.997720, test acc 0.789100, took 20.4 sec.\n",
      "Epoch 931: T.cost 0.008746, Train acc 0.998180, test acc 0.788000, took 19.4 sec.\n",
      "Epoch 932: T.cost 0.008632, Train acc 0.997700, test acc 0.786400, took 12.5 sec.\n",
      "Epoch 933: T.cost 0.009441, Train acc 0.997440, test acc 0.788000, took 20.6 sec.\n",
      "Epoch 934: T.cost 0.009601, Train acc 0.997500, test acc 0.787600, took 19.2 sec.\n",
      "Epoch 935: T.cost 0.009674, Train acc 0.997400, test acc 0.790200, took 12.5 sec.\n",
      "Epoch 936: T.cost 0.010056, Train acc 0.997240, test acc 0.786400, took 22.5 sec.\n",
      "Epoch 937: T.cost 0.009942, Train acc 0.997240, test acc 0.789900, took 17.4 sec.\n",
      "Epoch 938: T.cost 0.007080, Train acc 0.998080, test acc 0.782900, took 12.5 sec.\n",
      "New LR: 0.000623525591218\n",
      "Epoch 939: T.cost 0.008518, Train acc 0.997580, test acc 0.788700, took 13.4 sec.\n",
      "Epoch 940: T.cost 0.010238, Train acc 0.997200, test acc 0.786000, took 23.0 sec.\n",
      "Epoch 941: T.cost 0.010443, Train acc 0.997220, test acc 0.786000, took 15.8 sec.\n",
      "Epoch 942: T.cost 0.009230, Train acc 0.997500, test acc 0.791200, took 13.8 sec.\n",
      "Epoch 943: T.cost 0.007891, Train acc 0.997660, test acc 0.789900, took 23.0 sec.\n",
      "Epoch 944: T.cost 0.008650, Train acc 0.997720, test acc 0.793800, took 15.5 sec.\n",
      "Epoch 945: T.cost 0.008062, Train acc 0.997900, test acc 0.796000, took 15.4 sec.\n",
      "Epoch 946: T.cost 0.009460, Train acc 0.997480, test acc 0.790500, took 22.9 sec.\n",
      "Epoch 947: T.cost 0.007333, Train acc 0.997740, test acc 0.792900, took 13.9 sec.\n",
      "Epoch 948: T.cost 0.009539, Train acc 0.997420, test acc 0.781800, took 17.0 sec.\n",
      "Epoch 949: T.cost 0.009223, Train acc 0.997760, test acc 0.788900, took 22.8 sec.\n",
      "Epoch 950: T.cost 0.006253, Train acc 0.998220, test acc 0.787700, took 12.5 sec.\n",
      "Epoch 951: T.cost 0.007875, Train acc 0.997820, test acc 0.783000, took 17.0 sec.\n",
      "Epoch 952: T.cost 0.010776, Train acc 0.997380, test acc 0.788700, took 22.8 sec.\n",
      "Epoch 953: T.cost 0.009097, Train acc 0.997220, test acc 0.788300, took 12.5 sec.\n",
      "Epoch 954: T.cost 0.011605, Train acc 0.996900, test acc 0.788000, took 18.3 sec.\n",
      "Epoch 955: T.cost 0.009604, Train acc 0.997580, test acc 0.792900, took 21.5 sec.\n",
      "Epoch 956: T.cost 0.008864, Train acc 0.997740, test acc 0.787100, took 12.5 sec.\n",
      "Epoch 957: T.cost 0.009187, Train acc 0.997680, test acc 0.792700, took 20.3 sec.\n",
      "Epoch 958: T.cost 0.010157, Train acc 0.997240, test acc 0.787600, took 19.6 sec.\n",
      "New LR: 0.000617290330119\n",
      "Epoch 959: T.cost 0.007195, Train acc 0.998080, test acc 0.788600, took 12.5 sec.\n",
      "Epoch 960: T.cost 0.009175, Train acc 0.997680, test acc 0.791900, took 20.2 sec.\n",
      "Epoch 961: T.cost 0.008263, Train acc 0.997840, test acc 0.791500, took 19.7 sec.\n",
      "Epoch 962: T.cost 0.006400, Train acc 0.998220, test acc 0.794500, took 12.5 sec.\n",
      "Epoch 963: T.cost 0.008070, Train acc 0.997800, test acc 0.789300, took 21.4 sec.\n",
      "Epoch 964: T.cost 0.008763, Train acc 0.997640, test acc 0.794900, took 18.4 sec.\n",
      "Epoch 965: T.cost 0.007992, Train acc 0.997720, test acc 0.785000, took 12.5 sec.\n",
      "Epoch 966: T.cost 0.008706, Train acc 0.997760, test acc 0.790800, took 22.2 sec.\n",
      "Epoch 967: T.cost 0.005772, Train acc 0.998300, test acc 0.788400, took 17.6 sec.\n",
      "Epoch 968: T.cost 0.006981, Train acc 0.998200, test acc 0.787500, took 12.5 sec.\n",
      "Epoch 969: T.cost 0.010457, Train acc 0.997340, test acc 0.789900, took 12.5 sec.\n",
      "Epoch 970: T.cost 0.007688, Train acc 0.997880, test acc 0.792500, took 21.7 sec.\n",
      "Epoch 971: T.cost 0.007428, Train acc 0.997840, test acc 0.788900, took 18.2 sec.\n",
      "Epoch 972: T.cost 0.010257, Train acc 0.997140, test acc 0.793700, took 12.5 sec.\n",
      "Epoch 973: T.cost 0.008613, Train acc 0.997480, test acc 0.788600, took 20.9 sec.\n",
      "Epoch 974: T.cost 0.010488, Train acc 0.997180, test acc 0.789100, took 18.9 sec.\n",
      "Epoch 975: T.cost 0.007323, Train acc 0.998060, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 976: T.cost 0.007250, Train acc 0.997820, test acc 0.788300, took 21.9 sec.\n",
      "Epoch 977: T.cost 0.008539, Train acc 0.997660, test acc 0.790200, took 18.0 sec.\n",
      "Epoch 978: T.cost 0.008566, Train acc 0.997620, test acc 0.788200, took 12.5 sec.\n",
      "New LR: 0.000611117419903\n",
      "Epoch 979: T.cost 0.008006, Train acc 0.998060, test acc 0.791800, took 20.4 sec.\n",
      "Epoch 980: T.cost 0.009252, Train acc 0.997780, test acc 0.790300, took 19.5 sec.\n",
      "Epoch 981: T.cost 0.008622, Train acc 0.997820, test acc 0.791800, took 12.5 sec.\n",
      "Epoch 982: T.cost 0.010333, Train acc 0.997340, test acc 0.790100, took 19.5 sec.\n",
      "Epoch 983: T.cost 0.009489, Train acc 0.997580, test acc 0.793000, took 20.4 sec.\n",
      "Epoch 984: T.cost 0.006521, Train acc 0.998240, test acc 0.796300, took 12.5 sec.\n",
      "Epoch 985: T.cost 0.007426, Train acc 0.998040, test acc 0.794100, took 18.1 sec.\n",
      "Epoch 986: T.cost 0.009570, Train acc 0.997380, test acc 0.789900, took 21.8 sec.\n",
      "Epoch 987: T.cost 0.009338, Train acc 0.997660, test acc 0.788100, took 12.5 sec.\n",
      "Epoch 988: T.cost 0.008038, Train acc 0.998080, test acc 0.788700, took 13.3 sec.\n",
      "Epoch 989: T.cost 0.007592, Train acc 0.997880, test acc 0.792800, took 23.0 sec.\n",
      "Epoch 990: T.cost 0.007271, Train acc 0.998020, test acc 0.792100, took 16.2 sec.\n",
      "Epoch 991: T.cost 0.010245, Train acc 0.997660, test acc 0.795200, took 12.7 sec.\n",
      "Epoch 992: T.cost 0.008341, Train acc 0.997700, test acc 0.792800, took 21.4 sec.\n",
      "Epoch 993: T.cost 0.008875, Train acc 0.997680, test acc 0.789900, took 18.5 sec.\n",
      "Epoch 994: T.cost 0.008904, Train acc 0.997860, test acc 0.796200, took 12.5 sec.\n",
      "Epoch 995: T.cost 0.009912, Train acc 0.997380, test acc 0.793100, took 20.8 sec.\n",
      "Epoch 996: T.cost 0.008993, Train acc 0.997600, test acc 0.792400, took 19.1 sec.\n",
      "Epoch 997: T.cost 0.009025, Train acc 0.997640, test acc 0.791200, took 12.5 sec.\n",
      "Epoch 998: T.cost 0.010438, Train acc 0.997400, test acc 0.790300, took 14.8 sec.\n",
      "New LR: 0.000605006226688\n",
      "Epoch 999: T.cost 0.009726, Train acc 0.997580, test acc 0.790800, took 22.8 sec.\n",
      "Epoch 1000: T.cost 0.006616, Train acc 0.998120, test acc 0.794800, took 14.7 sec.\n",
      "Epoch 1001: T.cost 0.006409, Train acc 0.998240, test acc 0.793300, took 12.5 sec.\n",
      "Epoch 1002: T.cost 0.006827, Train acc 0.997960, test acc 0.792600, took 12.6 sec.\n",
      "Epoch 1003: T.cost 0.008871, Train acc 0.997940, test acc 0.790400, took 22.9 sec.\n",
      "Epoch 1004: T.cost 0.010829, Train acc 0.997220, test acc 0.790100, took 16.7 sec.\n",
      "Epoch 1005: T.cost 0.008573, Train acc 0.997920, test acc 0.793100, took 12.5 sec.\n",
      "Epoch 1006: T.cost 0.007574, Train acc 0.997720, test acc 0.790800, took 19.2 sec.\n",
      "Epoch 1007: T.cost 0.008457, Train acc 0.997940, test acc 0.793800, took 20.7 sec.\n",
      "Epoch 1008: T.cost 0.009061, Train acc 0.997560, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1009: T.cost 0.008179, Train acc 0.997640, test acc 0.792300, took 14.3 sec.\n",
      "Epoch 1010: T.cost 0.009311, Train acc 0.997380, test acc 0.797000, took 22.8 sec.\n",
      "Epoch 1011: T.cost 0.008866, Train acc 0.997700, test acc 0.793800, took 15.2 sec.\n",
      "Epoch 1012: T.cost 0.007885, Train acc 0.997920, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 1013: T.cost 0.007641, Train acc 0.997780, test acc 0.794200, took 19.3 sec.\n",
      "Epoch 1014: T.cost 0.008238, Train acc 0.997960, test acc 0.796900, took 20.6 sec.\n",
      "Epoch 1015: T.cost 0.007067, Train acc 0.998000, test acc 0.796400, took 12.5 sec.\n",
      "Epoch 1016: T.cost 0.007149, Train acc 0.998040, test acc 0.788500, took 15.0 sec.\n",
      "Epoch 1017: T.cost 0.010120, Train acc 0.997220, test acc 0.788900, took 22.9 sec.\n",
      "Epoch 1018: T.cost 0.008919, Train acc 0.997820, test acc 0.788800, took 14.4 sec.\n",
      "New LR: 0.000598956174217\n",
      "Epoch 1019: T.cost 0.007788, Train acc 0.997880, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1020: T.cost 0.008187, Train acc 0.997680, test acc 0.787000, took 17.7 sec.\n",
      "Epoch 1021: T.cost 0.006341, Train acc 0.998260, test acc 0.788900, took 22.1 sec.\n",
      "Epoch 1022: T.cost 0.006319, Train acc 0.998080, test acc 0.792700, took 12.5 sec.\n",
      "Epoch 1023: T.cost 0.008439, Train acc 0.997720, test acc 0.792800, took 12.5 sec.\n",
      "Epoch 1024: T.cost 0.008179, Train acc 0.997880, test acc 0.795300, took 20.6 sec.\n",
      "Epoch 1025: T.cost 0.007715, Train acc 0.997840, test acc 0.792200, took 19.2 sec.\n",
      "Epoch 1026: T.cost 0.007675, Train acc 0.997840, test acc 0.791300, took 12.5 sec.\n",
      "Epoch 1027: T.cost 0.009116, Train acc 0.997460, test acc 0.792600, took 12.5 sec.\n",
      "Epoch 1028: T.cost 0.006211, Train acc 0.998100, test acc 0.790500, took 21.0 sec.\n",
      "Epoch 1029: T.cost 0.007894, Train acc 0.997940, test acc 0.789900, took 18.9 sec.\n",
      "Epoch 1030: T.cost 0.006012, Train acc 0.998240, test acc 0.793500, took 12.5 sec.\n",
      "Epoch 1031: T.cost 0.005393, Train acc 0.998340, test acc 0.795500, took 12.5 sec.\n",
      "Epoch 1032: T.cost 0.008057, Train acc 0.998240, test acc 0.794900, took 22.1 sec.\n",
      "Epoch 1033: T.cost 0.008850, Train acc 0.997880, test acc 0.796500, took 17.7 sec.\n",
      "Epoch 1034: T.cost 0.009673, Train acc 0.997500, test acc 0.788700, took 12.5 sec.\n",
      "Epoch 1035: T.cost 0.007537, Train acc 0.997820, test acc 0.791000, took 12.5 sec.\n",
      "Epoch 1036: T.cost 0.008782, Train acc 0.997740, test acc 0.790400, took 20.5 sec.\n",
      "Epoch 1037: T.cost 0.008601, Train acc 0.997640, test acc 0.798100, took 19.3 sec.\n",
      "Epoch 1038: T.cost 0.007967, Train acc 0.998040, test acc 0.789400, took 12.5 sec.\n",
      "New LR: 0.00059296662861\n",
      "Epoch 1039: T.cost 0.008167, Train acc 0.997980, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 1040: T.cost 0.008377, Train acc 0.997780, test acc 0.795900, took 15.7 sec.\n",
      "Epoch 1041: T.cost 0.008843, Train acc 0.997620, test acc 0.793000, took 22.7 sec.\n",
      "Epoch 1042: T.cost 0.006209, Train acc 0.998260, test acc 0.795800, took 13.8 sec.\n",
      "Epoch 1043: T.cost 0.007224, Train acc 0.998120, test acc 0.799800, took 12.5 sec.\n",
      "Epoch 1044: T.cost 0.006252, Train acc 0.998140, test acc 0.791400, took 15.8 sec.\n",
      "Epoch 1045: T.cost 0.008855, Train acc 0.997820, test acc 0.792200, took 22.6 sec.\n",
      "Epoch 1046: T.cost 0.007270, Train acc 0.998080, test acc 0.792200, took 13.8 sec.\n",
      "Epoch 1047: T.cost 0.008527, Train acc 0.997640, test acc 0.791400, took 12.5 sec.\n",
      "Epoch 1048: T.cost 0.007505, Train acc 0.997980, test acc 0.794300, took 15.8 sec.\n",
      "Epoch 1049: T.cost 0.008844, Train acc 0.997660, test acc 0.794200, took 22.7 sec.\n",
      "Epoch 1050: T.cost 0.006951, Train acc 0.998220, test acc 0.794100, took 13.7 sec.\n",
      "Epoch 1051: T.cost 0.007505, Train acc 0.998060, test acc 0.787400, took 12.5 sec.\n",
      "Epoch 1052: T.cost 0.004730, Train acc 0.998340, test acc 0.797100, took 15.0 sec.\n",
      "Epoch 1053: T.cost 0.011000, Train acc 0.997380, test acc 0.788700, took 22.7 sec.\n",
      "Epoch 1054: T.cost 0.009506, Train acc 0.997660, test acc 0.792500, took 14.6 sec.\n",
      "Epoch 1055: T.cost 0.008798, Train acc 0.997660, test acc 0.788700, took 12.5 sec.\n",
      "Epoch 1056: T.cost 0.007737, Train acc 0.997780, test acc 0.790400, took 14.7 sec.\n",
      "Epoch 1057: T.cost 0.009521, Train acc 0.997240, test acc 0.790300, took 22.8 sec.\n",
      "Epoch 1058: T.cost 0.007430, Train acc 0.998040, test acc 0.790700, took 14.7 sec.\n",
      "New LR: 0.000587036955985\n",
      "Epoch 1059: T.cost 0.007349, Train acc 0.998060, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 1060: T.cost 0.006596, Train acc 0.998160, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1061: T.cost 0.006776, Train acc 0.998040, test acc 0.788800, took 19.8 sec.\n",
      "Epoch 1062: T.cost 0.009105, Train acc 0.997680, test acc 0.792800, took 20.0 sec.\n",
      "Epoch 1063: T.cost 0.008558, Train acc 0.997720, test acc 0.794400, took 12.5 sec.\n",
      "Epoch 1064: T.cost 0.006838, Train acc 0.997980, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1065: T.cost 0.005636, Train acc 0.998220, test acc 0.793700, took 16.8 sec.\n",
      "Epoch 1066: T.cost 0.007352, Train acc 0.998200, test acc 0.794500, took 23.0 sec.\n",
      "Epoch 1067: T.cost 0.008812, Train acc 0.997740, test acc 0.787100, took 12.6 sec.\n",
      "Epoch 1068: T.cost 0.008185, Train acc 0.997620, test acc 0.793400, took 12.5 sec.\n",
      "Epoch 1069: T.cost 0.007979, Train acc 0.997800, test acc 0.790700, took 12.5 sec.\n",
      "Epoch 1070: T.cost 0.008275, Train acc 0.997780, test acc 0.792700, took 21.6 sec.\n",
      "Epoch 1071: T.cost 0.009016, Train acc 0.997780, test acc 0.791900, took 18.2 sec.\n",
      "Epoch 1072: T.cost 0.008179, Train acc 0.998120, test acc 0.791200, took 12.5 sec.\n",
      "Epoch 1073: T.cost 0.008962, Train acc 0.997620, test acc 0.793500, took 12.5 sec.\n",
      "Epoch 1074: T.cost 0.007247, Train acc 0.997960, test acc 0.796100, took 13.3 sec.\n",
      "Epoch 1075: T.cost 0.008146, Train acc 0.997840, test acc 0.790000, took 22.8 sec.\n",
      "Epoch 1076: T.cost 0.005592, Train acc 0.998520, test acc 0.792900, took 16.1 sec.\n",
      "Epoch 1077: T.cost 0.006476, Train acc 0.998220, test acc 0.794300, took 12.5 sec.\n",
      "Epoch 1078: T.cost 0.008902, Train acc 0.997500, test acc 0.792900, took 12.5 sec.\n",
      "New LR: 0.000581166580087\n",
      "Epoch 1079: T.cost 0.006888, Train acc 0.998080, test acc 0.793400, took 13.4 sec.\n",
      "Epoch 1080: T.cost 0.006761, Train acc 0.998280, test acc 0.790900, took 22.6 sec.\n",
      "Epoch 1081: T.cost 0.006996, Train acc 0.998000, test acc 0.790100, took 16.2 sec.\n",
      "Epoch 1082: T.cost 0.008383, Train acc 0.997760, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 1083: T.cost 0.007482, Train acc 0.997920, test acc 0.785200, took 12.5 sec.\n",
      "Epoch 1084: T.cost 0.008974, Train acc 0.997720, test acc 0.791700, took 12.5 sec.\n",
      "Epoch 1085: T.cost 0.009407, Train acc 0.997660, test acc 0.790700, took 13.1 sec.\n",
      "Epoch 1086: T.cost 0.007506, Train acc 0.997900, test acc 0.789900, took 22.9 sec.\n",
      "Epoch 1087: T.cost 0.007860, Train acc 0.997940, test acc 0.792100, took 16.2 sec.\n",
      "Epoch 1088: T.cost 0.008639, Train acc 0.998000, test acc 0.794400, took 12.5 sec.\n",
      "Epoch 1089: T.cost 0.007347, Train acc 0.998220, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1090: T.cost 0.006728, Train acc 0.998160, test acc 0.795600, took 12.5 sec.\n",
      "Epoch 1091: T.cost 0.010152, Train acc 0.997660, test acc 0.795200, took 18.2 sec.\n",
      "Epoch 1092: T.cost 0.008706, Train acc 0.997800, test acc 0.799400, took 21.7 sec.\n",
      "Epoch 1093: T.cost 0.007067, Train acc 0.998160, test acc 0.796000, took 12.5 sec.\n",
      "Epoch 1094: T.cost 0.009558, Train acc 0.997620, test acc 0.794200, took 12.5 sec.\n",
      "Epoch 1095: T.cost 0.006148, Train acc 0.998080, test acc 0.797000, took 12.5 sec.\n",
      "Epoch 1096: T.cost 0.005859, Train acc 0.998440, test acc 0.789000, took 19.9 sec.\n",
      "Epoch 1097: T.cost 0.007275, Train acc 0.998260, test acc 0.794200, took 20.0 sec.\n",
      "Epoch 1098: T.cost 0.006874, Train acc 0.998160, test acc 0.796100, took 12.5 sec.\n",
      "New LR: 0.000575354924658\n",
      "Epoch 1099: T.cost 0.010073, Train acc 0.997440, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 1100: T.cost 0.006837, Train acc 0.998020, test acc 0.797700, took 12.5 sec.\n",
      "Epoch 1101: T.cost 0.007742, Train acc 0.997880, test acc 0.795300, took 15.4 sec.\n",
      "Epoch 1102: T.cost 0.007934, Train acc 0.998160, test acc 0.790500, took 22.7 sec.\n",
      "Epoch 1103: T.cost 0.007021, Train acc 0.998020, test acc 0.792400, took 14.2 sec.\n",
      "Epoch 1104: T.cost 0.006204, Train acc 0.998160, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1105: T.cost 0.006439, Train acc 0.998140, test acc 0.793600, took 12.5 sec.\n",
      "Epoch 1106: T.cost 0.008319, Train acc 0.997880, test acc 0.790300, took 12.5 sec.\n",
      "Epoch 1107: T.cost 0.007690, Train acc 0.998000, test acc 0.794600, took 18.4 sec.\n",
      "Epoch 1108: T.cost 0.006068, Train acc 0.998340, test acc 0.789800, took 21.4 sec.\n",
      "Epoch 1109: T.cost 0.005247, Train acc 0.998420, test acc 0.793200, took 12.5 sec.\n",
      "Epoch 1110: T.cost 0.008287, Train acc 0.997800, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 1111: T.cost 0.007013, Train acc 0.998020, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1112: T.cost 0.010986, Train acc 0.997240, test acc 0.787800, took 12.5 sec.\n",
      "Epoch 1113: T.cost 0.008261, Train acc 0.997820, test acc 0.787000, took 21.0 sec.\n",
      "Epoch 1114: T.cost 0.006217, Train acc 0.998220, test acc 0.786600, took 18.8 sec.\n",
      "Epoch 1115: T.cost 0.007677, Train acc 0.997940, test acc 0.789700, took 12.5 sec.\n",
      "Epoch 1116: T.cost 0.010255, Train acc 0.997560, test acc 0.788900, took 12.5 sec.\n",
      "Epoch 1117: T.cost 0.009294, Train acc 0.997800, test acc 0.791900, took 12.5 sec.\n",
      "Epoch 1118: T.cost 0.005818, Train acc 0.998420, test acc 0.795400, took 13.8 sec.\n",
      "New LR: 0.000569601355819\n",
      "Epoch 1119: T.cost 0.008721, Train acc 0.998220, test acc 0.790500, took 22.9 sec.\n",
      "Epoch 1120: T.cost 0.007426, Train acc 0.998100, test acc 0.794900, took 15.6 sec.\n",
      "Epoch 1121: T.cost 0.005219, Train acc 0.998660, test acc 0.790800, took 12.5 sec.\n",
      "Epoch 1122: T.cost 0.006921, Train acc 0.998080, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1123: T.cost 0.006705, Train acc 0.997880, test acc 0.796200, took 12.5 sec.\n",
      "Epoch 1124: T.cost 0.005681, Train acc 0.998420, test acc 0.791700, took 17.2 sec.\n",
      "Epoch 1125: T.cost 0.007719, Train acc 0.998020, test acc 0.793400, took 22.7 sec.\n",
      "Epoch 1126: T.cost 0.005597, Train acc 0.998580, test acc 0.795100, took 12.5 sec.\n",
      "Epoch 1127: T.cost 0.008814, Train acc 0.997660, test acc 0.790100, took 12.5 sec.\n",
      "Epoch 1128: T.cost 0.008131, Train acc 0.998200, test acc 0.791900, took 12.5 sec.\n",
      "Epoch 1129: T.cost 0.008683, Train acc 0.997880, test acc 0.793500, took 12.5 sec.\n",
      "Epoch 1130: T.cost 0.006031, Train acc 0.998380, test acc 0.793700, took 19.8 sec.\n",
      "Epoch 1131: T.cost 0.008092, Train acc 0.998060, test acc 0.792700, took 20.1 sec.\n",
      "Epoch 1132: T.cost 0.008015, Train acc 0.998140, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 1133: T.cost 0.009336, Train acc 0.997940, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1134: T.cost 0.006791, Train acc 0.997920, test acc 0.790900, took 12.5 sec.\n",
      "Epoch 1135: T.cost 0.005736, Train acc 0.998420, test acc 0.794400, took 12.5 sec.\n",
      "Epoch 1136: T.cost 0.006416, Train acc 0.998380, test acc 0.790100, took 17.7 sec.\n",
      "Epoch 1137: T.cost 0.006996, Train acc 0.998280, test acc 0.789100, took 22.0 sec.\n",
      "Epoch 1138: T.cost 0.007607, Train acc 0.998000, test acc 0.791100, took 12.5 sec.\n",
      "New LR: 0.000563905354938\n",
      "Epoch 1139: T.cost 0.009154, Train acc 0.997620, test acc 0.789500, took 12.5 sec.\n",
      "Epoch 1140: T.cost 0.007602, Train acc 0.997920, test acc 0.787400, took 12.5 sec.\n",
      "Epoch 1141: T.cost 0.006454, Train acc 0.998100, test acc 0.786600, took 12.5 sec.\n",
      "Epoch 1142: T.cost 0.007181, Train acc 0.998100, test acc 0.792000, took 12.5 sec.\n",
      "Epoch 1143: T.cost 0.008488, Train acc 0.997720, test acc 0.792700, took 18.8 sec.\n",
      "Epoch 1144: T.cost 0.006625, Train acc 0.998280, test acc 0.791600, took 21.0 sec.\n",
      "Epoch 1145: T.cost 0.008775, Train acc 0.997660, test acc 0.790900, took 12.5 sec.\n",
      "Epoch 1146: T.cost 0.005681, Train acc 0.998520, test acc 0.791400, took 12.5 sec.\n",
      "Epoch 1147: T.cost 0.005450, Train acc 0.998180, test acc 0.790100, took 12.5 sec.\n",
      "Epoch 1148: T.cost 0.008293, Train acc 0.997960, test acc 0.789900, took 12.5 sec.\n",
      "Epoch 1149: T.cost 0.008068, Train acc 0.997620, test acc 0.790200, took 16.1 sec.\n",
      "Epoch 1150: T.cost 0.006829, Train acc 0.998300, test acc 0.790600, took 22.6 sec.\n",
      "Epoch 1151: T.cost 0.006325, Train acc 0.998320, test acc 0.786100, took 13.5 sec.\n",
      "Epoch 1152: T.cost 0.005803, Train acc 0.998340, test acc 0.789900, took 12.5 sec.\n",
      "Epoch 1153: T.cost 0.006948, Train acc 0.997960, test acc 0.784700, took 12.5 sec.\n",
      "Epoch 1154: T.cost 0.007453, Train acc 0.997980, test acc 0.786400, took 12.5 sec.\n",
      "Epoch 1155: T.cost 0.006285, Train acc 0.998420, test acc 0.789300, took 15.7 sec.\n",
      "Epoch 1156: T.cost 0.006014, Train acc 0.998280, test acc 0.788000, took 22.7 sec.\n",
      "Epoch 1157: T.cost 0.005962, Train acc 0.998600, test acc 0.792100, took 13.8 sec.\n",
      "Epoch 1158: T.cost 0.007146, Train acc 0.998200, test acc 0.790600, took 12.5 sec.\n",
      "New LR: 0.000558266288135\n",
      "Epoch 1159: T.cost 0.009409, Train acc 0.998060, test acc 0.790500, took 12.5 sec.\n",
      "Epoch 1160: T.cost 0.006675, Train acc 0.998100, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1161: T.cost 0.009951, Train acc 0.997720, test acc 0.789300, took 17.5 sec.\n",
      "Epoch 1162: T.cost 0.008988, Train acc 0.997720, test acc 0.791900, took 22.4 sec.\n",
      "Epoch 1163: T.cost 0.009595, Train acc 0.997580, test acc 0.797800, took 12.5 sec.\n",
      "Epoch 1164: T.cost 0.006351, Train acc 0.998200, test acc 0.796100, took 12.5 sec.\n",
      "Epoch 1165: T.cost 0.006827, Train acc 0.998260, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 1166: T.cost 0.006543, Train acc 0.998280, test acc 0.794300, took 12.5 sec.\n",
      "Epoch 1167: T.cost 0.007673, Train acc 0.998320, test acc 0.796500, took 18.6 sec.\n",
      "Epoch 1168: T.cost 0.005440, Train acc 0.998580, test acc 0.792100, took 21.2 sec.\n",
      "Epoch 1169: T.cost 0.007801, Train acc 0.998280, test acc 0.789900, took 12.5 sec.\n",
      "Epoch 1170: T.cost 0.008556, Train acc 0.998080, test acc 0.791800, took 12.5 sec.\n",
      "Epoch 1171: T.cost 0.007964, Train acc 0.997740, test acc 0.794200, took 12.5 sec.\n",
      "Epoch 1172: T.cost 0.006385, Train acc 0.998140, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1173: T.cost 0.006685, Train acc 0.998160, test acc 0.793400, took 18.0 sec.\n",
      "Epoch 1174: T.cost 0.005520, Train acc 0.998400, test acc 0.795400, took 21.8 sec.\n",
      "Epoch 1175: T.cost 0.006720, Train acc 0.998240, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1176: T.cost 0.008071, Train acc 0.997720, test acc 0.787500, took 12.5 sec.\n",
      "Epoch 1177: T.cost 0.007225, Train acc 0.998020, test acc 0.791400, took 12.5 sec.\n",
      "Epoch 1178: T.cost 0.005580, Train acc 0.998280, test acc 0.792100, took 12.5 sec.\n",
      "New LR: 0.000552683636779\n",
      "Epoch 1179: T.cost 0.005824, Train acc 0.998260, test acc 0.785800, took 16.0 sec.\n",
      "Epoch 1180: T.cost 0.007273, Train acc 0.998160, test acc 0.791300, took 22.8 sec.\n",
      "Epoch 1181: T.cost 0.008333, Train acc 0.997800, test acc 0.789900, took 13.4 sec.\n",
      "Epoch 1182: T.cost 0.005892, Train acc 0.998280, test acc 0.793900, took 12.5 sec.\n",
      "Epoch 1183: T.cost 0.008588, Train acc 0.998020, test acc 0.788900, took 12.5 sec.\n",
      "Epoch 1184: T.cost 0.007843, Train acc 0.998080, test acc 0.791900, took 12.5 sec.\n",
      "Epoch 1185: T.cost 0.006874, Train acc 0.998440, test acc 0.788900, took 14.7 sec.\n",
      "Epoch 1186: T.cost 0.008007, Train acc 0.998080, test acc 0.791700, took 22.6 sec.\n",
      "Epoch 1187: T.cost 0.006515, Train acc 0.998380, test acc 0.794800, took 14.9 sec.\n",
      "Epoch 1188: T.cost 0.009621, Train acc 0.997760, test acc 0.791600, took 12.5 sec.\n",
      "Epoch 1189: T.cost 0.004821, Train acc 0.998520, test acc 0.793100, took 12.5 sec.\n",
      "Epoch 1190: T.cost 0.007129, Train acc 0.998020, test acc 0.785900, took 12.5 sec.\n",
      "Epoch 1191: T.cost 0.008032, Train acc 0.997800, test acc 0.789200, took 13.5 sec.\n",
      "Epoch 1192: T.cost 0.006328, Train acc 0.998280, test acc 0.795600, took 22.9 sec.\n",
      "Epoch 1193: T.cost 0.005796, Train acc 0.998480, test acc 0.796400, took 15.8 sec.\n",
      "Epoch 1194: T.cost 0.005641, Train acc 0.998500, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 1195: T.cost 0.005585, Train acc 0.998460, test acc 0.796800, took 12.5 sec.\n",
      "Epoch 1196: T.cost 0.007793, Train acc 0.998020, test acc 0.790300, took 12.5 sec.\n",
      "Epoch 1197: T.cost 0.005790, Train acc 0.998320, test acc 0.792800, took 12.5 sec.\n",
      "Epoch 1198: T.cost 0.007164, Train acc 0.998380, test acc 0.796200, took 22.5 sec.\n",
      "New LR: 0.000547156824614\n",
      "Epoch 1199: T.cost 0.006425, Train acc 0.998160, test acc 0.790300, took 17.3 sec.\n",
      "Epoch 1200: T.cost 0.004824, Train acc 0.998480, test acc 0.789000, took 12.5 sec.\n",
      "Epoch 1201: T.cost 0.006349, Train acc 0.998240, test acc 0.789400, took 12.5 sec.\n",
      "Epoch 1202: T.cost 0.006895, Train acc 0.998400, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1203: T.cost 0.006100, Train acc 0.998320, test acc 0.787300, took 12.5 sec.\n",
      "Epoch 1204: T.cost 0.007131, Train acc 0.998040, test acc 0.788100, took 12.5 sec.\n",
      "Epoch 1205: T.cost 0.008416, Train acc 0.997940, test acc 0.791100, took 19.8 sec.\n",
      "Epoch 1206: T.cost 0.005522, Train acc 0.998600, test acc 0.794000, took 20.0 sec.\n",
      "Epoch 1207: T.cost 0.007264, Train acc 0.998280, test acc 0.786800, took 12.5 sec.\n",
      "Epoch 1208: T.cost 0.010545, Train acc 0.997580, test acc 0.790900, took 12.5 sec.\n",
      "Epoch 1209: T.cost 0.006378, Train acc 0.998180, test acc 0.792900, took 12.5 sec.\n",
      "Epoch 1210: T.cost 0.008152, Train acc 0.997940, test acc 0.797700, took 12.5 sec.\n",
      "Epoch 1211: T.cost 0.004542, Train acc 0.998760, test acc 0.794100, took 15.8 sec.\n",
      "Epoch 1212: T.cost 0.007546, Train acc 0.998180, test acc 0.795400, took 22.7 sec.\n",
      "Epoch 1213: T.cost 0.008583, Train acc 0.998060, test acc 0.790300, took 13.7 sec.\n",
      "Epoch 1214: T.cost 0.007178, Train acc 0.998180, test acc 0.791700, took 12.5 sec.\n",
      "Epoch 1215: T.cost 0.006823, Train acc 0.998040, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1216: T.cost 0.007363, Train acc 0.997880, test acc 0.794200, took 12.5 sec.\n",
      "Epoch 1217: T.cost 0.006980, Train acc 0.998080, test acc 0.789500, took 13.8 sec.\n",
      "Epoch 1218: T.cost 0.006569, Train acc 0.998480, test acc 0.797900, took 22.9 sec.\n",
      "New LR: 0.000541685275384\n",
      "Epoch 1219: T.cost 0.006351, Train acc 0.998460, test acc 0.788700, took 15.6 sec.\n",
      "Epoch 1220: T.cost 0.007203, Train acc 0.998440, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1221: T.cost 0.006741, Train acc 0.998280, test acc 0.798500, took 12.5 sec.\n",
      "Epoch 1222: T.cost 0.005875, Train acc 0.998500, test acc 0.795700, took 12.5 sec.\n",
      "Epoch 1223: T.cost 0.007984, Train acc 0.998140, test acc 0.788500, took 12.5 sec.\n",
      "Epoch 1224: T.cost 0.008015, Train acc 0.998020, test acc 0.788500, took 20.0 sec.\n",
      "Epoch 1225: T.cost 0.006630, Train acc 0.998460, test acc 0.786000, took 19.9 sec.\n",
      "Epoch 1226: T.cost 0.007935, Train acc 0.998020, test acc 0.794000, took 12.5 sec.\n",
      "Epoch 1227: T.cost 0.006417, Train acc 0.998280, test acc 0.787600, took 12.5 sec.\n",
      "Epoch 1228: T.cost 0.006924, Train acc 0.998280, test acc 0.792600, took 12.5 sec.\n",
      "Epoch 1229: T.cost 0.007340, Train acc 0.997960, test acc 0.792400, took 12.5 sec.\n",
      "Epoch 1230: T.cost 0.005366, Train acc 0.998640, test acc 0.795100, took 18.2 sec.\n",
      "Epoch 1231: T.cost 0.007382, Train acc 0.998080, test acc 0.791000, took 21.6 sec.\n",
      "Epoch 1232: T.cost 0.008233, Train acc 0.997960, test acc 0.796000, took 12.5 sec.\n",
      "Epoch 1233: T.cost 0.003904, Train acc 0.998880, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1234: T.cost 0.006561, Train acc 0.998200, test acc 0.795200, took 12.5 sec.\n",
      "Epoch 1235: T.cost 0.006870, Train acc 0.998260, test acc 0.793300, took 12.5 sec.\n",
      "Epoch 1236: T.cost 0.006836, Train acc 0.998160, test acc 0.789300, took 14.3 sec.\n",
      "Epoch 1237: T.cost 0.006281, Train acc 0.998120, test acc 0.792300, took 22.7 sec.\n",
      "Epoch 1238: T.cost 0.006072, Train acc 0.998560, test acc 0.789100, took 15.3 sec.\n",
      "New LR: 0.000536268412834\n",
      "Epoch 1239: T.cost 0.005929, Train acc 0.998460, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 1240: T.cost 0.005992, Train acc 0.998300, test acc 0.793500, took 12.5 sec.\n",
      "Epoch 1241: T.cost 0.007914, Train acc 0.998280, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 1242: T.cost 0.006961, Train acc 0.998040, test acc 0.795900, took 12.5 sec.\n",
      "Epoch 1243: T.cost 0.006482, Train acc 0.998140, test acc 0.791500, took 22.4 sec.\n",
      "Epoch 1244: T.cost 0.006995, Train acc 0.998080, test acc 0.788900, took 17.5 sec.\n",
      "Epoch 1245: T.cost 0.005414, Train acc 0.998560, test acc 0.794600, took 12.5 sec.\n",
      "Epoch 1246: T.cost 0.006401, Train acc 0.998160, test acc 0.793600, took 12.5 sec.\n",
      "Epoch 1247: T.cost 0.006520, Train acc 0.998240, test acc 0.793200, took 12.5 sec.\n",
      "Epoch 1248: T.cost 0.008102, Train acc 0.997800, test acc 0.791000, took 12.5 sec.\n",
      "Epoch 1249: T.cost 0.008064, Train acc 0.997880, test acc 0.790500, took 15.8 sec.\n",
      "Epoch 1250: T.cost 0.006607, Train acc 0.998140, test acc 0.789300, took 22.6 sec.\n",
      "Epoch 1251: T.cost 0.005917, Train acc 0.998620, test acc 0.791100, took 13.8 sec.\n",
      "Epoch 1252: T.cost 0.006804, Train acc 0.998200, test acc 0.788800, took 12.5 sec.\n",
      "Epoch 1253: T.cost 0.006030, Train acc 0.998400, test acc 0.792800, took 12.5 sec.\n",
      "Epoch 1254: T.cost 0.006999, Train acc 0.998260, test acc 0.794900, took 12.5 sec.\n",
      "Epoch 1255: T.cost 0.006431, Train acc 0.998380, test acc 0.794500, took 13.8 sec.\n",
      "Epoch 1256: T.cost 0.006067, Train acc 0.998380, test acc 0.796700, took 22.8 sec.\n",
      "Epoch 1257: T.cost 0.006643, Train acc 0.998520, test acc 0.793900, took 15.7 sec.\n",
      "Epoch 1258: T.cost 0.006645, Train acc 0.998260, test acc 0.792800, took 12.5 sec.\n",
      "New LR: 0.000530905718333\n",
      "Epoch 1259: T.cost 0.006025, Train acc 0.998340, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 1260: T.cost 0.007133, Train acc 0.998340, test acc 0.796600, took 12.5 sec.\n",
      "Epoch 1261: T.cost 0.006889, Train acc 0.997960, test acc 0.792700, took 12.5 sec.\n",
      "Epoch 1262: T.cost 0.005763, Train acc 0.998560, test acc 0.791300, took 17.8 sec.\n",
      "Epoch 1263: T.cost 0.007824, Train acc 0.998160, test acc 0.790500, took 22.0 sec.\n",
      "Epoch 1264: T.cost 0.006328, Train acc 0.998280, test acc 0.792800, took 12.5 sec.\n",
      "Epoch 1265: T.cost 0.005367, Train acc 0.998380, test acc 0.792000, took 12.5 sec.\n",
      "Epoch 1266: T.cost 0.007131, Train acc 0.998240, test acc 0.793600, took 12.5 sec.\n",
      "Epoch 1267: T.cost 0.004793, Train acc 0.998560, test acc 0.791700, took 12.5 sec.\n",
      "Epoch 1268: T.cost 0.007287, Train acc 0.998300, test acc 0.797500, took 12.5 sec.\n",
      "Epoch 1269: T.cost 0.006613, Train acc 0.998320, test acc 0.792400, took 14.3 sec.\n",
      "Epoch 1270: T.cost 0.008183, Train acc 0.997900, test acc 0.795900, took 22.8 sec.\n",
      "Epoch 1271: T.cost 0.006803, Train acc 0.998320, test acc 0.791300, took 15.2 sec.\n",
      "Epoch 1272: T.cost 0.006775, Train acc 0.998400, test acc 0.791500, took 12.5 sec.\n",
      "Epoch 1273: T.cost 0.008889, Train acc 0.997840, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1274: T.cost 0.005605, Train acc 0.998540, test acc 0.795700, took 12.5 sec.\n",
      "Epoch 1275: T.cost 0.006644, Train acc 0.998240, test acc 0.797700, took 12.5 sec.\n",
      "Epoch 1276: T.cost 0.003548, Train acc 0.998920, test acc 0.791000, took 17.7 sec.\n",
      "Epoch 1277: T.cost 0.006667, Train acc 0.998060, test acc 0.791800, took 22.1 sec.\n",
      "Epoch 1278: T.cost 0.006450, Train acc 0.998320, test acc 0.792600, took 12.5 sec.\n",
      "New LR: 0.000525596673251\n",
      "Epoch 1279: T.cost 0.007902, Train acc 0.997980, test acc 0.797000, took 12.5 sec.\n",
      "Epoch 1280: T.cost 0.004603, Train acc 0.998680, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1281: T.cost 0.007134, Train acc 0.998140, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1282: T.cost 0.007360, Train acc 0.998280, test acc 0.791000, took 17.0 sec.\n",
      "Epoch 1283: T.cost 0.007528, Train acc 0.997760, test acc 0.786600, took 22.7 sec.\n",
      "Epoch 1284: T.cost 0.008885, Train acc 0.997860, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 1285: T.cost 0.006132, Train acc 0.998440, test acc 0.788100, took 12.5 sec.\n",
      "Epoch 1286: T.cost 0.008499, Train acc 0.997960, test acc 0.792600, took 12.5 sec.\n",
      "Epoch 1287: T.cost 0.005949, Train acc 0.998540, test acc 0.794500, took 12.5 sec.\n",
      "Epoch 1288: T.cost 0.005953, Train acc 0.998440, test acc 0.796900, took 12.5 sec.\n",
      "Epoch 1289: T.cost 0.003040, Train acc 0.999040, test acc 0.796900, took 21.0 sec.\n",
      "Epoch 1290: T.cost 0.005821, Train acc 0.998400, test acc 0.797600, took 18.8 sec.\n",
      "Epoch 1291: T.cost 0.007223, Train acc 0.998400, test acc 0.798300, took 12.5 sec.\n",
      "Epoch 1292: T.cost 0.005550, Train acc 0.998460, test acc 0.795600, took 12.5 sec.\n",
      "Epoch 1293: T.cost 0.005358, Train acc 0.998580, test acc 0.793300, took 12.5 sec.\n",
      "Epoch 1294: T.cost 0.005993, Train acc 0.998500, test acc 0.786800, took 12.5 sec.\n",
      "Epoch 1295: T.cost 0.005419, Train acc 0.998560, test acc 0.795100, took 20.0 sec.\n",
      "Epoch 1296: T.cost 0.006006, Train acc 0.998540, test acc 0.793600, took 19.8 sec.\n",
      "Epoch 1297: T.cost 0.005995, Train acc 0.998320, test acc 0.797300, took 12.5 sec.\n",
      "Epoch 1298: T.cost 0.007383, Train acc 0.997940, test acc 0.789200, took 12.5 sec.\n",
      "New LR: 0.000520340701332\n",
      "Epoch 1299: T.cost 0.004693, Train acc 0.998660, test acc 0.790800, took 12.5 sec.\n",
      "Epoch 1300: T.cost 0.006822, Train acc 0.998480, test acc 0.794000, took 12.5 sec.\n",
      "Epoch 1301: T.cost 0.004971, Train acc 0.998600, test acc 0.794800, took 13.6 sec.\n",
      "Epoch 1302: T.cost 0.005642, Train acc 0.998480, test acc 0.795200, took 22.6 sec.\n",
      "Epoch 1303: T.cost 0.006323, Train acc 0.998060, test acc 0.797200, took 16.1 sec.\n",
      "Epoch 1304: T.cost 0.004588, Train acc 0.998920, test acc 0.794300, took 12.5 sec.\n",
      "Epoch 1305: T.cost 0.007755, Train acc 0.998180, test acc 0.791500, took 12.5 sec.\n",
      "Epoch 1306: T.cost 0.007678, Train acc 0.998060, test acc 0.794300, took 12.5 sec.\n",
      "Epoch 1307: T.cost 0.005159, Train acc 0.998400, test acc 0.793000, took 12.5 sec.\n",
      "Epoch 1308: T.cost 0.006204, Train acc 0.998280, test acc 0.791400, took 21.5 sec.\n",
      "Epoch 1309: T.cost 0.009096, Train acc 0.997900, test acc 0.789500, took 18.4 sec.\n",
      "Epoch 1310: T.cost 0.006382, Train acc 0.998340, test acc 0.800000, took 12.5 sec.\n",
      "Epoch 1311: T.cost 0.006663, Train acc 0.998420, test acc 0.799900, took 12.5 sec.\n",
      "Epoch 1312: T.cost 0.005600, Train acc 0.998800, test acc 0.797700, took 12.5 sec.\n",
      "Epoch 1313: T.cost 0.005118, Train acc 0.998560, test acc 0.793500, took 12.5 sec.\n",
      "Epoch 1314: T.cost 0.008570, Train acc 0.997560, test acc 0.794500, took 12.7 sec.\n",
      "Epoch 1315: T.cost 0.006483, Train acc 0.998380, test acc 0.796900, took 22.7 sec.\n",
      "Epoch 1316: T.cost 0.005581, Train acc 0.998320, test acc 0.797800, took 16.9 sec.\n",
      "Epoch 1317: T.cost 0.006718, Train acc 0.998480, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1318: T.cost 0.008025, Train acc 0.998080, test acc 0.793200, took 12.5 sec.\n",
      "New LR: 0.000515137283946\n",
      "Epoch 1319: T.cost 0.004840, Train acc 0.998720, test acc 0.797100, took 12.5 sec.\n",
      "Epoch 1320: T.cost 0.004570, Train acc 0.998820, test acc 0.797500, took 12.5 sec.\n",
      "Epoch 1321: T.cost 0.007954, Train acc 0.998100, test acc 0.798100, took 21.2 sec.\n",
      "Epoch 1322: T.cost 0.006031, Train acc 0.998220, test acc 0.794300, took 18.6 sec.\n",
      "Epoch 1323: T.cost 0.005134, Train acc 0.998660, test acc 0.795700, took 12.5 sec.\n",
      "Epoch 1324: T.cost 0.006280, Train acc 0.998280, test acc 0.800600, took 12.5 sec.\n",
      "Epoch 1325: T.cost 0.004879, Train acc 0.998360, test acc 0.798600, took 12.5 sec.\n",
      "Epoch 1326: T.cost 0.005666, Train acc 0.998660, test acc 0.795500, took 12.5 sec.\n",
      "Epoch 1327: T.cost 0.007231, Train acc 0.998260, test acc 0.796800, took 12.5 sec.\n",
      "Epoch 1328: T.cost 0.008556, Train acc 0.997840, test acc 0.796500, took 22.7 sec.\n",
      "Epoch 1329: T.cost 0.005177, Train acc 0.998600, test acc 0.793400, took 17.1 sec.\n",
      "Epoch 1330: T.cost 0.005832, Train acc 0.998600, test acc 0.791300, took 12.5 sec.\n",
      "Epoch 1331: T.cost 0.005615, Train acc 0.998560, test acc 0.791100, took 12.5 sec.\n",
      "Epoch 1332: T.cost 0.007521, Train acc 0.998100, test acc 0.794200, took 12.5 sec.\n",
      "Epoch 1333: T.cost 0.005149, Train acc 0.998660, test acc 0.797200, took 12.5 sec.\n",
      "Epoch 1334: T.cost 0.005650, Train acc 0.998520, test acc 0.791500, took 12.5 sec.\n",
      "Epoch 1335: T.cost 0.008748, Train acc 0.997900, test acc 0.793500, took 19.8 sec.\n",
      "Epoch 1336: T.cost 0.005721, Train acc 0.998420, test acc 0.789400, took 20.0 sec.\n",
      "Epoch 1337: T.cost 0.005663, Train acc 0.998400, test acc 0.795500, took 12.5 sec.\n",
      "Epoch 1338: T.cost 0.006336, Train acc 0.998680, test acc 0.794700, took 12.5 sec.\n",
      "New LR: 0.000509985902463\n",
      "Epoch 1339: T.cost 0.005250, Train acc 0.998600, test acc 0.793800, took 12.5 sec.\n",
      "Epoch 1340: T.cost 0.006484, Train acc 0.998200, test acc 0.798200, took 12.5 sec.\n",
      "Epoch 1341: T.cost 0.006533, Train acc 0.998380, test acc 0.794100, took 13.1 sec.\n",
      "Epoch 1342: T.cost 0.005754, Train acc 0.998340, test acc 0.795300, took 22.8 sec.\n",
      "Epoch 1343: T.cost 0.005803, Train acc 0.998640, test acc 0.795300, took 16.4 sec.\n",
      "Epoch 1344: T.cost 0.006757, Train acc 0.998420, test acc 0.790100, took 12.5 sec.\n",
      "Epoch 1345: T.cost 0.004299, Train acc 0.998820, test acc 0.793600, took 12.5 sec.\n",
      "Epoch 1346: T.cost 0.004272, Train acc 0.998800, test acc 0.797600, took 12.5 sec.\n",
      "Epoch 1347: T.cost 0.009781, Train acc 0.997940, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1348: T.cost 0.005937, Train acc 0.998320, test acc 0.793200, took 18.3 sec.\n",
      "Epoch 1349: T.cost 0.006284, Train acc 0.998360, test acc 0.798600, took 21.5 sec.\n",
      "Epoch 1350: T.cost 0.005488, Train acc 0.998360, test acc 0.796400, took 12.5 sec.\n",
      "Epoch 1351: T.cost 0.004862, Train acc 0.998560, test acc 0.799000, took 12.5 sec.\n",
      "Epoch 1352: T.cost 0.006435, Train acc 0.998240, test acc 0.795100, took 12.5 sec.\n",
      "Epoch 1353: T.cost 0.006008, Train acc 0.998220, test acc 0.796100, took 12.5 sec.\n",
      "Epoch 1354: T.cost 0.007530, Train acc 0.998040, test acc 0.790900, took 12.9 sec.\n",
      "Epoch 1355: T.cost 0.006215, Train acc 0.998280, test acc 0.796200, took 22.6 sec.\n",
      "Epoch 1356: T.cost 0.006294, Train acc 0.998240, test acc 0.792200, took 16.7 sec.\n",
      "Epoch 1357: T.cost 0.005483, Train acc 0.998640, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 1358: T.cost 0.005078, Train acc 0.998460, test acc 0.793700, took 12.5 sec.\n",
      "New LR: 0.000504886038252\n",
      "Epoch 1359: T.cost 0.005614, Train acc 0.998520, test acc 0.791600, took 12.5 sec.\n",
      "Epoch 1360: T.cost 0.005815, Train acc 0.998460, test acc 0.791500, took 12.5 sec.\n",
      "Epoch 1361: T.cost 0.007423, Train acc 0.998160, test acc 0.793100, took 17.6 sec.\n",
      "Epoch 1362: T.cost 0.004847, Train acc 0.998580, test acc 0.798700, took 22.2 sec.\n",
      "Epoch 1363: T.cost 0.005541, Train acc 0.998380, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1364: T.cost 0.006352, Train acc 0.998520, test acc 0.789700, took 12.5 sec.\n",
      "Epoch 1365: T.cost 0.006307, Train acc 0.998380, test acc 0.796000, took 12.5 sec.\n",
      "Epoch 1366: T.cost 0.005634, Train acc 0.998520, test acc 0.795800, took 12.5 sec.\n",
      "Epoch 1367: T.cost 0.004999, Train acc 0.998820, test acc 0.796500, took 12.5 sec.\n",
      "Epoch 1368: T.cost 0.005569, Train acc 0.998460, test acc 0.798500, took 20.6 sec.\n",
      "Epoch 1369: T.cost 0.008398, Train acc 0.998220, test acc 0.794500, took 19.2 sec.\n",
      "Epoch 1370: T.cost 0.005336, Train acc 0.998700, test acc 0.797300, took 12.5 sec.\n",
      "Epoch 1371: T.cost 0.007551, Train acc 0.998240, test acc 0.795500, took 12.5 sec.\n",
      "Epoch 1372: T.cost 0.005630, Train acc 0.998540, test acc 0.791800, took 12.5 sec.\n",
      "Epoch 1373: T.cost 0.005465, Train acc 0.998600, test acc 0.796300, took 12.5 sec.\n",
      "Epoch 1374: T.cost 0.005162, Train acc 0.998920, test acc 0.796900, took 16.5 sec.\n",
      "Epoch 1375: T.cost 0.006641, Train acc 0.998340, test acc 0.794900, took 22.7 sec.\n",
      "Epoch 1376: T.cost 0.005919, Train acc 0.998680, test acc 0.795900, took 13.1 sec.\n",
      "Epoch 1377: T.cost 0.004732, Train acc 0.998680, test acc 0.793700, took 12.5 sec.\n",
      "Epoch 1378: T.cost 0.006176, Train acc 0.998180, test acc 0.795000, took 12.5 sec.\n",
      "New LR: 0.000499837172683\n",
      "Epoch 1379: T.cost 0.004284, Train acc 0.998820, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1380: T.cost 0.007239, Train acc 0.998180, test acc 0.792900, took 12.5 sec.\n",
      "Epoch 1381: T.cost 0.003368, Train acc 0.999020, test acc 0.797000, took 18.2 sec.\n",
      "Epoch 1382: T.cost 0.006197, Train acc 0.998460, test acc 0.792700, took 21.7 sec.\n",
      "Epoch 1383: T.cost 0.007270, Train acc 0.998140, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1384: T.cost 0.004958, Train acc 0.998960, test acc 0.792000, took 12.5 sec.\n",
      "Epoch 1385: T.cost 0.006301, Train acc 0.998220, test acc 0.791600, took 12.5 sec.\n",
      "Epoch 1386: T.cost 0.009653, Train acc 0.998380, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1387: T.cost 0.008788, Train acc 0.997900, test acc 0.793500, took 14.0 sec.\n",
      "Epoch 1388: T.cost 0.007580, Train acc 0.998320, test acc 0.793300, took 22.8 sec.\n",
      "Epoch 1389: T.cost 0.005787, Train acc 0.998380, test acc 0.794500, took 15.5 sec.\n",
      "Epoch 1390: T.cost 0.006113, Train acc 0.998520, test acc 0.789700, took 12.5 sec.\n",
      "Epoch 1391: T.cost 0.005679, Train acc 0.998440, test acc 0.796500, took 12.5 sec.\n",
      "Epoch 1392: T.cost 0.004207, Train acc 0.998820, test acc 0.796500, took 12.5 sec.\n",
      "Epoch 1393: T.cost 0.004113, Train acc 0.998740, test acc 0.797500, took 12.5 sec.\n",
      "Epoch 1394: T.cost 0.005989, Train acc 0.998480, test acc 0.798600, took 17.2 sec.\n",
      "Epoch 1395: T.cost 0.006295, Train acc 0.998260, test acc 0.795100, took 22.4 sec.\n",
      "Epoch 1396: T.cost 0.004690, Train acc 0.998620, test acc 0.792000, took 12.7 sec.\n",
      "Epoch 1397: T.cost 0.006422, Train acc 0.998260, test acc 0.796300, took 12.5 sec.\n",
      "Epoch 1398: T.cost 0.006437, Train acc 0.998460, test acc 0.797000, took 12.5 sec.\n",
      "New LR: 0.000494838787126\n",
      "Epoch 1399: T.cost 0.007907, Train acc 0.998300, test acc 0.795300, took 12.5 sec.\n",
      "Epoch 1400: T.cost 0.006809, Train acc 0.998460, test acc 0.788500, took 12.5 sec.\n",
      "Epoch 1401: T.cost 0.007604, Train acc 0.998240, test acc 0.794200, took 13.0 sec.\n",
      "Epoch 1402: T.cost 0.006506, Train acc 0.998420, test acc 0.787600, took 22.8 sec.\n",
      "Epoch 1403: T.cost 0.005272, Train acc 0.998500, test acc 0.791100, took 16.4 sec.\n",
      "Epoch 1404: T.cost 0.006285, Train acc 0.998380, test acc 0.790100, took 12.5 sec.\n",
      "Epoch 1405: T.cost 0.005811, Train acc 0.998440, test acc 0.792700, took 12.5 sec.\n",
      "Epoch 1406: T.cost 0.005975, Train acc 0.998480, test acc 0.790100, took 12.5 sec.\n",
      "Epoch 1407: T.cost 0.005671, Train acc 0.998480, test acc 0.795600, took 12.5 sec.\n",
      "Epoch 1408: T.cost 0.004510, Train acc 0.998720, test acc 0.794100, took 16.9 sec.\n",
      "Epoch 1409: T.cost 0.005643, Train acc 0.998620, test acc 0.795400, took 22.3 sec.\n",
      "Epoch 1410: T.cost 0.006421, Train acc 0.998440, test acc 0.796700, took 13.0 sec.\n",
      "Epoch 1411: T.cost 0.006749, Train acc 0.998500, test acc 0.794700, took 12.5 sec.\n",
      "Epoch 1412: T.cost 0.004779, Train acc 0.998600, test acc 0.792100, took 12.5 sec.\n",
      "Epoch 1413: T.cost 0.005497, Train acc 0.998760, test acc 0.794400, took 12.5 sec.\n",
      "Epoch 1414: T.cost 0.005951, Train acc 0.998560, test acc 0.794900, took 12.5 sec.\n",
      "Epoch 1415: T.cost 0.006863, Train acc 0.998180, test acc 0.794500, took 21.8 sec.\n",
      "Epoch 1416: T.cost 0.006908, Train acc 0.998180, test acc 0.798800, took 18.0 sec.\n",
      "Epoch 1417: T.cost 0.005351, Train acc 0.998540, test acc 0.792200, took 12.5 sec.\n",
      "Epoch 1418: T.cost 0.005942, Train acc 0.998880, test acc 0.794700, took 12.5 sec.\n",
      "New LR: 0.000489890420577\n",
      "Epoch 1419: T.cost 0.005073, Train acc 0.998620, test acc 0.797000, took 12.5 sec.\n",
      "Epoch 1420: T.cost 0.006884, Train acc 0.998140, test acc 0.796900, took 12.5 sec.\n",
      "Epoch 1421: T.cost 0.005040, Train acc 0.998580, test acc 0.797500, took 14.3 sec.\n",
      "Epoch 1422: T.cost 0.003530, Train acc 0.998940, test acc 0.793700, took 22.7 sec.\n",
      "Epoch 1423: T.cost 0.003809, Train acc 0.999060, test acc 0.791000, took 15.3 sec.\n",
      "Epoch 1424: T.cost 0.004661, Train acc 0.998800, test acc 0.797000, took 12.5 sec.\n",
      "Epoch 1425: T.cost 0.006848, Train acc 0.998340, test acc 0.786800, took 12.5 sec.\n",
      "Epoch 1426: T.cost 0.005875, Train acc 0.998500, test acc 0.787700, took 12.5 sec.\n",
      "Epoch 1427: T.cost 0.006234, Train acc 0.998420, test acc 0.793300, took 12.5 sec.\n",
      "Epoch 1428: T.cost 0.006483, Train acc 0.998380, test acc 0.790800, took 19.4 sec.\n",
      "Epoch 1429: T.cost 0.007078, Train acc 0.998200, test acc 0.792100, took 20.5 sec.\n",
      "Epoch 1430: T.cost 0.005558, Train acc 0.998480, test acc 0.793700, took 12.5 sec.\n",
      "Epoch 1431: T.cost 0.004817, Train acc 0.998740, test acc 0.790600, took 12.5 sec.\n",
      "Epoch 1432: T.cost 0.005164, Train acc 0.998540, test acc 0.789300, took 12.5 sec.\n",
      "Epoch 1433: T.cost 0.003505, Train acc 0.998760, test acc 0.795300, took 12.5 sec.\n",
      "Epoch 1434: T.cost 0.005722, Train acc 0.998540, test acc 0.791600, took 12.6 sec.\n",
      "Epoch 1435: T.cost 0.005471, Train acc 0.998660, test acc 0.794400, took 22.7 sec.\n",
      "Epoch 1436: T.cost 0.003671, Train acc 0.999000, test acc 0.795700, took 16.9 sec.\n",
      "Epoch 1437: T.cost 0.007381, Train acc 0.998160, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1438: T.cost 0.006981, Train acc 0.998500, test acc 0.791800, took 12.5 sec.\n",
      "New LR: 0.000484991496778\n",
      "Epoch 1439: T.cost 0.005249, Train acc 0.998660, test acc 0.791800, took 12.5 sec.\n",
      "Epoch 1440: T.cost 0.003688, Train acc 0.998920, test acc 0.792900, took 12.5 sec.\n",
      "Epoch 1441: T.cost 0.006169, Train acc 0.998480, test acc 0.794900, took 16.9 sec.\n",
      "Epoch 1442: T.cost 0.006293, Train acc 0.998380, test acc 0.795500, took 22.6 sec.\n",
      "Epoch 1443: T.cost 0.005406, Train acc 0.998440, test acc 0.793300, took 12.9 sec.\n",
      "Epoch 1444: T.cost 0.005483, Train acc 0.998520, test acc 0.794300, took 12.5 sec.\n",
      "Epoch 1445: T.cost 0.006190, Train acc 0.998580, test acc 0.788500, took 12.5 sec.\n",
      "Epoch 1446: T.cost 0.009204, Train acc 0.997840, test acc 0.789800, took 12.5 sec.\n",
      "Epoch 1447: T.cost 0.007167, Train acc 0.998240, test acc 0.788200, took 12.5 sec.\n",
      "Epoch 1448: T.cost 0.003723, Train acc 0.998820, test acc 0.794900, took 20.8 sec.\n",
      "Epoch 1449: T.cost 0.005974, Train acc 0.998520, test acc 0.789900, took 19.1 sec.\n",
      "Epoch 1450: T.cost 0.006169, Train acc 0.998580, test acc 0.792300, took 12.5 sec.\n",
      "Epoch 1451: T.cost 0.004910, Train acc 0.998800, test acc 0.789700, took 12.5 sec.\n",
      "Epoch 1452: T.cost 0.005674, Train acc 0.998600, test acc 0.793600, took 12.5 sec.\n",
      "Epoch 1453: T.cost 0.004981, Train acc 0.998660, test acc 0.798000, took 12.5 sec.\n",
      "Epoch 1454: T.cost 0.004474, Train acc 0.998740, test acc 0.798200, took 12.5 sec.\n",
      "Epoch 1455: T.cost 0.004343, Train acc 0.998720, test acc 0.792900, took 21.5 sec.\n",
      "Epoch 1456: T.cost 0.004145, Train acc 0.998900, test acc 0.796100, took 18.3 sec.\n",
      "Epoch 1457: T.cost 0.008081, Train acc 0.998060, test acc 0.795600, took 12.5 sec.\n",
      "Epoch 1458: T.cost 0.006021, Train acc 0.998560, test acc 0.793700, took 12.5 sec.\n",
      "New LR: 0.000480141583539\n",
      "Epoch 1459: T.cost 0.006283, Train acc 0.998460, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1460: T.cost 0.005394, Train acc 0.998460, test acc 0.794900, took 12.5 sec.\n",
      "Epoch 1461: T.cost 0.005844, Train acc 0.998700, test acc 0.793000, took 15.7 sec.\n",
      "Epoch 1462: T.cost 0.004255, Train acc 0.998700, test acc 0.799100, took 22.7 sec.\n",
      "Epoch 1463: T.cost 0.005931, Train acc 0.998600, test acc 0.795200, took 13.8 sec.\n",
      "Epoch 1464: T.cost 0.007129, Train acc 0.998320, test acc 0.793200, took 12.5 sec.\n",
      "Epoch 1465: T.cost 0.005575, Train acc 0.998780, test acc 0.791700, took 12.5 sec.\n",
      "Epoch 1466: T.cost 0.004555, Train acc 0.999060, test acc 0.794200, took 12.5 sec.\n",
      "Epoch 1467: T.cost 0.003802, Train acc 0.998680, test acc 0.791900, took 12.5 sec.\n",
      "Epoch 1468: T.cost 0.005287, Train acc 0.998580, test acc 0.792900, took 12.5 sec.\n",
      "Epoch 1469: T.cost 0.003295, Train acc 0.998980, test acc 0.793600, took 19.4 sec.\n",
      "Epoch 1470: T.cost 0.007379, Train acc 0.998260, test acc 0.791600, took 20.5 sec.\n",
      "Epoch 1471: T.cost 0.005592, Train acc 0.998500, test acc 0.793100, took 12.5 sec.\n",
      "Epoch 1472: T.cost 0.003300, Train acc 0.999100, test acc 0.796900, took 12.5 sec.\n",
      "Epoch 1473: T.cost 0.005559, Train acc 0.998360, test acc 0.790000, took 12.5 sec.\n",
      "Epoch 1474: T.cost 0.005049, Train acc 0.998660, test acc 0.790500, took 12.5 sec.\n",
      "Epoch 1475: T.cost 0.006249, Train acc 0.998560, test acc 0.792500, took 12.5 sec.\n",
      "Epoch 1476: T.cost 0.006657, Train acc 0.998320, test acc 0.796500, took 22.3 sec.\n",
      "Epoch 1477: T.cost 0.003712, Train acc 0.999020, test acc 0.795200, took 17.5 sec.\n",
      "Epoch 1478: T.cost 0.005882, Train acc 0.998480, test acc 0.791400, took 12.5 sec.\n",
      "New LR: 0.000475340162229\n",
      "Epoch 1479: T.cost 0.005853, Train acc 0.998440, test acc 0.792600, took 12.5 sec.\n",
      "Epoch 1480: T.cost 0.005462, Train acc 0.998540, test acc 0.795000, took 12.5 sec.\n",
      "Epoch 1481: T.cost 0.006143, Train acc 0.998400, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 1482: T.cost 0.003435, Train acc 0.998840, test acc 0.792900, took 16.6 sec.\n",
      "Epoch 1483: T.cost 0.004125, Train acc 0.998840, test acc 0.795200, took 22.6 sec.\n",
      "Epoch 1484: T.cost 0.006603, Train acc 0.998320, test acc 0.791800, took 13.1 sec.\n",
      "Epoch 1485: T.cost 0.007517, Train acc 0.998320, test acc 0.789700, took 12.5 sec.\n",
      "Epoch 1486: T.cost 0.005688, Train acc 0.998760, test acc 0.792600, took 12.5 sec.\n",
      "Epoch 1487: T.cost 0.004428, Train acc 0.998760, test acc 0.790900, took 12.5 sec.\n",
      "Epoch 1488: T.cost 0.006532, Train acc 0.998480, test acc 0.789600, took 12.5 sec.\n",
      "Epoch 1489: T.cost 0.004768, Train acc 0.998800, test acc 0.795500, took 18.7 sec.\n",
      "Epoch 1490: T.cost 0.006337, Train acc 0.998440, test acc 0.791500, took 21.2 sec.\n",
      "Epoch 1491: T.cost 0.005748, Train acc 0.998660, test acc 0.794100, took 12.5 sec.\n",
      "Epoch 1492: T.cost 0.006306, Train acc 0.998600, test acc 0.790400, took 12.5 sec.\n",
      "Epoch 1493: T.cost 0.004828, Train acc 0.998720, test acc 0.795100, took 12.5 sec.\n",
      "Epoch 1494: T.cost 0.003598, Train acc 0.999000, test acc 0.794800, took 12.5 sec.\n",
      "Epoch 1495: T.cost 0.007528, Train acc 0.998160, test acc 0.791200, took 12.5 sec.\n",
      "Epoch 1496: T.cost 0.006070, Train acc 0.998720, test acc 0.789800, took 21.7 sec.\n",
      "Epoch 1497: T.cost 0.007583, Train acc 0.998440, test acc 0.793700, took 18.1 sec.\n",
      "Epoch 1498: T.cost 0.005167, Train acc 0.998740, test acc 0.796500, took 12.5 sec.\n",
      "New LR: 0.000470586771844\n",
      "Epoch 1499: T.cost 0.005249, Train acc 0.998860, test acc 0.797500, took 12.5 sec.\n",
      "\n",
      "Total time spent: 2.6363e+04 seconds\n",
      "Traing Acc: 0.99886\n",
      "Test Acc: 0.7975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format}, suppress=True)\n",
    "train_accs, test_accs = [], []\n",
    "total_time = 0\n",
    "param_outputs, disc_outputs = [], []\n",
    "disc_dist_t_1 = None\n",
    "quantized_bins = []\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_cost, train_acc, param_output, disc_output = train_epoch(data['X_train'], data['y_train'])\n",
    "        test_acc = eval_epoch(data['X_test'], data['y_test'])\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        if DISC:\n",
    "            param_outputs = np.append(param_outputs, param_output)\n",
    "            disc_outputs = np.append(disc_outputs, disc_output)\n",
    "\n",
    "        if (n+1) % 20 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.99\n",
    "            print \"New LR:\", new_lr\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "        # Non-uniform Quantization\n",
    "        if DISC:\n",
    "            if n>0 and np.mod(n, 10) == 0:\n",
    "                dist = disc_output.reshape((-1, DISC_UNIT))\n",
    "                q_bins = find_quantization_bins(dist, sharedBins=sharedBins)\n",
    "                quantized_bins.append(q_bins)\n",
    "\n",
    "        time_spent = time.time() - start_time\n",
    "        total_time += time_spent\n",
    "        print \"Epoch {0}: T.cost {1:0.6f}, Train acc {2:0.6f}, test acc {3:0.6f}, took {4:.3} sec.\".format(\n",
    "                n, train_cost, train_acc, test_acc, time_spent)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "print \"\\nTotal time spent: {0:.5} seconds\\nTraing Acc: {1}\\nTest Acc: {2}\\n\".format(total_time, train_acc, test_acc) \n",
    "\n",
    "if DISC:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'dense_params': param_output,\n",
    "             'disc_params': disc_output,\n",
    "             'quantized_bins': quantized_bins}\n",
    "else:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'disc_params': disc_output}   \n",
    "\n",
    "with open(TEST_NAME + '.model', 'wb') as fp:\n",
    "  pickle.dump(story, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAHCCAYAAAC5alPUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVLktmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f189725f950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(1-np.array(train_accs), label='Training Error')\n",
    "plt.plot(1-np.array(test_accs), label='Test Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=8)\n",
    "plt.ylabel('Error', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dcfef8167e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Histograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdense_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisc_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Histograms\n",
    "dense_params = param_outputs.reshape((-1, 6))\n",
    "disc_params = disc_outputs.reshape((-1, 6))\n",
    "\n",
    "bin_count = 100\n",
    "plot_over = True\n",
    "\n",
    "for i in range(0, 6):\n",
    "    dns = dense_params[:, i]\n",
    "    \n",
    "    \n",
    "    dsc = disc_params[:, i]\n",
    "    print len(np.unique(dsc))\n",
    "    #PS: Using normed histograms to plot them over\n",
    "    # Theta x Dense\n",
    "    plt.figure()\n",
    "    n, bins, patches = plt.hist(dns, bins=bin_count, normed=plot_over, histtype='stepfilled')\n",
    "    plt.setp(patches, 'facecolor', 'r', 'alpha', 0.55)\n",
    "    if not plot_over:\n",
    "        plt.xlabel(('Theta({0}) - Discrete Output').format(i+1))\n",
    "        plt.ylabel('Frequency (Consider bin size)')\n",
    "        plt.grid(True)\n",
    "        plt.figure()\n",
    "    \n",
    "    # Theta x Discrete\n",
    "    n, bins, patches = plt.hist(dsc, bins=np.unique(dsc), normed=plot_over, histtype='stepfilled')\n",
    "    plt.setp(patches, 'facecolor', 'g', 'alpha', 0.55)\n",
    "    if not plot_over:\n",
    "        plt.xlabel(('Theta({0}) - Discrete Output').format(i+1))\n",
    "    else:\n",
    "        plt.xlabel(('Theta({0})').format(i+1))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,14))\n",
    "for i in range(3):\n",
    "    plt.subplot(321+i*2)\n",
    "    plt.imshow(data['X_test'][i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Original 60x60', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(322+i*2)\n",
    "    plt.imshow(test_transform[i].reshape(DIM//3, DIM//3), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Transformed 20x20', fontsize=20)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
