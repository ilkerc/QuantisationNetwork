{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: Tesla K40c (CNMeM is enabled with initial size: 40.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "# os.environ['THEANO_FLAGS']='contexts=dev0->cuda0;dev1->cuda1'\n",
    "os.environ['THEANO_FLAGS']='device=gpu1'\n",
    "import theano\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits as load_sk_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from lasagne.layers import InputLayer, DenseLayer, DropoutLayer\n",
    "from helpers.DiscreteLayer import DiscreteLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer dataset cl\n",
    "* 30 Dimensions of Features\n",
    "* 2 Classes\n",
    "* 569 examples (212(M),357(B))\n",
    "\n",
    "I'll be using ML.Perceptron, for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Disc. Layer Settings\n",
    "DISC = True\n",
    "QUANT_UNIT = 50\n",
    "QUANT = np.array(np.repeat(0.125, QUANT_UNIT), dtype='float32')\n",
    "VARIANCE_DEVIDER = .1\n",
    "ADDITIONAL_COST = False\n",
    "# Uniform Settings\n",
    "UNIFORM_QUANT = False\n",
    "UNIFORM_STEP = 10000\n",
    "\n",
    "# Test Specs\n",
    "TEST_NAME = 'digits-qnonuniform-var64'\n",
    "\n",
    "# Additional Settings\n",
    "lasagne.random.set_rng(np.random.RandomState(12345))  # Set random state so we can investigate results\n",
    "np.random.seed(1234)\n",
    "#theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = load_sk_dataset()\n",
    "    X = dataset['data']\n",
    "    no_classes = len(np.unique(dataset['target']))\n",
    "    X = normalize(X, norm='l2')\n",
    "    #     X = (X - X.mean()) / X.std()\n",
    "    Y = dataset['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=41)\n",
    "    return dict(X_train=X_train,\n",
    "               y_train=y_train,\n",
    "               X_test=X_test,\n",
    "               y_test=y_test,\n",
    "               classes=no_classes)\n",
    "data = load_dataset()\n",
    "NUM_CLASSES = data['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Discret. Layer\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(Xs, disc, classnum, QUANT_UNIT):\n",
    "    tanh = lasagne.nonlinearities.tanh\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    \n",
    "    l_in = InputLayer(shape=Xs.shape)\n",
    "    # l_dense1 = DenseLayer(l_in, num_units=25, nonlinearity=tanh)\n",
    "    l_dense2 = DenseLayer(l_in, num_units=QUANT_UNIT, nonlinearity=tanh, name='param_regressor')\n",
    "    if disc:\n",
    "        sharedBins = theano.shared(None, name='sharedBins')\n",
    "        l_dis = DiscreteLayer(l_dense2, sharedBins=sharedBins, name='disclayer')\n",
    "        print(\"Using Discret. Layer\")\n",
    "    else:\n",
    "        l_dis = l_dense2\n",
    "        print(\"No Disc. Layer\")\n",
    "    l_class = DenseLayer(l_dis, num_units=classnum, nonlinearity=softmax)\n",
    "    \n",
    "    if disc:\n",
    "        return l_class, sharedBins\n",
    "    else:\n",
    "        return l_class\n",
    "\n",
    "if DISC:\n",
    "    model, sharedBins = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "else:\n",
    "    model = build_mlp(data['X_train'], disc=DISC, classnum=NUM_CLASSES, QUANT_UNIT=QUANT_UNIT)\n",
    "model_params = lasagne.layers.get_all_params(model, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: dist, dist.shape = (-1, num_units)\n",
    "Find quantization bins of a given dist history\n",
    "Returns a list of (x, num_units), where x's length is a random variable\n",
    "\"\"\"\n",
    "def find_quantization_bins(dist, sharedBins):\n",
    "    # Quantizer function\n",
    "    def Q(x, y):\n",
    "        return y * np.floor((x/y) + .5)\n",
    "    \n",
    "    shape = dist.shape\n",
    "    init_Q = QUANT\n",
    "    final_Q = []\n",
    "    \n",
    "    # If Uniform Quantization\n",
    "    if UNIFORM_QUANT:\n",
    "        uniform_bin = np.linspace(-1, 1, UNIFORM_STEP)\n",
    "        uniform_bins = np.tile(uniform_bin, (shape[0], 1))\n",
    "        sharedBins.set_value(uniform_bins)\n",
    "        print \"Uniform Quantization\"\n",
    "        return uniform_bins\n",
    "    \n",
    "    # Theta iterator\n",
    "    for i in range(shape[1]):\n",
    "        theta_i = dist[:, i]\n",
    "        \n",
    "        # Whats is the error threshold for this distribution\n",
    "        Q_eps = np.var(theta_i) / VARIANCE_DEVIDER\n",
    "        \n",
    "        # Batch Iterator\n",
    "        final_Q_i = []\n",
    "        for j in range(shape[0]):\n",
    "            theta = theta_i[j]\n",
    "            \n",
    "            # Quantized theta = Quantization bins\n",
    "            q = init_Q[i]\n",
    "            x_i = theta\n",
    "            x_o = Q(x_i, q)\n",
    "            \n",
    "            # Optimize x_o\n",
    "\n",
    "            while(np.abs(x_o - x_i) > Q_eps):\n",
    "                q = q / 2\n",
    "                x_o = Q(x_i, q)\n",
    "            \n",
    "            # End of optimisation\n",
    "            final_Q_i.append(x_o)\n",
    "        \n",
    "        # Append to outer list\n",
    "        uniques = np.unique(np.array(final_Q_i))\n",
    "        final_Q.append(uniques.astype(theano.config.floatX))\n",
    "        \n",
    "    # Report\n",
    "    print \"New Bin Sizes: [\" + \", \".join([str(final_Q[x].shape[0]) for x in range(shape[1])] ) + \"]\"\n",
    "    sharedBins.set_value(final_Q)\n",
    "    return final_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_functions():\n",
    "    X = T.matrix(dtype=theano.config.floatX)\n",
    "    y = T.ivector()\n",
    "\n",
    "    ## Layer History\n",
    "    if DISC:\n",
    "        l_disc = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'disclayer')\n",
    "        l_paramreg = next(l for l in lasagne.layers.get_all_layers(model) if l.name is 'param_regressor')\n",
    "        l_disc_output, l_paramreg_output = lasagne.layers.get_output([l_disc, l_paramreg], X, deterministic=False)\n",
    "    ## Layer History\n",
    "\n",
    "    # training output\n",
    "    output_train = lasagne.layers.get_output(model, X, deterministic=False)\n",
    "\n",
    "    # evaluation output. Also includes output of transform for plotting\n",
    "    output_eval = lasagne.layers.get_output(model, X, deterministic=True)\n",
    "\n",
    "    sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(output_train, y))\n",
    "    \n",
    "    if ADDITIONAL_COST and DISC:\n",
    "        cost += T.mean(lasagne.objectives.squared_error(l_disc_output, l_paramreg_output))\n",
    "    \n",
    "    updates = lasagne.updates.adam(cost, model_params, learning_rate=sh_lr)\n",
    "    \n",
    "    # updates = lasagne.updates.nesterov_momentum(cost, model_params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    if DISC:\n",
    "        train = theano.function([X, y], [cost, output_train, l_disc_output, l_paramreg_output], updates=updates, allow_input_downcast=True)\n",
    "    else:\n",
    "        train = theano.function([X, y], [cost, output_train], updates=updates, allow_input_downcast=True)\n",
    "    eval = theano.function([X], [output_eval], allow_input_downcast=True)\n",
    "    \n",
    "    return train, eval, sh_lr\n",
    "\n",
    "train, eval, sh_lr = build_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y):\n",
    "    # History Keeping\n",
    "    param_output = []\n",
    "    disc_output = []\n",
    "    # History\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        if DISC:\n",
    "            cost, output_train, l_disc_output, l_paramreg_output = train(X_batch, y_batch)\n",
    "            param_output = np.append(param_output, l_paramreg_output)\n",
    "            disc_output = np.append(disc_output, l_disc_output)\n",
    "        else:\n",
    "            cost, output_train = train(X_batch, y_batch)\n",
    "        costs += [cost]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "    \n",
    "    return np.mean(costs), correct / float(num_samples), param_output, disc_output\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    output_eval = eval(X)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: T.cost 2.086357, Train acc 0.485038, test acc 0.752778, took 0.126 sec.\n",
      "Epoch 1: T.cost 1.562996, Train acc 0.789144, test acc 0.830556, took 0.0988 sec.\n",
      "Epoch 2: T.cost 1.060107, Train acc 0.852470, test acc 0.880556, took 0.0978 sec.\n",
      "Epoch 3: T.cost 0.739876, Train acc 0.898399, test acc 0.908333, took 0.0997 sec.\n",
      "Epoch 4: T.cost 0.554915, Train acc 0.917884, test acc 0.925000, took 0.1 sec.\n",
      "Epoch 5: T.cost 0.440401, Train acc 0.931106, test acc 0.927778, took 0.101 sec.\n",
      "Epoch 6: T.cost 0.364117, Train acc 0.933890, test acc 0.933333, took 0.0984 sec.\n",
      "Epoch 7: T.cost 0.310434, Train acc 0.941545, test acc 0.933333, took 0.0847 sec.\n",
      "Epoch 8: T.cost 0.270964, Train acc 0.945720, test acc 0.933333, took 0.086 sec.\n",
      "Epoch 9: T.cost 0.240856, Train acc 0.950592, test acc 0.938889, took 0.0857 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 11, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 10: T.cost 0.217164, Train acc 0.954071, test acc 0.941667, took 0.352 sec.\n",
      "Epoch 11: T.cost 0.199204, Train acc 0.958246, test acc 0.950000, took 0.791 sec.\n",
      "Epoch 12: T.cost 0.182658, Train acc 0.962422, test acc 0.950000, took 0.977 sec.\n",
      "Epoch 13: T.cost 0.168662, Train acc 0.965901, test acc 0.950000, took 1.02 sec.\n",
      "Epoch 14: T.cost 0.156928, Train acc 0.967293, test acc 0.955556, took 1.14 sec.\n",
      "Epoch 15: T.cost 0.147461, Train acc 0.968685, test acc 0.955556, took 0.857 sec.\n",
      "Epoch 16: T.cost 0.138612, Train acc 0.972164, test acc 0.955556, took 0.822 sec.\n",
      "Epoch 17: T.cost 0.130349, Train acc 0.974948, test acc 0.958333, took 0.828 sec.\n",
      "Epoch 18: T.cost 0.124519, Train acc 0.974948, test acc 0.955556, took 0.864 sec.\n",
      "New LR: 0.000900000042748\n",
      "Epoch 19: T.cost 0.118167, Train acc 0.977035, test acc 0.958333, took 0.824 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 11, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 20: T.cost 0.112005, Train acc 0.978427, test acc 0.958333, took 1.24 sec.\n",
      "Epoch 21: T.cost 0.107776, Train acc 0.980515, test acc 0.958333, took 0.827 sec.\n",
      "Epoch 22: T.cost 0.103429, Train acc 0.981907, test acc 0.958333, took 0.968 sec.\n",
      "Epoch 23: T.cost 0.098703, Train acc 0.982603, test acc 0.961111, took 0.876 sec.\n",
      "Epoch 24: T.cost 0.095339, Train acc 0.981907, test acc 0.963889, took 0.958 sec.\n",
      "Epoch 25: T.cost 0.091694, Train acc 0.981907, test acc 0.966667, took 0.93 sec.\n",
      "Epoch 26: T.cost 0.088166, Train acc 0.983299, test acc 0.966667, took 0.878 sec.\n",
      "Epoch 27: T.cost 0.085892, Train acc 0.983994, test acc 0.966667, took 0.821 sec.\n",
      "Epoch 28: T.cost 0.082727, Train acc 0.986082, test acc 0.966667, took 0.822 sec.\n",
      "Epoch 29: T.cost 0.079699, Train acc 0.985386, test acc 0.966667, took 0.836 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 30: T.cost 0.077169, Train acc 0.986778, test acc 0.969444, took 1.12 sec.\n",
      "Epoch 31: T.cost 0.074603, Train acc 0.986778, test acc 0.969444, took 0.831 sec.\n",
      "Epoch 32: T.cost 0.071910, Train acc 0.986778, test acc 0.969444, took 0.826 sec.\n",
      "Epoch 33: T.cost 0.069790, Train acc 0.986082, test acc 0.969444, took 0.826 sec.\n",
      "Epoch 34: T.cost 0.068161, Train acc 0.986082, test acc 0.969444, took 0.825 sec.\n",
      "Epoch 35: T.cost 0.066135, Train acc 0.986082, test acc 0.969444, took 0.826 sec.\n",
      "Epoch 36: T.cost 0.063837, Train acc 0.986082, test acc 0.969444, took 0.828 sec.\n",
      "Epoch 37: T.cost 0.061840, Train acc 0.986082, test acc 0.969444, took 0.835 sec.\n",
      "Epoch 38: T.cost 0.059760, Train acc 0.986082, test acc 0.969444, took 0.84 sec.\n",
      "New LR: 0.000810000038473\n",
      "Epoch 39: T.cost 0.057476, Train acc 0.986778, test acc 0.969444, took 0.831 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 40: T.cost 0.055831, Train acc 0.988866, test acc 0.969444, took 1.11 sec.\n",
      "Epoch 41: T.cost 0.053638, Train acc 0.989562, test acc 0.969444, took 0.831 sec.\n",
      "Epoch 42: T.cost 0.052209, Train acc 0.990257, test acc 0.969444, took 0.832 sec.\n",
      "Epoch 43: T.cost 0.050726, Train acc 0.990257, test acc 0.969444, took 0.85 sec.\n",
      "Epoch 44: T.cost 0.049574, Train acc 0.990953, test acc 0.969444, took 0.85 sec.\n",
      "Epoch 45: T.cost 0.048279, Train acc 0.991649, test acc 0.969444, took 0.853 sec.\n",
      "Epoch 46: T.cost 0.047080, Train acc 0.991649, test acc 0.969444, took 0.865 sec.\n",
      "Epoch 47: T.cost 0.045872, Train acc 0.993041, test acc 0.966667, took 0.837 sec.\n",
      "Epoch 48: T.cost 0.044472, Train acc 0.993041, test acc 0.966667, took 0.835 sec.\n",
      "Epoch 49: T.cost 0.042747, Train acc 0.993041, test acc 0.966667, took 0.836 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 50: T.cost 0.041672, Train acc 0.993737, test acc 0.966667, took 1.14 sec.\n",
      "Epoch 51: T.cost 0.040816, Train acc 0.993737, test acc 0.966667, took 0.837 sec.\n",
      "Epoch 52: T.cost 0.039410, Train acc 0.993737, test acc 0.969444, took 0.836 sec.\n",
      "Epoch 53: T.cost 0.038271, Train acc 0.993737, test acc 0.972222, took 0.838 sec.\n",
      "Epoch 54: T.cost 0.037434, Train acc 0.993737, test acc 0.972222, took 0.84 sec.\n",
      "Epoch 55: T.cost 0.036034, Train acc 0.995129, test acc 0.972222, took 0.849 sec.\n",
      "Epoch 56: T.cost 0.034896, Train acc 0.995129, test acc 0.972222, took 0.859 sec.\n",
      "Epoch 57: T.cost 0.033798, Train acc 0.995129, test acc 0.972222, took 0.84 sec.\n",
      "Epoch 58: T.cost 0.032739, Train acc 0.995825, test acc 0.972222, took 0.868 sec.\n",
      "New LR: 0.000729000050342\n",
      "Epoch 59: T.cost 0.031975, Train acc 0.995825, test acc 0.972222, took 0.873 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 60: T.cost 0.030589, Train acc 0.995129, test acc 0.972222, took 1.18 sec.\n",
      "Epoch 61: T.cost 0.029822, Train acc 0.995825, test acc 0.972222, took 0.838 sec.\n",
      "Epoch 62: T.cost 0.029056, Train acc 0.996521, test acc 0.972222, took 0.841 sec.\n",
      "Epoch 63: T.cost 0.028523, Train acc 0.996521, test acc 0.972222, took 0.882 sec.\n",
      "Epoch 64: T.cost 0.027756, Train acc 0.997216, test acc 0.972222, took 0.869 sec.\n",
      "Epoch 65: T.cost 0.027300, Train acc 0.995825, test acc 0.972222, took 0.854 sec.\n",
      "Epoch 66: T.cost 0.026399, Train acc 0.997216, test acc 0.972222, took 0.842 sec.\n",
      "Epoch 67: T.cost 0.025744, Train acc 0.997216, test acc 0.975000, took 0.881 sec.\n",
      "Epoch 68: T.cost 0.024895, Train acc 0.997216, test acc 0.975000, took 0.87 sec.\n",
      "Epoch 69: T.cost 0.023991, Train acc 0.998608, test acc 0.972222, took 0.862 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 70: T.cost 0.023643, Train acc 0.998608, test acc 0.975000, took 1.15 sec.\n",
      "Epoch 71: T.cost 0.022996, Train acc 0.997912, test acc 0.975000, took 0.876 sec.\n",
      "Epoch 72: T.cost 0.022272, Train acc 0.997912, test acc 0.975000, took 0.886 sec.\n",
      "Epoch 73: T.cost 0.021800, Train acc 0.997912, test acc 0.972222, took 0.87 sec.\n",
      "Epoch 74: T.cost 0.020982, Train acc 0.997912, test acc 0.972222, took 0.858 sec.\n",
      "Epoch 75: T.cost 0.020573, Train acc 0.997912, test acc 0.972222, took 0.886 sec.\n",
      "Epoch 76: T.cost 0.020214, Train acc 0.997912, test acc 0.972222, took 0.889 sec.\n",
      "Epoch 77: T.cost 0.019704, Train acc 0.997912, test acc 0.972222, took 0.868 sec.\n",
      "Epoch 78: T.cost 0.019357, Train acc 0.997912, test acc 0.972222, took 0.89 sec.\n",
      "New LR: 0.000656100071501\n",
      "Epoch 79: T.cost 0.018697, Train acc 0.997912, test acc 0.972222, took 0.849 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 80: T.cost 0.018155, Train acc 0.997912, test acc 0.972222, took 1.16 sec.\n",
      "Epoch 81: T.cost 0.017644, Train acc 0.997912, test acc 0.972222, took 0.863 sec.\n",
      "Epoch 82: T.cost 0.017294, Train acc 0.997912, test acc 0.972222, took 0.87 sec.\n",
      "Epoch 83: T.cost 0.016887, Train acc 0.998608, test acc 0.972222, took 0.868 sec.\n",
      "Epoch 84: T.cost 0.016545, Train acc 0.998608, test acc 0.972222, took 0.874 sec.\n",
      "Epoch 85: T.cost 0.016305, Train acc 0.999304, test acc 0.972222, took 0.849 sec.\n",
      "Epoch 86: T.cost 0.015821, Train acc 0.999304, test acc 0.972222, took 0.85 sec.\n",
      "Epoch 87: T.cost 0.015425, Train acc 0.999304, test acc 0.972222, took 0.855 sec.\n",
      "Epoch 88: T.cost 0.015166, Train acc 0.999304, test acc 0.972222, took 0.868 sec.\n",
      "Epoch 89: T.cost 0.014961, Train acc 1.000000, test acc 0.972222, took 0.898 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 90: T.cost 0.014532, Train acc 1.000000, test acc 0.972222, took 1.17 sec.\n",
      "Epoch 91: T.cost 0.014287, Train acc 1.000000, test acc 0.972222, took 0.88 sec.\n",
      "Epoch 92: T.cost 0.013821, Train acc 1.000000, test acc 0.972222, took 0.872 sec.\n",
      "Epoch 93: T.cost 0.013696, Train acc 1.000000, test acc 0.972222, took 0.899 sec.\n",
      "Epoch 94: T.cost 0.013454, Train acc 1.000000, test acc 0.972222, took 0.9 sec.\n",
      "Epoch 95: T.cost 0.013263, Train acc 1.000000, test acc 0.972222, took 0.863 sec.\n",
      "Epoch 96: T.cost 0.013052, Train acc 1.000000, test acc 0.972222, took 0.872 sec.\n",
      "Epoch 97: T.cost 0.012811, Train acc 1.000000, test acc 0.972222, took 0.874 sec.\n",
      "Epoch 98: T.cost 0.012354, Train acc 1.000000, test acc 0.969444, took 0.869 sec.\n",
      "New LR: 0.000590490043396\n",
      "Epoch 99: T.cost 0.012123, Train acc 1.000000, test acc 0.972222, took 0.861 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 100: T.cost 0.011864, Train acc 1.000000, test acc 0.969444, took 1.17 sec.\n",
      "Epoch 101: T.cost 0.011407, Train acc 1.000000, test acc 0.969444, took 0.856 sec.\n",
      "Epoch 102: T.cost 0.011295, Train acc 1.000000, test acc 0.969444, took 0.88 sec.\n",
      "Epoch 103: T.cost 0.010932, Train acc 1.000000, test acc 0.969444, took 0.902 sec.\n",
      "Epoch 104: T.cost 0.010734, Train acc 1.000000, test acc 0.969444, took 0.895 sec.\n",
      "Epoch 105: T.cost 0.010431, Train acc 1.000000, test acc 0.969444, took 0.876 sec.\n",
      "Epoch 106: T.cost 0.010466, Train acc 1.000000, test acc 0.969444, took 0.859 sec.\n",
      "Epoch 107: T.cost 0.010149, Train acc 1.000000, test acc 0.969444, took 0.904 sec.\n",
      "Epoch 108: T.cost 0.010020, Train acc 1.000000, test acc 0.969444, took 0.884 sec.\n",
      "Epoch 109: T.cost 0.009734, Train acc 1.000000, test acc 0.969444, took 0.893 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 110: T.cost 0.009473, Train acc 1.000000, test acc 0.969444, took 1.15 sec.\n",
      "Epoch 111: T.cost 0.009368, Train acc 1.000000, test acc 0.969444, took 0.893 sec.\n",
      "Epoch 112: T.cost 0.009150, Train acc 1.000000, test acc 0.972222, took 0.904 sec.\n",
      "Epoch 113: T.cost 0.009005, Train acc 1.000000, test acc 0.969444, took 0.884 sec.\n",
      "Epoch 114: T.cost 0.008703, Train acc 1.000000, test acc 0.969444, took 0.885 sec.\n",
      "Epoch 115: T.cost 0.008341, Train acc 1.000000, test acc 0.969444, took 0.888 sec.\n",
      "Epoch 116: T.cost 0.007957, Train acc 1.000000, test acc 0.969444, took 0.869 sec.\n",
      "Epoch 117: T.cost 0.007959, Train acc 1.000000, test acc 0.969444, took 0.879 sec.\n",
      "Epoch 118: T.cost 0.007594, Train acc 1.000000, test acc 0.969444, took 0.905 sec.\n",
      "New LR: 0.000531441054773\n",
      "Epoch 119: T.cost 0.007553, Train acc 1.000000, test acc 0.969444, took 0.871 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 120: T.cost 0.007269, Train acc 1.000000, test acc 0.975000, took 1.19 sec.\n",
      "Epoch 121: T.cost 0.007128, Train acc 1.000000, test acc 0.972222, took 0.866 sec.\n",
      "Epoch 122: T.cost 0.006959, Train acc 1.000000, test acc 0.975000, took 0.907 sec.\n",
      "Epoch 123: T.cost 0.006849, Train acc 1.000000, test acc 0.975000, took 0.887 sec.\n",
      "Epoch 124: T.cost 0.006811, Train acc 1.000000, test acc 0.972222, took 0.891 sec.\n",
      "Epoch 125: T.cost 0.006695, Train acc 1.000000, test acc 0.969444, took 0.886 sec.\n",
      "Epoch 126: T.cost 0.006440, Train acc 1.000000, test acc 0.972222, took 0.903 sec.\n",
      "Epoch 127: T.cost 0.006327, Train acc 1.000000, test acc 0.966667, took 0.881 sec.\n",
      "Epoch 128: T.cost 0.006219, Train acc 1.000000, test acc 0.969444, took 0.892 sec.\n",
      "Epoch 129: T.cost 0.006221, Train acc 1.000000, test acc 0.969444, took 0.874 sec.\n",
      "New Bin Sizes: [11, 11, 10, 12, 12, 11, 12, 11, 12, 10, 10, 11, 11, 11, 11, 13, 14, 10, 10, 10, 10, 12, 11, 12, 11, 11, 10, 12, 10, 11, 9, 12, 11, 12, 14, 12, 10, 10, 12, 11, 12, 13, 9, 10, 11, 10, 10, 12, 12, 10]\n",
      "Epoch 130: T.cost 0.005977, Train acc 1.000000, test acc 0.972222, took 1.18 sec.\n",
      "Epoch 131: T.cost 0.005852, Train acc 1.000000, test acc 0.969444, took 0.882 sec.\n",
      "Epoch 132: T.cost 0.005778, Train acc 1.000000, test acc 0.972222, took 0.893 sec.\n",
      "Epoch 133: T.cost 0.005668, Train acc 1.000000, test acc 0.969444, took 0.88 sec.\n",
      "Epoch 134: T.cost 0.005677, Train acc 1.000000, test acc 0.969444, took 0.894 sec.\n",
      "Epoch 135: T.cost 0.005487, Train acc 1.000000, test acc 0.972222, took 0.914 sec.\n",
      "Epoch 136: T.cost 0.005333, Train acc 1.000000, test acc 0.972222, took 0.878 sec.\n",
      "Epoch 137: T.cost 0.005211, Train acc 1.000000, test acc 0.972222, took 0.876 sec.\n",
      "Epoch 138: T.cost 0.005210, Train acc 1.000000, test acc 0.972222, took 0.886 sec.\n",
      "New LR: 0.000478296959773\n",
      "Epoch 139: T.cost 0.004917, Train acc 1.000000, test acc 0.972222, took 0.91 sec.\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format}, suppress=True)\n",
    "train_accs, test_accs = [], []\n",
    "total_time = 0\n",
    "param_outputs, disc_outputs = [], []\n",
    "disc_dist_t_1 = None\n",
    "quantized_bins = []\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_cost, train_acc, param_output, disc_output = train_epoch(data['X_train'], data['y_train'])\n",
    "        test_acc = eval_epoch(data['X_test'], data['y_test'])\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        if DISC:\n",
    "            param_outputs = np.append(param_outputs, param_output)\n",
    "            disc_outputs = np.append(disc_outputs, disc_output)\n",
    "\n",
    "        if (n+1) % 20 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.90\n",
    "            print \"New LR:\", new_lr\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "        # Non-uniform Quantization\n",
    "        if DISC:\n",
    "            if n>0 and np.mod(n, 10) == 0:\n",
    "                dist = disc_output.reshape((-1, QUANT_UNIT))\n",
    "                q_bins = find_quantization_bins(dist, sharedBins=sharedBins)\n",
    "                quantized_bins.append(q_bins)\n",
    "\n",
    "        time_spent = time.time() - start_time\n",
    "        total_time += time_spent\n",
    "        print \"Epoch {0}: T.cost {1:0.6f}, Train acc {2:0.6f}, test acc {3:0.6f}, took {4:.3} sec.\".format(\n",
    "                n, train_cost, train_acc, test_acc, time_spent)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "print \"\\nTotal time spent: {0:.5} seconds\\nTraing Acc: {1}\\nTest Acc: {2}\\n\".format(total_time, train_acc, test_acc) \n",
    "\n",
    "if DISC:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'dense_params': param_output,\n",
    "             'disc_params': disc_output,\n",
    "             'quantized_bins': quantized_bins}\n",
    "else:\n",
    "    story = {'train_accs': train_accs,\n",
    "             'test_accs': test_accs,\n",
    "             'epoch_reached': n, \n",
    "             'total_time': total_time,\n",
    "             'disc_enabled': DISC,\n",
    "             'learning_rate': LEARNING_RATE,\n",
    "             'batch_size': BATCH_SIZE,\n",
    "             'disc_params': disc_output}   \n",
    "\n",
    "with open(TEST_NAME + '.model', 'wb') as fp:\n",
    "  pickle.dump(story, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTOGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_output_r = disc_output.reshape((-1, QUANT_UNIT))\n",
    "dense_output_r = param_output.reshape((-1, QUANT_UNIT))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dense_output_r[:, 4], disc_output_r[:, 4], 'g.')\n",
    "plt.plot(dense_output_r[:, 1], disc_output_r[:, 1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.semilogy(1-np.array(train_accs), label='Training Error')\n",
    "plt.semilogy(1-np.array(test_accs), label='Test Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=8)\n",
    "plt.ylabel('Error', fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
